{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot scaleogram from Morlet wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Julia program to read a selected .HXV file and display 30-minute time series plots\n",
    "## JW December 2022\n",
    "#using ContinuousWavelets \n",
    "using CSV\n",
    "using Dates, DataFrames, Distributions, DSP\n",
    "##using Gtk\n",
    "using LaTeXStrings\n",
    "using NativeFileDialog\n",
    "using Plots\n",
    "using Printf\n",
    "using Statistics #, StatsPlots\n",
    "using Tk\n",
    "\n",
    "\n",
    "function get_displacements(arry)\n",
    "#####################################\n",
    "    \n",
    "    displacements = []\n",
    "\n",
    "    if length(arry[1]) == 3\n",
    "    \n",
    "        for i in arry\n",
    "            append!(displacements,parse(Int, SubString.(i, 1, 1), base=16)*16^2 + parse(Int, SubString.(i, 2, 2), base=16)*16^1 + parse(Int, SubString.(i, 3, 3), base=16)*16^0)\n",
    "        end\n",
    "        \n",
    "    else\n",
    "        \n",
    "        for i in arry\n",
    "            append!(displacements,parse(Int, SubString.(i, 1, 1), base=16)*16^1 + parse(Int, SubString.(i, 2, 2), base=16)*16^0)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    displacements[findall(>=(2048), displacements)] = 2048 .- displacements[findall(>=(2048), displacements)];\n",
    "    \n",
    "    return(displacements./100)\n",
    "    \n",
    "    end     # get_displacements()\n",
    "\n",
    "\n",
    "function get_HNW(df)\n",
    "#####################################\n",
    "        \n",
    "##    df = DataFrame(CSV.File(infil,header=0, delim=\",\", types=String));\n",
    "\n",
    "    # Calculate sequence numbers\n",
    "    arry = SubString.(df.Column1, 3, 4)\n",
    "\n",
    "    global sequence = []\n",
    "\n",
    "    for i in arry\n",
    "        append!(sequence,parse(Int, SubString.(i, 1, 1), base=16)*16^1 + parse(Int, SubString.(i, 2, 2), base=16)*16^0)\n",
    "    end\n",
    "\n",
    "    # Calculate heave WSEs\n",
    "    arry = SubString.(df.Column3, 1, 3);\n",
    "    heave = get_displacements(arry);\n",
    "\n",
    "    # Calculate north WSEs\n",
    "    arry = SubString.(df.Column3, 4, ) .* SubString.(df.Column4, 1, 2)\n",
    "    north = get_displacements(arry);\n",
    "\n",
    "    # Calculate north WSEs\n",
    "    arry = SubString.(df.Column4, 3, 4) .* SubString.(df.Column5, 1, 1)\n",
    "    west = get_displacements(arry);\n",
    "\n",
    "    return(heave, north, west)\n",
    "\n",
    "    end    # get_HNW()\n",
    "\n",
    "\n",
    "function calc_wse(df, wse_df, start_date)\n",
    "#####################################    \n",
    "    \n",
    "    heave, north, west = get_HNW(df)\n",
    "    \n",
    "    # Identify any gaps in the recorded data\n",
    "    tt = [0]\n",
    "    append!(tt,diff(sequence))\n",
    "    tt[tt.<0] .+= 256;\n",
    "    tt1 = cumsum(tt);\n",
    "    \n",
    "    if length(tt1) > 2304\n",
    "        tt1 = tt1[1:2304]\n",
    "    end\n",
    "\n",
    "    [wse_df[tt1[i]+1,2] = heave[i] for i in eachindex(tt1)];\n",
    "    [wse_df[tt1[i]+1,3] = north[i] for i in eachindex(tt1)];\n",
    "    [wse_df[tt1[i]+1,4] = west[i] for i in eachindex(tt1)];\n",
    "    \n",
    "    return(wse_df)\n",
    "    \n",
    "    end    # calc_wse()\n",
    "\n",
    "\n",
    "function spike_value(wse)\n",
    "#####################################\n",
    "    \n",
    "    median_value = median(wse)\n",
    "    std_value = std(wse)\n",
    "\n",
    "    return(median_value + 3*std_value)\n",
    "\n",
    "    end    # spike_value()\n",
    "\n",
    "\n",
    "function plot_wses(df, wse_df, is_gps)\n",
    "#####################################\n",
    "    \n",
    "    spike = spike_value(wse_df.Heave)\n",
    "    heave_spikes = findall(i->(i>=spike), abs.(wse_df.Heave));\n",
    "\n",
    "    spike = spike_value(wse_df.North)\n",
    "    north_spikes = findall(i->(i>=spike), abs.(wse_df.North));\n",
    "\n",
    "    spike = spike_value(wse_df.West)\n",
    "    west_spikes = findall(i->(i>=spike), abs.(wse_df.West));\n",
    "    \n",
    "    # create plots of heave, north, and west\n",
    "    title_string = Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\") # * \" UTC\"\n",
    "    p1_hnw = scatter(wse_df[heave_spikes,:].Date, wse_df[heave_spikes,:].Heave, label=\"\", ylabel=\"Heave\", markershape=:circle, ms=4, mc=:white, ma=1, msc=:red, msa=0.25, msw=0.5)\n",
    "    p1_hnw = plot!(wse_df.Date,wse_df.Heave, label=\"\", c=\"#4a536b\", lw=0.5, title=title_string, titlefontsize=12) ##last(split(infil,\"\\\\\")))\n",
    "\n",
    "    # get plotting limits\n",
    "    x_lim1 = xlims(p1_hnw)[1]; y_lim1 = ylims(p1_hnw)[1]\n",
    "    x_lim2 = xlims(p1_hnw)[2]; y_lim2 = ylims(p1_hnw)[2]\n",
    "\n",
    "    p2_hnw = scatter(wse_df[north_spikes,:].Date, wse_df[north_spikes,:].North, label=\"\", ylabel=\"North\", markershape=:circle, ms=4, mc=:white, ma=1, msc=:red, msa=0.25, msw=0.5)\n",
    "    p2_hnw = plot!(wse_df.Date,wse_df.North, label=\"\", c=\"#aed6dc\", lw=0.5)\n",
    "    p3_hnw = scatter(wse_df[west_spikes,:].Date,wse_df[west_spikes,:].West, label=\"\", ylabel=\"West\", markershape=:circle, ms=4, mc=:white, ma=1, msc=:red, msa=0.25, msw=0.5)\n",
    "    p3_hnw = plot!(wse_df.Date,wse_df.West, label=\"\", c=\"#ff9a8d\", lw=0.5)\n",
    "\n",
    "    hline!(p1_hnw, [0], lw=0.5, label=\"\")\n",
    "    hline!(p2_hnw, [0], lw=0.5, label=\"\")\n",
    "    hline!(p3_hnw, [0], lw=0.5, label=\"\")\n",
    "    \n",
    "    if is_gps\n",
    "        \n",
    "        println(\"GPS buoy\")\n",
    "        flush(stdout)\n",
    "        \n",
    "        # Locate GPS errors\n",
    "        global gps_errors = findall(isodd,parse.(Int,SubString.(string.(df.Column4), 2, 2), base = 16))\n",
    "\n",
    "        if length(gps_errors) > 0\n",
    "            vline!(p1_hnw, [wse_df.Date[1]], lw=0.5, ls=:dash, c=:red, label=\"GPS error\")\n",
    "            println(length(gps_errors),\" GPS errors detected\")\n",
    "            flush(stdout)\n",
    "        end\n",
    "\n",
    "        for i in gps_errors\n",
    "            vline!(p1_hnw, [wse_df.Date[i]], lw=0.5, ls=:dash, c=:red, label=\"\")\n",
    "        end\n",
    "    \n",
    "    end\n",
    "\n",
    "    # get plotting limits\n",
    "    x_lim1 = xlims(p1_hnw)[1]; y_lim1 = ylims(p1_hnw)[1]\n",
    "    x_lim2 = xlims(p1_hnw)[2]; y_lim2 = ylims(p1_hnw)[2]\n",
    "\n",
    "    # display plots to screen\n",
    "    tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "    # display plots to screen\n",
    "    plot_wse = Plots.plot(p1_hnw, p2_hnw, p3_hnw, layout = (3, 1), size = (1400, 600),\n",
    "        xlim=(first(wse_df.Date),last(wse_df.Date)), xticks=(tm_tick,ticks), xtickfontsize=7,ytickfontsize=8,\n",
    "        framestyle = :box,fg_legend=:transparent, legend=:bottomleft,\n",
    "        leftmargin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)            \n",
    "\n",
    "    display(plot_wse)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # plot_wses)\n",
    "\n",
    "\n",
    "function do_fft(heave, N)\n",
    "################################################\n",
    "# calculate the Fourier coefficients vide (5.6.2)\n",
    "\n",
    "    return([sum([heave[k]*exp(2*pi*-1im*k*l/N) for k in (1:N)]) for l in (1:N)])\n",
    "\n",
    "    end    # do_fft()\n",
    "\n",
    "\n",
    "function calc_psd(Hl, N)\n",
    "################################################\n",
    "# The power spectral density is obtained from the Fourier coefficients\n",
    "    \n",
    "    PSD = zeros(trunc(Int,N/2))\n",
    "\n",
    "    for l = 1:trunc(Int,N/2)   \n",
    "        if (l==1) || (l==trunc(Int,N/2)-1)\n",
    "            PSD[l] = abs(Hl[l])^2\n",
    "        else\n",
    "            PSD[l] = abs(Hl[l])^2+abs(Hl[N-l-1])^2\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Smooth coefficients vide (5.6.6)\n",
    "    PSD_smooth = PSD\n",
    "    [PSD_smooth[i] = PSD[i-1]/4 + PSD[i]/2 + PSD[i+1]/4 for i in (2:trunc(Int,N/2)-1)]\n",
    "\n",
    "    return(PSD_smooth)\n",
    "\n",
    "    end    # calc_psd()\n",
    "\n",
    "\n",
    "function calc_tp5(f2,Sf)\n",
    "##########################################\n",
    "# Calculate Tp5 via Read method\n",
    "    \n",
    "    Sf_max = maximum(Sf)\n",
    "\n",
    "    numerator = 0; denominator = 0\n",
    "\n",
    "    Sf_sum = cumsum(Sf.*Sf_max).^5\n",
    "\n",
    "    for i in eachindex(f2)\n",
    "        w = Sf[i] / Sf_max\n",
    "        numerator +=  f2[i] * w^5\n",
    "        denominator += w^5\n",
    "    end\n",
    "\n",
    "    Fp5 = numerator / denominator\n",
    "    \n",
    "    return(Fp5)    # calc_tp5()\n",
    "\n",
    "    end    # calc_tp5()\n",
    "\n",
    "\n",
    "function calc_hm0(Sf,freq)\n",
    "########################################## \n",
    "    \n",
    "    ax1 = (last(freq) - first(freq)) / (length(freq)-1)\n",
    "\n",
    "    # calc spectral moments m0, m1, m2, m3, and m4\n",
    "    s00 = 0; m0 = 0\n",
    "\n",
    "    for ii in 1:128\n",
    "\n",
    "        s00 += freq[ii]^0 * Sf[ii];\n",
    "\n",
    "    end\n",
    "\n",
    "    m0 = 0.5*ax1*(first(freq)^0*first(Sf) + 2*s00 + last(freq)^0*last(Sf))\n",
    "\n",
    "    return(4 * m0^0.5)\n",
    "\n",
    "    end    # calc_hm0()\n",
    "\n",
    "\n",
    "function calculate_frequency_domain_parameters(f2, spectra)\n",
    "##########################################\n",
    "# Calculate frequency-domain parameters    \n",
    "# Calls: calc_tp5()\n",
    "    \n",
    "    ax1 = (last(f2) - first(f2)) / (length(f2)-1)\n",
    "\n",
    "    # calc spectral moments m0, m1, m2, m3, and m4\n",
    "    s00 = 0; s01 = 0; s02 = 0; s03 = 0; s04 = 0;\n",
    "    m0 = 0; m1 = 0; m2 = 0; m3 = 0; m4 = 0\n",
    "\n",
    "    for ii in 1:128\n",
    "\n",
    "        s00 += f2[ii]^0 * spectra[ii]\n",
    "        s01 += f2[ii]^1 * spectra[ii]\n",
    "        s02 += f2[ii]^2 * spectra[ii]\n",
    "        s03 += f2[ii]^3 * spectra[ii]\n",
    "        s04 += f2[ii]^4 * spectra[ii]\n",
    "\n",
    "    end\n",
    "\n",
    "    m0 = 0.5*ax1*(first(f2)^0*first(spectra) + 2*s00 + last(f2)^0*last(spectra))\n",
    "    m1 = 0.5*ax1*(first(f2)^1*first(spectra) + 2*s01 + last(f2)^1*last(spectra))\n",
    "    m2 = 0.5*ax1*(first(f2)^2*first(spectra) + 2*s02 + last(f2)^2*last(spectra))\n",
    "    m3 = 0.5*ax1*(first(f2)^3*first(spectra) + 2*s03 + last(f2)^3*last(spectra))\n",
    "    m4 = 0.5*ax1*(first(f2)^4*first(spectra) + 2*s04 + last(f2)^4*last(spectra))\n",
    "\n",
    "    ##println(\"m0 = \",m0,\" m1 = \",m1, \" m2 = \",m2, \" m3 = \",m2, \" m4 = \",m4)\n",
    "\n",
    "    # calc wave parameters Hm0, Hrms, T01, T02, Tc\n",
    "    Hm0 = 4*sqrt(m0)     # Tucker & Pitt p.32 (2.2-6b)\n",
    "    Hrms = sqrt(8*m0)    # Goda 2nd. Edition p.262 (9.15)\n",
    "    T01 = m0/m1          # Tucker & Pitt p.41 Table 2.2 \n",
    "    T02 = sqrt(m0/m2)    # Tucker & Pitt p.40 (2.3-2)\n",
    "    Tc = sqrt(m2/m4)     # Tucker & Pitt p.41 Table 2.2 - also see Notes\n",
    "\n",
    "    # identify spectral peak and frequency as peak\n",
    "    Fp = f2[argmax(spectra)]\n",
    "    Tp = 1/Fp\n",
    "    fp5 = calc_tp5(f2, spectra)\n",
    "    Tp5 = 1/fp5\n",
    "\n",
    "    # calculate spectral width vide Tucker and Pitt p.85 (5.2-8)\n",
    "    # Note: for JONSWAP, v = 0.39; for PM, v = 0.425\n",
    "    v = (m0*m2 / m1^2 - 1)^0.5\n",
    "\n",
    "    # calculate Skewness vide Tucker and Pitt p.109 (5.5-17)\n",
    "    Skewness = (m0^2 * m3/m1^3 - 3*v^2 - 1) / v^3;\n",
    "    \n",
    "    return(Hm0, Hrms, T01, T02, Tc, Tp, fp5, Tp5, Skewness)\n",
    "    \n",
    "    end    # calculate_frequency_domain_parameters()\n",
    "\n",
    "\n",
    "function calc_representative_spectra(frequency,Hm0,Tp,gamma)\n",
    "##########################################    \n",
    "    \"\"\"\n",
    "    function to calculate representative spectrum based on the Jonswap formula in Tucker and Pitt p.339 (10.3-9a)\n",
    "\n",
    "    inputs:\n",
    "        frequency - array of spectral frequencies\n",
    "        Hm0 - floating point value\n",
    "        Tp - floating point value\n",
    "        gamma - floating point value - Peak ehhancement factor (Enter 1 for PM, or 3.3 for Jonswap)\n",
    "\n",
    "        Typical calls:\n",
    "        Spectra_PM = calc_representative_spectra(f2, Hm0, Tp, 1.0)\n",
    "        Spectra_JONSWAP = calc_representative_spectra(f2, Hm0, Tp, 3.3)\n",
    "\n",
    "    returns:\n",
    "        Sf - array of representative spectra        \n",
    "    \"\"\"\n",
    "\n",
    "    alpha = 1    # initial Philips constant (will decrease for each iteration required)\n",
    "    g = 9.81\n",
    "    fp = 1/Tp    # peak frequency\n",
    "\n",
    "    hm0 = 99.    # set this to large value (so it will change on first iteration)\n",
    "\n",
    "    Sf = [];\n",
    "\n",
    "    while((Hm0 - hm0) <= 0.0005)\n",
    "\n",
    "        Sf = vcat([alpha*g^2 * (2*pi)^-4 * ff^-5 * exp(-1.25 * (ff/fp)^-4) * gamma^exp(-(ff-fp)^2/(2*0.07^2 * fp^2)) for ff in frequency[findall(<=(fp), frequency)]],\n",
    "                [alpha*g^2 * (2*pi)^-4 * ff^-5 * exp(-1.25 * (ff/fp)^-4) * gamma^exp(-(ff-fp)^2/(2*0.09^2 * fp^2)) for ff in frequency[findall(>(fp), frequency)]]);\n",
    "        Sf[1] = 0;\n",
    "\n",
    "###################################################################################################################################            \n",
    "###  See discussion at https://stackoverflow.com/questions/44915116/how-to-decide-between-scipy-integrate-simps-or-numpy-trapz  ###\n",
    "###################################################################################################################################\n",
    "\n",
    "        ##        hm0 = 4*(np.trapz(Sf, frequency))^0.5    # calculate new Hm0 based on Sf values\n",
    "        hm0 = calc_hm0(Sf,frequency);   # see https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.integrate.simps.html        \n",
    "        alpha *= 0.95;    # reduce alpha by 5% so iterations approach a limit of 0.0005\n",
    "\n",
    "    end\n",
    "\n",
    "    return(Sf)\n",
    "        \n",
    "    end    # calc_representative_spectra()\n",
    "\n",
    "\n",
    "function plot_spectra(wse_df)\n",
    "################################################\n",
    "    \n",
    "    heave = wse_df.Heave\n",
    "    Sample_frequency = 1.28\n",
    "    \n",
    "    # convert heave to matrix of individual 256-value spectra\n",
    "    segments = Periodograms.arraysplit(heave, 512, 256)\n",
    "    combined_segments = []\n",
    "    \n",
    "    for i in eachindex(segments)\n",
    "        push!(combined_segments,power(periodogram(segments[i],nfft=512,fs=Sample_frequency,window=hanning)))\n",
    "    end\n",
    "    \n",
    "    global freqs1 = freq(periodogram(segments[1],nfft=512,fs=Sample_frequency,window=hanning))\n",
    "    global Pden = mean(combined_segments, dims = 1)\n",
    "\n",
    "    # use Welch's method as a check\n",
    "    global ps_w = welch_pgram(heave, 512, 256; onesided=true, nfft=512, fs=Sample_frequency, window=hanning);\n",
    "    global f2 = freq(ps_w);\n",
    "    global Pden2 = power(ps_w);\n",
    "\n",
    "\n",
    "    Hm0, Hrms, T01, T02, Tc, Tp, fp5, Tp5, Skewness = calculate_frequency_domain_parameters(f2, Pden2)\n",
    "    @printf(\"%s; Hm0 = %5.2fm; Hrms = %5.2fm; T01 = %5.2fs; T02 = %5.2fs; Tc = %5.2fs; Tp = %5.2fs; Tp5 = %5.2fs; Skewness = %5.4f\",\n",
    "        Dates.format(first(wse_df.Date), \"yyyy-mm-dd HH:MM\"),Hm0, Hrms, T01, T02, Tc, Tp, Tp5, Skewness)\n",
    "    \n",
    "    # Calculate representative spectra for P-M and JONSWAP\n",
    "    Spectra_PM = calc_representative_spectra(f2, Hm0, Tp, 1.0);\n",
    "    Spectra_JONSWAP = calc_representative_spectra(f2, Hm0, Tp, 3.3);\n",
    "\n",
    "    # determing maximum y-axis value for spectral plots\n",
    "    max_y = maximum([maximum(Pden[1]),maximum(Pden2),maximum(Spectra_JONSWAP)]) * 1.05\n",
    "    \n",
    "    if max_y < 0.1\n",
    "        tick_val = 0.01\n",
    "    elseif  max_y < 1\n",
    "        tick_val = 0.1\n",
    "    elseif max_y < 10\n",
    "        tick_val = 1   \n",
    "    else\n",
    "        tick_val = 5\n",
    "    end\n",
    "\n",
    "    # Plot the representative spectra\n",
    "    p_spectra = plot(f2,Spectra_JONSWAP, lw=2, c=:lightblue, label=\"JONSWAP spectrum (\" * L\"\\gamma\" * \" = 3.3)\")\n",
    "    p_spectra = plot!(f2,Spectra_PM, lw=2, c=:lightgreen, label=\"Pierson-Moskowitz spectrum (\" * L\"\\gamma\" * \" = 1.0)\\n\")\n",
    "    \n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    \n",
    "    # Add frequency-domain parameters to plot\n",
    "    x_lim = xlims(p_spectra)[1]; y_lim = ylims(p_spectra)[2]\n",
    "    p_spectra = annotate!(x_lim*-25, y_lim*0.65, \"Hm0 = \" * string(round(Hm0, digits=2)) * \"m\",annotationfontsize=10) \n",
    "    p_spectra = annotate!(x_lim*-25, y_lim*0.60, \"Hrms = \" * string(round(Hrms, digits=2)) * \"m\") \n",
    "    p_spectra = annotate!(x_lim*-25, y_lim*0.55, \"T01 = \" * string(round(T01, digits=2)) * \"s\") \n",
    "    p_spectra = annotate!(x_lim*-25, y_lim*0.50, \"T02 = \" * string(round(T02, digits=2)) * \"s\") \n",
    "    p_spectra = annotate!(x_lim*-25, y_lim*0.45, \"Tp = \" * string(round(Tp, digits=2)) * \"s\") \n",
    "    p_spectra = annotate!(x_lim*-25, y_lim*0.40, \"Tp5 = \" * string(round(Tp5, digits=2)) * \"s\") \n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    \n",
    "    \n",
    "    \n",
    "    # plot calculated spectra\n",
    "    p_spectra = plot!(freqs1, Pden, label=\"Calc\\n\", \n",
    "        c=:yellow, lw=3, fillrange = 0, fillalpha = 0.05, fillcolor = :blue)\n",
    "        \n",
    "    # plot Welch's spectra\n",
    "    p_spectra = plot!(f2, Pden2, label=\"Welch's method\", \n",
    "        c=:red, lw=1, fillrange = 0, fillalpha = 0.05, fillcolor = :red)\n",
    "    \n",
    "    p_spectra = vline!([fp5; fp5], lw=1, ls =:dash, c=:red, label=\"Tp5\")\n",
    "    \n",
    "    plot_spc = Plots.plot(p_spectra, layout = (1, 1), size = (1400, 600), framestyle = :box, \n",
    "        xlim=(0,0.64),  xticks = 0:0.05:1.28, xtickfontsize=7, ytickfontsize=8, xlabel=\"Frequency (Hertz)\",\n",
    "        ylim=(0,max_y), yticks=0:tick_val:max_y, ylabel=\"Spectral Density (sq.m/Hertz)\",\n",
    "        fg_legend=:transparent, title = \" Spectral plot\", titlefontsize=12,\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, \n",
    "        grid=true, gridlinewidth=0.5, gridalpha=1, foreground_color_grid=\"lightgrey\")            \n",
    "\n",
    "    display(plot_spc)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # plot_spectra()\n",
    "\n",
    "\n",
    "function plot_spectrogram(wse_df)\n",
    "    \n",
    "    heave = wse_df.Heave;\n",
    "    nw=256;\n",
    "    spec = DSP.Periodograms.spectrogram(heave, nw, 250; fs=1.28,window=hanning);\n",
    "\n",
    "    # display plots to screen\n",
    "    tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "    spec1 = plot(first(wse_df.Date) + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, DSP.Periodograms.power(spec), lw=1, c=cgrad(:Spectral, rev=true), colorbar=false, \n",
    "        size=(1400, 600), framestyle = :box, title=\"Spectrogram\", \n",
    "        xlim=(first(wse_df.Date),last(wse_df.Date)), xticks=(tm_tick,ticks), xtickfontsize=7, xlabel=\"Time (s)\",\n",
    "        ytickfontsize=8, ylabel=\"Frequency (Hz)\",\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, show=true) \n",
    "\n",
    "    display(spec1)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # plot_spectrogram()\n",
    "\n",
    "\n",
    "function process_spectrum_file(df)\n",
    "                                \n",
    "    global is_gps = false\n",
    "    sync_word_location = findall(x -> x == \"7FFF\", df.Column2)\n",
    "\n",
    "    for j in sync_word_location\n",
    "\n",
    "        i = df.Column2[j+1]\n",
    "        word_number = parse(Int, SubString.(i, 1, 1), base=16)*16^0\n",
    "        word = parse(Int, SubString.(i, 2, 2), base=16)*16^2 + parse(Int, SubString.(i, 3, 3), base=16)*16^1 + parse(Int, SubString.(i, 4, 4), base=16)*16^0\n",
    "    #    println(j,' ',word_number,' ',word)\n",
    "\n",
    "        # Test whether buoy is MkIII or DWR-G - see p.51 Table 5.7.5a. Organization and significance of the system file data \n",
    "        # If DWR-G:\n",
    "        #     Av0 = 0; Ax0 = 0; Ay0 = 0; O = 0; and Inclination = 0\n",
    "        \n",
    "        if (word_number == 7 && word == 0)\n",
    "            is_gps = true\n",
    "        end\n",
    "\n",
    "    end\n",
    "    \n",
    "    return(is_gps)\n",
    "    \n",
    "    end    # process_spectrum_file()\n",
    "\n",
    "\n",
    "################################################\n",
    "################################################\n",
    "##           START OF MAIN PROGRAM\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "hxv_directory = pick_folder()\n",
    "\n",
    "# build list of all hxv files in selected directory\n",
    "hxv_files = filter(x->occursin(\".hxv\",x), readdir(hxv_directory));\n",
    "hxv_files = hxv_files[findall(x->endswith(uppercase(x), \".HXV\"), hxv_files)];\n",
    "\n",
    "w = Toplevel(\"Select Date\", 235, 400)\n",
    "tcl(\"pack\", \"propagate\", w, false)\n",
    "f = Frame(w)\n",
    "pack(f, expand=true, fill=\"both\")\n",
    "\n",
    "f1 = Frame(f)\n",
    "lb = Treeview(f1, hxv_files)\n",
    "scrollbars_add(f1, lb)\n",
    "pack(f1,  expand=true, fill=\"both\")\n",
    "\n",
    "tcl(\"ttk::style\", \"configure\", \"TButton\", foreground=\"blue\", font=\"arial 16 bold\")\n",
    "b = Button(f, \"Ok\")\n",
    "pack(b)\n",
    "\n",
    "bind(b, \"command\") do path\n",
    "    \n",
    "    global file_choice = get_value(lb);\n",
    "    \n",
    "    # Select a HXV file\n",
    "    global infil = hxv_directory * \"\\\\\" * file_choice[1]\n",
    "    println(\"Selected \",infil)\n",
    "\n",
    "    df = DataFrame(CSV.File(infil,header=0, delim=\",\", types=String));\n",
    "\n",
    "#==\n",
    "    # extract the datetime from the file name\n",
    "    date_str = split(infil,\".\")[1]\n",
    "    ll = length(date_str)\n",
    "    start_date = DateTime.(date_str[ll-16:ll-1], \"yyyy-mm-ddTHHhMMZ\")\n",
    "==#\n",
    "    # Extract the date and time string and convert to DateTime object\n",
    "    date_time_str = match(r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}h\\d{2}\", infil).match\n",
    "    start_date = DateTime(date_time_str, \"yyyy-mm-ddTHHhMM\")\n",
    "    \n",
    "    # create df of 2304 rows, each 0.78s apart\n",
    "    wse_df = DataFrame(Date = unix2datetime.(datetime2unix.(start_date) .+ (0:1/1.28:1800-1/1.28)), \n",
    "        Heave = zeros(2304), \n",
    "        North = zeros(2304), \n",
    "        West = zeros(2304));\n",
    "\n",
    "    # populate the df based on sequence numbers\n",
    "    global wse_df = calc_wse(df, wse_df, start_date)\n",
    "    is_gps = process_spectrum_file(df)\n",
    "\n",
    "    plot_wses(df, wse_df,is_gps)\n",
    "#    plot_spectrogram(wse_df)\n",
    "    plot_spectra(wse_df)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary test code for optimization - JW September 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, CSV\n",
    "\n",
    "function get_displacements(arry)\n",
    "    #####################################\n",
    "    \n",
    "    n = length(arry[1])\n",
    "    displacements = Vector{Int}(undef, length(arry))\n",
    "\n",
    "    for (idx, i) in enumerate(arry)\n",
    "        if n == 3\n",
    "            displacements[idx] = parse(Int, SubString(i, 1, 1), base=16) * 16^2 +\n",
    "                                 parse(Int, SubString(i, 2, 2), base=16) * 16^1 +\n",
    "                                 parse(Int, SubString(i, 3, 3), base=16)\n",
    "        else\n",
    "            displacements[idx] = parse(Int, SubString(i, 1, 1), base=16) * 16^1 +\n",
    "                                 parse(Int, SubString(i, 2, 2), base=16) * 16^0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Adjust displacements for values >= 2048\n",
    "    idxs = findall(>=(2048), displacements)\n",
    "    displacements[idxs] .= 2048 .- displacements[idxs]\n",
    "\n",
    "    return(displacements ./ 100)\n",
    "end  # get_displacements()\n",
    "\n",
    "function get_HNW(infil)\n",
    "    #####################################\n",
    "        \n",
    "    df = DataFrame(CSV.File(infil, header=0, delim=\",\", types=String))\n",
    "\n",
    "    # Calculate sequence numbers\n",
    "    arry = SubString.(df.Column1, 3, 4)\n",
    "    sequence = [parse(Int, SubString(i, 1, 1), base=16) * 16^1 + parse(Int, SubString(i, 2, 2), base=16) for i in arry]\n",
    "\n",
    "    # Calculate heave, north, and west WSEs\n",
    "    heave = get_displacements(SubString.(df.Column3, 1, 3))\n",
    "    north = get_displacements(SubString.(df.Column3, 4, ) .* SubString.(df.Column4, 1, 2))\n",
    "    west = get_displacements(SubString.(df.Column4, 3, 4) .* SubString.(df.Column5, 1, 1))\n",
    "\n",
    "    return heave, north, west, sequence\n",
    "end  # get_HNW()\n",
    "\n",
    "\n",
    "heave, north, west, sequence = get_HNW(infil)\n",
    "\n",
    "# Identify any gaps in the recorded data\n",
    "tt = [0]\n",
    "append!(tt, diff(sequence))\n",
    "tt[tt .< 0] .+= 256\n",
    "tt1 = cumsum(tt)\n",
    "\n",
    "# Limit to a maximum of 2304 entries\n",
    "if length(tt1) > 2304\n",
    "    tt1 = tt1[1:2304]\n",
    "end\n",
    "\n",
    "# Assign values to wse_df based on tt1\n",
    "for i in eachindex(tt1)\n",
    "    wse_df[tt1[i] + 1, 2] = heave[i]\n",
    "    wse_df[tt1[i] + 1, 3] = north[i]\n",
    "    wse_df[tt1[i] + 1, 4] = west[i]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved version of extracting WSE's from df and handling gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames: DataFrame\n",
    "using Dates: DateTime, unix2datetime\n",
    "using DSP: welch_pgram, freq, power, hanning\n",
    "using NativeFileDialog: pick_folder\n",
    "\n",
    "##import DataFrames: Not, select!\n",
    "\n",
    "# Function to convert frequency in Hertz to Period in Seconds\n",
    "function convert_frequency_to_period(frequencies)\n",
    "#################################################\n",
    "    \n",
    "return(1.0 ./ frequencies)\n",
    "    \n",
    "end    # convert_frequency_to_period()\n",
    "\n",
    "\n",
    "################################################\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "rec_len = 2304\n",
    "sample_frequency = 1.28 # sample frequency in Hertz\n",
    "sample_length = 1800 # record length in seconds\n",
    "sample_rate = Float64(1/sample_frequency) # sample spacing in seconds\n",
    "\n",
    "global infil = hxv_directory * \"\\\\\" * file_choice[1]\n",
    "    println(\"Selected \",infil)\n",
    "\n",
    "date_str = split(infil,\".\")[1]\n",
    "ll = length(date_str)\n",
    "start_date = DateTime.(date_str[ll-16:ll-1], \"yyyy-mm-ddTHHhMMZ\")\n",
    "\n",
    "# Create df of dates and NaN's\n",
    "wse_df = DataFrame(\n",
    "    Date = unix2datetime.(datetime2unix.(start_date) .+ (0:sample_rate:sample_length - sample_rate)), \n",
    "    Heave = fill(NaN, rec_len),\n",
    "    North = fill(NaN, rec_len),\n",
    "    West = fill(NaN, rec_len),  \n",
    "    GPS_flag = fill(0, rec_len)  \n",
    ")\n",
    "\n",
    "# read HXV file to df\n",
    "df = DataFrame(CSV.File(infil, header=0, delim=\",\", types=String))\n",
    "\n",
    "# remove df rows where string length != 4\n",
    "filter!(row -> all(length(row[i]) == 4 for i in 1:ncol(df)), df)\n",
    "println(nrow(df),\" rows available for processing\")\n",
    "\n",
    "# determine if buoy is DWR-G\n",
    "is_gps = false\n",
    "\n",
    "sync_word_location = findall(==(\"7FFF\"), df.Column2)\n",
    "\n",
    "if !isempty(sync_word_location)  # Proceed only if we found any \"7FFF\"\n",
    "    next_row_data = df.Column2[sync_word_location .+ 1]\n",
    "    word_numbers = parse.(Int, SubString.(next_row_data, 1, 1), base=16)\n",
    "    words = parse.(Int, SubString.(next_row_data, 2, 4), base=16)\n",
    "    \n",
    "    if any((word_numbers .== 7) .& (words .== 0))\n",
    "        is_gps = true\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "is_gps ? println(\"GPS buoy\") : println(\"MkIII buoy\")   \n",
    "    \n",
    "hex_arr = SubString.(df.Column1, 3, 4)\n",
    "arr = parse.(Int, hex_arr, base=16)\n",
    "diffs = diff(arr)\n",
    "diffs[diffs .< 0] .+= 256\n",
    "cumulative_values = cumsum([1; diffs])\n",
    "valid_indices = findall(<=(rec_len), cumulative_values)\n",
    "valid_rows = cumulative_values[valid_indices]\n",
    "\n",
    "# truncate number of rows if > rec_len\n",
    "df = df[1:min(nrow(df), rec_len), :]\n",
    "\n",
    "# Calculate heave, north, and west WSEs\n",
    "wse_df[valid_rows, 2] .= get_displacements(SubString.(df.Column3, 1, 3))\n",
    "north_hex = SubString.(df.Column3, 4, ) .* SubString.(df.Column4, 1, 2)\n",
    "wse_df[valid_rows, 3] .= get_displacements(north_hex)\n",
    "wse_df[valid_rows, 4] .= get_displacements(SubString.(df.Column4, 3, 4) .* SubString.(df.Column5, 1, 1))\n",
    "\n",
    "\n",
    "# Function to check the LSB of a hexadecimal value\n",
    "check_lsb(hex_str) = parse(Int, hex_str, base=16) & 1 == 1 ? 1 : 0\n",
    "\n",
    "# Apply the function to each element in the array using map\n",
    "wse_df[valid_rows, 5] = Int16.(map(check_lsb, north_hex))\n",
    "\n",
    "ps_w = welch_pgram(wse_df.Heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "f2 = freq(ps_w)\n",
    "Pden2 = power(ps_w)\n",
    "\n",
    "p1 = plot()\n",
    "\n",
    "tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(5))\n",
    "ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "# Find indices of all values equal to 1 (represents Datawell GPS flag)\n",
    "gps_flag = findall(==(1), wse_df.GPS_flag)\n",
    "\n",
    "p1 = plot()\n",
    "\n",
    "# show GPS errors\n",
    "for jj in gps_flag\n",
    "    p1 = vline!([wse_df.Date[jj]], lw=1, c=:red, label=\"\")\n",
    "end\n",
    "\n",
    "# Plot the mean of the Noise Floor values\n",
    "periods_sec = convert_frequency_to_period(f2)\n",
    "# Create the two plots with specified sizes\n",
    "p1 = plot!(wse_df.Date, wse_df.Heave, lc=:blue, lw=:0.5, ylabel=\"WSE (m)\", label=\"\", xlims=(wse_df.Date[1],wse_df.Date[end]), xticks=(tm_tick,ticks))\n",
    "p2 = plot(f2, Pden2, lc=:blue, lw=:2, alpha=:0.75, fillrange=0, fillalpha=:0.125, label=\"\", xlim=(0,0.64), ylim=(0,Inf),\n",
    "        xlabel=\"Frequency (Hz)\", ylabel=\"S(f) (m²/Hz)\")\n",
    "p3 = plot(plot(periods_sec, Pden2, lw=:2, label=\"\", yaxis=:log, yminorticks=10, minorgrid=:true, xlabel= \"Wave Period (s)\", \n",
    "        ylabel=\"S(f) (m²/Hz)\"), xlims=(0,200))\n",
    "\n",
    "# Define the layout with varying sizes\n",
    "l = @layout [a{0.5h}; b{0.5w} c{0.5w} [ Plots.grid(1,1) ] ]\n",
    "\n",
    "title = Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "\n",
    "# Combine the three plots\n",
    "p1_p2_plot = plot(p1, p2, p3, framestyle = :box, leftmargin = 10Plots.mm, layout=l, suptitle=title, size=(1200, 800))\n",
    "\n",
    "display(p1_p2_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, Statistics\n",
    "\n",
    "# Function to identify runs of positive or negative values and their indices\n",
    "function identify_runs_with_indices(data::Vector{Float64})\n",
    "    runs = []\n",
    "    current_run = []\n",
    "    current_indices = []\n",
    "    for i in 1:length(data)\n",
    "        if isempty(current_run) || sign(data[i]) == sign(current_run[end])\n",
    "            push!(current_run, data[i])\n",
    "            push!(current_indices, i)\n",
    "        else\n",
    "            push!(runs, (current_run, current_indices))\n",
    "            current_run = [data[i]]\n",
    "            current_indices = [i]\n",
    "        end\n",
    "    end\n",
    "    push!(runs, (current_run, current_indices))  # Add the last run\n",
    "    return runs\n",
    "end\n",
    "\n",
    "# Identify runs in the Heave column\n",
    "runs_with_indices = identify_runs_with_indices(wse_df.Heave)\n",
    "\n",
    "# Analyze runs to find unusually long runs\n",
    "threshold = 10  # Define a threshold for what you consider a long run\n",
    "long_runs_with_indices = filter(run -> length(run[1]) > threshold, runs_with_indices)\n",
    "\n",
    "# Print the indices and values of long runs\n",
    "for (run, indices) in long_runs_with_indices\n",
    "    println(\"Long run detected: Length = $(length(run)), Start Index = $(indices[1]), End Index = $(indices[end]), Values = $run\")\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Replace NaNs with the mean of the non-NaN values\n",
    "wse_df.Heave = coalesce.(wse_df.Heave, mean(skipmissing(wse_df.Heave)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Filesystem: isfile\n",
    "using CSV: CSV\n",
    "using CurveFit: curve_fit\n",
    "using DataFrames: DataFrame, ncol, nrow\n",
    "using Dates: Dates, DateTime, unix2datetime, datetime2unix, Minute\n",
    "using DSP: welch_pgram, freq, power, hanning\n",
    "using NativeFileDialog: pick_folder\n",
    "using Serialization: deserialize\n",
    "using Statistics: median, var\n",
    "using Tk: bind, Button, destroy, Frame, get_value, pack, scrollbars_add, tcl, Toplevel, Treeview\n",
    "using Plots: Plots, plot, plot!, annotate!, vline!, @layout, text\n",
    "using Polynomials: Polynomial\n",
    "\n",
    "# Function to apply adaptive polynomial fit to WSE's affected by GPS errors\n",
    "# Includes: adaptive polynomial degree, dynamic window size, and handling of closely spaced errors.\n",
    "function fix_gps_errors(heave_bad, date, gps_flag)\n",
    "    \n",
    "    heave = deepcopy(heave_bad)\n",
    "    gps_errors = findall(==(1), gps_flag)\n",
    "    heave_length = length(heave)\n",
    "    \n",
    "    if !isempty(gps_errors)\n",
    "        \n",
    "        println(length(gps_errors), \" GPS errors at \", Dates.format.(date, \"yyyy-mm-dd HH:MM\"))\n",
    "        flush(stdout)\n",
    "\n",
    "        # Combine adjacent GPS errors into groups\n",
    "        grouped_errors = [gps_errors[1]]\n",
    "        for i in 2:length(gps_errors)\n",
    "            if gps_errors[i] - gps_errors[i-1] > 10  # e.g., if errors are > 10 points apart, treat them separately\n",
    "                push!(grouped_errors, gps_errors[i])\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for ii in reverse(grouped_errors)\n",
    "            error_center = ii\n",
    "\n",
    "            if error_center <= 3\n",
    "                error_center = 3\n",
    "            end\n",
    "\n",
    "            if error_center >= heave_length - 3\n",
    "                error_center = heave_length - 3\n",
    "            end\n",
    "\n",
    "            # Dynamic window size based on wave variability (variance)\n",
    "            # You can adjust this logic based on how variable the data is\n",
    "            region_variability = var(heave[max(1, error_center-120):min(heave_length, error_center+120)])\n",
    "            lower_offset = upper_offset = min(max(round(Int, 120 / sqrt(region_variability + 1)), 30), 120)\n",
    "\n",
    "            if error_center <= lower_offset\n",
    "                lower_offset = error_center - 1\n",
    "            end\n",
    "\n",
    "            if error_center + upper_offset > heave_length\n",
    "                upper_offset = heave_length - error_center\n",
    "            end\n",
    "\n",
    "            lower_offset = max(lower_offset, 2)\n",
    "            upper_offset = max(upper_offset, 2)\n",
    "\n",
    "            left_side_points = max(1, error_center - lower_offset):error_center\n",
    "            right_side_points = error_center:min(heave_length, error_center + upper_offset)\n",
    "\n",
    "            # Adaptive polynomial fitting by testing different degrees\n",
    "            best_fit_degree = 2\n",
    "            min_rss = Inf\n",
    "            \n",
    "            for degree in 1:4  # Test degrees from 1 to 4\n",
    "                fit1 = curve_fit(Polynomial, left_side_points, heave[left_side_points], degree)\n",
    "                fit2 = curve_fit(Polynomial, right_side_points, heave[right_side_points], degree)\n",
    "                rss = sum((heave[left_side_points] - fit1.(left_side_points)).^2) +\n",
    "                      sum((heave[right_side_points] - fit2.(right_side_points)).^2)\n",
    "                if rss < min_rss\n",
    "                    min_rss = rss\n",
    "                    best_fit_degree = degree\n",
    "                end\n",
    "            end\n",
    "\n",
    "            # Perform the best fit using the selected degree\n",
    "            fit1 = curve_fit(Polynomial, left_side_points, heave[left_side_points], best_fit_degree)\n",
    "            yfit1 = fit1.(left_side_points)\n",
    "            yfit1[end] = 0.0  # Set the last point of the left fit to 0\n",
    "\n",
    "            fit2 = curve_fit(Polynomial, right_side_points, heave[right_side_points], best_fit_degree)\n",
    "            yfit2 = fit2.(right_side_points)\n",
    "            yfit2[1] = 0.0  # Set the first point of the right fit to 0\n",
    "\n",
    "            # Apply polynomial results to WSEs on both sides of GPS error\n",
    "            heave[left_side_points] .= heave[left_side_points] - yfit1\n",
    "            heave[right_side_points] .= heave[right_side_points] - yfit2\n",
    "            heave[ii] = 0.0  # Set WSE at GPS error location to 0\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    return(heave)\n",
    "        \n",
    "end  # fix_gps_errors()\n",
    "\n",
    "\n",
    "# function to convert HEX values into WSEs\n",
    "function get_displacements(arry)\n",
    "#####################################\n",
    "    \n",
    "    displacements = []\n",
    "\n",
    "    if length(arry[1]) == 3\n",
    "    \n",
    "        for i in arry\n",
    "            append!(displacements,parse(Int, SubString.(i, 1, 1), base=16)*16^2 \n",
    "                + parse(Int, SubString.(i, 2, 2), base=16)*16^1 \n",
    "                + parse(Int, SubString.(i, 3, 3), base=16)*16^0)\n",
    "        end\n",
    "        \n",
    "    else\n",
    "        \n",
    "        for i in arry\n",
    "            append!(displacements,parse(Int, SubString.(i, 1, 1), base=16)*16^1 \n",
    "                + parse(Int, SubString.(i, 2, 2), base=16)*16^0)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    displacements[findall(>=(2048), displacements)] = 2048 .- displacements[findall(>=(2048), displacements)];\n",
    "    \n",
    "    return(displacements./100)\n",
    "    \n",
    "    end     # get_displacements()\n",
    "\n",
    "\n",
    "# Function to convert frequency in Hertz to Period in Seconds\n",
    "function convert_frequency_to_period(frequencies)\n",
    "#################################################\n",
    "    \n",
    "    return(1.0 ./ frequencies)\n",
    "    \n",
    "end    # convert_frequency_to_period()\n",
    "\n",
    "\n",
    "# read Noise Floor from saved .bin file\n",
    "function read_noise_floor_file(file_path)\n",
    "#########################################\n",
    "    \n",
    "    # Assuming the file contains a serialized DataFrame\n",
    "    open(file_path, \"r\") do io\n",
    "        return(deserialize(io))\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "function get_noise_floor()\n",
    "##########################\n",
    "    \n",
    "    # determine whether data is coming from JW's laptop or QGHL office computer\n",
    "    paths = [\n",
    "        \"C:\\\\Users\\\\Jim\\\\Julia_programs\\\\Datawell\\\\RDT_vector\\\\Data\\\\Noise_floor.bin\",\n",
    "        \"C:\\\\Users\\\\PC1\\\\Julia_programs\\\\Datawell\\\\RDT_vector\\\\Data\\\\Noise_floor.bin\"\n",
    "    ]\n",
    "    \n",
    "    noise_floor_file = nothing\n",
    "    \n",
    "    for path in paths\n",
    "        if isfile(path)\n",
    "            noise_floor_file = path\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if isnothing(noise_floor_file)\n",
    "        println(\"File not found in any of the provided paths. Exiting program.\")\n",
    "        flush(stdout)\n",
    "        exit(1)\n",
    "    end\n",
    "    \n",
    "    println(\"Reading Noise Floor data from \", noise_floor_file)\n",
    "    \n",
    "    # Deserialize the DataFrame from the file\n",
    "    noise_floors_df = read_noise_floor_file(noise_floor_file)\n",
    "    \n",
    "    # Extract all spectral arrays from the DataFrame\n",
    "    spectral_values = noise_floors_df.Pden2\n",
    "    \n",
    "    # Convert the list of arrays into a matrix where each row is a spectrum\n",
    "    spectral_matrix = hcat(spectral_values...)'\n",
    "    \n",
    "    # Calculate the median spectra (median of each column)\n",
    "    median_spectra = median(spectral_matrix, dims=1)\n",
    "    \n",
    "    # Convert the results to vectors\n",
    "    median_spectra_vector = vec(median_spectra)\n",
    "\n",
    "    return(median_spectra_vector)\n",
    "\n",
    "end    # get_noise_floor() \n",
    "\n",
    "\n",
    "################################################\n",
    "################################################\n",
    "##           START OF MAIN PROGRAM\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))\n",
    "\n",
    "rec_len = 2304\n",
    "sample_frequency = 1.28 # sample frequency in Hertz\n",
    "sample_length = 1800 # record length in seconds\n",
    "sample_rate = Float64(1/sample_frequency) # sample spacing in seconds\n",
    "\n",
    "#using Logging: NullLogger, with_logger\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "global median_spectra_vector = get_noise_floor()\n",
    "\n",
    "hxv_directory = pick_folder()\n",
    "\n",
    "# build list of all hxv files in selected directory\n",
    "hxv_files = filter(x->occursin(\".hxv\",x), readdir(hxv_directory));\n",
    "hxv_files = hxv_files[findall(x->endswith(uppercase(x), \".HXV\"), hxv_files)];\n",
    "\n",
    "w = Toplevel(\"Select Date\", 235, 800)\n",
    "tcl(\"pack\", \"propagate\", w, false)\n",
    "f = Frame(w)\n",
    "pack(f, expand=true, fill=\"both\")\n",
    "\n",
    "f1 = Frame(f)\n",
    "lb = Treeview(f1, hxv_files)\n",
    "scrollbars_add(f1, lb)\n",
    "pack(f1, expand=true, fill=\"both\")\n",
    "\n",
    "tcl(\"ttk::style\", \"configure\", \"TButton\", foreground=\"blue\", font=\"arial 16 bold\")\n",
    "b = Button(f, \"Exit\")\n",
    "pack(b)\n",
    "\n",
    "infil_ref = Ref(\"\")\n",
    "\n",
    "function handle_selection(infil_ref)\n",
    "####################################\n",
    "    \n",
    "    file_choice = get_value(lb)\n",
    "    infil_ref[] = hxv_directory * \"\\\\\" * file_choice[1]\n",
    "    println(\"Selected \", infil_ref[])\n",
    "    flush(stdout)\n",
    "\n",
    "    infil = infil_ref[]\n",
    "\n",
    "    df = DataFrame(CSV.File(infil,header=0, delim=\",\", types=String));\n",
    "\n",
    "    # extract the datetime from the file name\n",
    "    date_str = split(infil,\".\")[1]\n",
    "    ll = length(date_str)\n",
    "    start_date = DateTime.(date_str[ll-16:ll-1], \"yyyy-mm-ddTHHhMMZ\")\n",
    "\n",
    "    # Create df of dates and NaN's\n",
    "    global wse_df = DataFrame(\n",
    "        Date = unix2datetime.(datetime2unix.(start_date) .+ (0:sample_rate:sample_length - sample_rate)), \n",
    "        Heave = fill(NaN, rec_len),\n",
    "        North = fill(NaN, rec_len),\n",
    "        West = fill(NaN, rec_len),  \n",
    "        GPS_flag = fill(0, rec_len)  \n",
    "    )\n",
    "    \n",
    "    # read HXV file to df\n",
    "    df = DataFrame(CSV.File(infil, header=0, delim=\",\", types=String))\n",
    "    \n",
    "    # remove df rows where string length != 4\n",
    "    filter!(row -> all(length(row[i]) == 4 for i in 1:ncol(df)), df)\n",
    "    println(nrow(df),\" rows available for processing\")\n",
    "    \n",
    "    # determine if buoy is DWR-G\n",
    "    global is_gps = false\n",
    "    \n",
    "    sync_word_location = findall(==(\"7FFF\"), df.Column2)\n",
    "    \n",
    "    if !isempty(sync_word_location)  # Proceed only if we've found any \"7FFF\"\n",
    "\n",
    "        next_row_data = df.Column2[sync_word_location[findall(<(rec_len-1),sync_word_location)] .+ 1]\n",
    "        word_numbers = parse.(Int, SubString.(next_row_data, 1, 1), base=16)\n",
    "        words = parse.(Int, SubString.(next_row_data, 2, 4), base=16)\n",
    "\n",
    "        if any((word_numbers .== 7) .& (words .== 0))\n",
    "            is_gps = true\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    hex_arr = SubString.(df.Column1, 3, 4)\n",
    "    arr = parse.(Int, hex_arr, base=16)\n",
    "    diffs = diff(arr)\n",
    "    diffs[diffs .< 0] .+= 256\n",
    "    cumulative_values = cumsum([1; diffs])\n",
    "    valid_indices = findall(<=(rec_len), cumulative_values)\n",
    "    valid_rows = cumulative_values[valid_indices]\n",
    "    \n",
    "    # truncate number of rows if > rec_len\n",
    "    df = df[1:min(nrow(df), rec_len), :]\n",
    "    \n",
    "    # Calculate heave, north, and west WSEs\n",
    "    wse_df[valid_rows, 2] .= get_displacements(SubString.(df.Column3, 1, 3))\n",
    "    north_hex = SubString.(df.Column3, 4, ) .* SubString.(df.Column4, 1, 2)\n",
    "    wse_df[valid_rows, 3] .= get_displacements(north_hex)\n",
    "    wse_df[valid_rows, 4] .= get_displacements(SubString.(df.Column4, 3, 4) .* SubString.(df.Column5, 1, 1))\n",
    "       \n",
    "    # Function to check the LSB of a hexadecimal value\n",
    "    check_lsb(hex_str) = parse(Int, hex_str, base=16) & 1 == 1 ? 1 : 0\n",
    "    wse_df[valid_rows, 5] = Int16.(map(check_lsb, north_hex))\n",
    "\n",
    "    global original_df = deepcopy(wse_df);\n",
    "\n",
    "    # need to replace any NaN's with 0's in order to calculate spectra\n",
    "    replace_nan(v) = map(x -> isnan(x) ? zero(x) : x, v)\n",
    "    heave = map(replace_nan, wse_df.Heave)\n",
    "    \n",
    "    ps_w = welch_pgram(heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "    f2 = freq(ps_w)\n",
    "    Pden2 = power(ps_w)\n",
    "    \n",
    "    # convert frequency (Hz) to period (s)\n",
    "    periods_sec = convert_frequency_to_period(f2)\n",
    "\n",
    "    tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "    \n",
    "    # Determine if GPS data should be fully plotted or only the first lines\n",
    "    if is_gps\n",
    "        \n",
    "        println(\"GPS buoy\")\n",
    "\n",
    "        # Find indices of all values equal to 1 (represents Datawell GPS flag)\n",
    "        gps_flag = findall(==(1), wse_df.GPS_flag)\n",
    "        gps_errors_count = length(gps_flag)\n",
    "           \n",
    "        gps_errors_count > 0 ? error_string = string(length(gps_flag),\" GPS errors flagged\") : error_string = \"No GPS errors flagged\"\n",
    "    \n",
    "        fixed_heave = fix_gps_errors(heave, wse_df.Date[1],  wse_df.GPS_flag)\n",
    "        ps_w1 = welch_pgram(fixed_heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "        f2_fixed = freq(ps_w1)\n",
    "        Pden2_fixed = power(ps_w1)\n",
    "        \n",
    "        # Plot for GPS buoy (all lines)\n",
    "        p1 = plot(wse_df.Date, heave, lc=:yellow, lw=:0.5, ylabel=\"WSE (m)\", label=\"\", \n",
    "                  xlims=(wse_df.Date[1],wse_df.Date[end]), ylims=(minimum(heave), maximum(heave)), xticks=(tm_tick,ticks))\n",
    "        \n",
    "        # Show GPS errors identified by Datawell\n",
    "        for jj in gps_flag\n",
    "            p1 = vline!([wse_df.Date[jj]], lw=1, c=:red, label=\"\")\n",
    "        end\n",
    "        \n",
    "        p1 = plot!(wse_df.Date, fixed_heave, lc=:blue, lw=:1, alpha=:0.75, label=\"\")\n",
    "        p1 = annotate!(wse_df.Date[50], maximum(heave)*0.9, text(error_string, :left, 12))\n",
    "        \n",
    "        # p2 and p3 plots\n",
    "        p2 = plot(f2, Pden2, lc=:yellow, lw=:2, alpha=:1, xlim=(0,0.64), ylim=(0,Inf), label=\"\", xlabel=\"Frequency (Hz)\", \n",
    "            ylabel=\"S(f) (m²/Hz)\", fg_legend=:transparent, bg_legend=:transparent)\n",
    "        p2 = plot!(f2_fixed, Pden2_fixed, lc=:blue, lw=:0.75, alpha=0.75, fillrange=0, fillcolor=:blue, fillalpha=0.1, label=\"\")\n",
    "        p2 = plot!(f2, median_spectra_vector, lw=:2, lc=:red, fillrange=0, fillalpha=0.075, fillcolor=:red, label=\"Median Noise Floor\")\n",
    "    \n",
    "        p3 = plot(plot(periods_sec, Pden2, lc=:yellow, lw=:2, label=\"\", yaxis=:log, yminorticks=10, minorgrid=:true, xlabel= \"Wave Period (s)\", \n",
    "            ylabel=\"S(f) (m²/Hz)\"), xlims=(0,200), legend=:bottomright, fg_legend=:transparent, bg_legend=:transparent)\n",
    "        p3 = plot!(periods_sec, Pden2_fixed, lc=:blue, lw=:0.75, label=\"\")\n",
    "        p3 = plot!(periods_sec, median_spectra_vector, lw=:2, lc=:red, fillrange=0.00001, fillalpha=0.075, fillcolor=:red, label=\"Median Noise Floor\")\n",
    "    \n",
    "    else\n",
    "        \n",
    "        println(\"MkIII buoy\")\n",
    "    \n",
    "        # Plot for MkIII buoy (first lines only)\n",
    "        p1 = plot(wse_df.Date, heave, lc=:blue, lw=:0.5, ylabel=\"WSE (m)\", label=\"\", \n",
    "                  xlims=(wse_df.Date[1],wse_df.Date[end]), ylims=(minimum(heave), maximum(heave)), xticks=(tm_tick,ticks))\n",
    "    \n",
    "        # p2 and p3 plots - plot only first line of each\n",
    "        p2 = plot(f2, Pden2, lc=:blue, lw=:2, alpha=1, fillrange=0, fillcolor=:blue, fillalpha=0.1, xlim=(0,0.64), \n",
    "            ylim=(0,Inf), label=\"\", xlabel=\"Frequency (Hz)\", ylabel=\"S(f) (m²/Hz)\", fg_legend=:transparent, bg_legend=:transparent)\n",
    "        p2 = vline!([1/30], lc=:red,lw=:2,ls=:dash,label=\"Datawell MkIII 30s cut-off\")\n",
    "    \n",
    "        p3 = plot(periods_sec, Pden2, lc=:blue, lw=:2, minorgrid=:true, label=\"\", xlabel=\"Wave Period (s)\", \n",
    "            ylabel=\"S(f) (m²/Hz)\", xlims=(0,50), fg_legend=:transparent, bg_legend=:transparent)\n",
    "        p3 = vline!([30], lc=:red,lw=:2,ls=:dash,label=\"Datawell MkIII 30s cut-off\")\n",
    "    end\n",
    "\n",
    "    # Combine the three plots as before\n",
    "    l = @layout [a{0.5h}; b{0.5w} c{0.5w} [Plots.grid(1,1)]]\n",
    "    title = Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "    \n",
    "    p1_p2__p3_plot = plot(p1, p2, p3, framestyle = :box, leftmargin = 10Plots.mm, layout=l, suptitle=title, size=(1200, 800))\n",
    "\n",
    "    # Extract the date part from infil\n",
    "    filename = split(infil, \"\\\\\") |> last\n",
    "    date_part = split(filename, \"T\")[1]\n",
    "    \n",
    "    # Extract the time part from title and format it\n",
    "    time_part = replace(split(title, \" \")[2], \":\" => \"\")\n",
    "    \n",
    "    # Combine the parts into the desired format\n",
    "    output_filename = \".\\\\Plots\\\\\" * date_part * \"_\" * time_part * \".png\"\n",
    "    \n",
    "##    savefig(output_filename) \n",
    "    \n",
    "    display(p1_p2__p3_plot)\n",
    "    \n",
    "end\n",
    "\n",
    "function exit_callback()\n",
    "########################\n",
    "    \n",
    "    destroy(w)  # Close the window when Exit button is pressed\n",
    "\n",
    "end\n",
    "\n",
    "bind(b, \"command\") do path\n",
    "    \n",
    "    exit_callback()\n",
    "    \n",
    "end\n",
    "\n",
    "# Bind double-click event to the Treeview\n",
    "bind(lb, \"<Double-1>\") do event\n",
    "    \n",
    "    handle_selection(infil_ref)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate method of locating GPS errors without flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using CurveFit: LogFit\n",
    "\n",
    "x = Dates.datetime2julian.(wse_df.Date)\n",
    "y = wse_df.Heave\n",
    "\n",
    "fit1 = curve_fit(LogFit, x, y)\n",
    "yfit1 = fit1.(x)\n",
    "\n",
    "p1 = plot(wse_df.Date,wse_df.Heave, label=\"Original data\")\n",
    "p1 = plot!(wse_df.Date,yfit1, label = \"Curve fit\")\n",
    "\n",
    "# Locate maximum run of negative values + 1 to identify center of GPS error\n",
    "negatives = findall(x->x<0, wse_df.Heave)\n",
    "neg_max = wse_df.Date[(negatives)[findmax(diff(negatives))[2]]+1]\n",
    "p1 = vline!([neg_max],lw=:1,color=:red,z_order=:1,label=neg_max)\n",
    "\n",
    "# display plots to screen\n",
    "tm_tick = range(first(wse_df.Date), last(wse_df.Date), step=Minute(5))\n",
    "ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "title = Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "fix_gps = plot(p1,size = (1600, 600), title=title, titlefontsize=10, framestyle = :box, fg_legend=:transparent, bg_legend=:transparent, legend=:topright,\n",
    "                xlim=(first(wse_df.Date),last(wse_df.Date)), bottommargin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1,xticks=(tm_tick,ticks))\n",
    "\n",
    "display(fix_gps)\n",
    "\n",
    "\n",
    "error_center = findfirst(x -> x == wse_df.Date[(negatives)[findmax(diff(negatives))[2]]+1], wse_df.Date)\n",
    "ii = error_center\n",
    "heave = deepcopy(wse_df.Heave)\n",
    "\n",
    "# User-selected offset either side of GPS error\n",
    "lower_offset = upper_offset = 50\n",
    "\n",
    "if error_center <= lower_offset\n",
    "    lower_offset = error_center - 1\n",
    "end\n",
    "\n",
    "if error_center+upper_offset > 2304\n",
    "    upper_offset = 2304 - error_center\n",
    "end\n",
    "\n",
    "# Fit curve to subset of heave before GPS error\n",
    "left_side_points = error_center-lower_offset:error_center\n",
    "fit1 = curve_fit(Polynomial, left_side_points, heave[left_side_points], 2)\n",
    "yfit1 = fit1.(left_side_points)\n",
    "yfit1[length(yfit1)] = 0.0\n",
    "\n",
    "# Fit curve to subset of heave after GPS error\n",
    "right_side_points = error_center:error_center+upper_offset\n",
    "fit2 = curve_fit(Polynomial, right_side_points, heave[right_side_points], 2)\n",
    "yfit2 = fit2.(right_side_points)\n",
    "yfit2[1] = 0.0\n",
    "\n",
    "# apply polynomial results to wse's on both sides of GPS error\n",
    "heave[left_side_points] .= heave[left_side_points] - yfit1\n",
    "heave[right_side_points] .= heave[right_side_points] - yfit2\n",
    "heave[ii] = 0.0    # set wse at GPS error location to 0\n",
    "\n",
    "p2 = plot(wse_df.Date, wse_df.Heave, color=:yellow, label=\"Original data\")\n",
    "p2 = plot!(wse_df.Date, heave, color=:blue, label=\"Corrected data\")\n",
    "\n",
    "fixed_gps = plot(p2,size = (1600, 600), title=title, titlefontsize=10, framestyle = :box, fg_legend=:transparent, bg_legend=:transparent, legend=:topright,\n",
    "                xlim=(first(wse_df.Date),last(wse_df.Date)), bottommargin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1,xticks=(tm_tick,ticks))\n",
    "\n",
    "display(fixed_gps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate method of locating GPS errors without flag\n",
    "########################################################\n",
    "\n",
    "using Base.Filesystem: isfile\n",
    "using CSV: CSV\n",
    "using CurveFit: curve_fit, Polynomial\n",
    "using DataFrames: DataFrame, ncol, nrow\n",
    "using Dates: Dates, DateTime, unix2datetime, datetime2unix, Minute\n",
    "using DSP: welch_pgram, freq, power, hanning\n",
    "using NativeFileDialog: pick_folder\n",
    "using Serialization: deserialize\n",
    "using Statistics: median, var\n",
    "using Tk: bind, Button, destroy, Frame, get_value, pack, scrollbars_add, tcl, Toplevel, Treeview\n",
    "using Plots: Plots, plot, plot!, annotate!, vline!, @layout, text\n",
    "using Polynomials: Polynomial\n",
    "\n",
    "function find_runs_of_errors(heave_data, min_run_length)\n",
    "########################################################\n",
    "    \n",
    "    # Create a boolean array where true indicates a suspected error (based on heave being negative)\n",
    "    suspected_errors = [x < 0 for x in heave_data]  # Detect negatives; adjust for positives if needed\n",
    "\n",
    "    # Find all contiguous runs of true values (negative heave)\n",
    "    runs_of_errors = []\n",
    "    current_run = []\n",
    "\n",
    "    for (i, is_error) in enumerate(suspected_errors)\n",
    "        \n",
    "        if is_error\n",
    "            push!(current_run, i)\n",
    "        else\n",
    "            # If the current run is long enough, store it\n",
    "            if length(current_run) >= min_run_length\n",
    "                push!(runs_of_errors, current_run)\n",
    "            end\n",
    "            current_run = []  # Reset for the next run\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    # Handle any run that ends at the last index\n",
    "    if !isempty(current_run) && length(current_run) >= min_run_length\n",
    "        push!(runs_of_errors, current_run)\n",
    "    end\n",
    "\n",
    "    return(runs_of_errors)  # returns a list of index arrays, each representing a run of errors\n",
    "\n",
    "end    # find_runs_of_errors()\n",
    "\n",
    "\n",
    "function fit_and_correct!(heave, points, degree)\n",
    "    ############################################\n",
    "    \n",
    "    fit = curve_fit(Polynomial, points, heave[points], degree)\n",
    "    yfit = fit.(points)\n",
    "    \n",
    "    return(yfit)\n",
    "    \n",
    "end    # fit_and_correct!()\n",
    "    \n",
    "\n",
    "function correct_small_runs!(heave, left_side_points, right_side_points)\n",
    "########################################################################\n",
    "    \n",
    "    # Determine the degree of the polynomial fit\n",
    "    left_degree = length(left_side_points) < 4 ? 1 : 2\n",
    "    right_degree = length(right_side_points) < 4 ? 1 : 2\n",
    "\n",
    "    # Fit and correct left side\n",
    "    yfit1 = fit_and_correct!(heave, left_side_points, left_degree)\n",
    "    yfit1[end] = 0.0  # Ensure the fit at the center is set to 0\n",
    "    heave[left_side_points] .= heave[left_side_points] - yfit1\n",
    "\n",
    "    # Fit and correct right side\n",
    "    yfit2 = fit_and_correct!(heave, right_side_points, right_degree)\n",
    "    yfit2 = 0.0  # Ensure the fit starts at 0 on the right side\n",
    "    heave[right_side_points] .= heave[right_side_points] - yfit2\n",
    "\n",
    "end    # correct_small_runs!()\n",
    "\n",
    "\n",
    "function get_run_centers(runs_of_errors)\n",
    "########################################\n",
    "    \n",
    "    centers = []\n",
    "    \n",
    "    for run in runs_of_errors\n",
    "        center_idx = run[length(run)] #run[div(length(run), 2) + 1]  # Take the middle of the run\n",
    "        push!(centers, center_idx)\n",
    "    end\n",
    "    \n",
    "    return(centers)\n",
    "    \n",
    "end    # get_run_centers()\n",
    "\n",
    "\n",
    "function calc_spectra(heave, sample_frequency)\n",
    "##############################################\n",
    "    \n",
    "    ps_w = welch_pgram(heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "    f2 = freq(ps_w)\n",
    "    Pden2 = power(ps_w)\n",
    "    \n",
    "    return(f2, Pden2)\n",
    "\n",
    "end    # calc_spectra()\n",
    "\n",
    "\n",
    "function fit_and_correct_heave!(heave, left_side_points, right_side_points)\n",
    "###########################################################################\n",
    "    \n",
    "    # Fit and correct left side\n",
    "    yfit1 = fit_and_correct!(heave, left_side_points, 2)\n",
    "    yfit1[end] = 0.0  # Ensure the fit at the center is set to 0\n",
    "    heave[left_side_points] .= heave[left_side_points] - yfit1\n",
    "\n",
    "    # Fit and correct right side\n",
    "    yfit2 = fit_and_correct!(heave, right_side_points, 2)\n",
    "    yfit2[1] = 0.0  # Ensure the fit starts at 0 on the right side\n",
    "    heave[right_side_points] .= heave[right_side_points] .- yfit2\n",
    "\n",
    "end    # function fit_heave()\n",
    "\n",
    "\n",
    "# function to convert HEX values into WSEs\n",
    "function get_displacements(arry)\n",
    "#####################################\n",
    "    \n",
    "    displacements = []\n",
    "\n",
    "    if length(arry[1]) == 3\n",
    "    \n",
    "        for i in arry\n",
    "            append!(displacements,parse(Int, SubString.(i, 1, 1), base=16)*16^2 \n",
    "                + parse(Int, SubString.(i, 2, 2), base=16)*16^1 \n",
    "                + parse(Int, SubString.(i, 3, 3), base=16)*16^0)\n",
    "        end\n",
    "        \n",
    "    else\n",
    "        \n",
    "        for i in arry\n",
    "            append!(displacements,parse(Int, SubString.(i, 1, 1), base=16)*16^1 \n",
    "                + parse(Int, SubString.(i, 2, 2), base=16)*16^0)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    displacements[findall(>=(2048), displacements)] = 2048 .- displacements[findall(>=(2048), displacements)];\n",
    "    \n",
    "    return(displacements./100)\n",
    "    \n",
    "    end     # get_displacements()\n",
    "\n",
    "\n",
    "# Function to convert frequency in Hertz to Period in Seconds\n",
    "function convert_frequency_to_period(frequencies)\n",
    "#################################################\n",
    "    \n",
    "    return(1.0 ./ frequencies)\n",
    "    \n",
    "end    # convert_frequency_to_period()\n",
    "\n",
    "\n",
    "# read Noise Floor from saved .bin file\n",
    "function read_noise_floor_file(file_path)\n",
    "#########################################\n",
    "    \n",
    "    # Assuming the file contains a serialized DataFrame\n",
    "    open(file_path, \"r\") do io\n",
    "        return(deserialize(io))\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "function get_noise_floor()\n",
    "##########################\n",
    "    \n",
    "    # determine whether data is coming from JW's laptop or QGHL office computer\n",
    "    paths = [\n",
    "        \"C:\\\\Users\\\\Jim\\\\Julia_programs\\\\Datawell\\\\RDT_vector\\\\Data\\\\Noise_floor.bin\",\n",
    "        \"C:\\\\Users\\\\PC1\\\\Julia_programs\\\\Datawell\\\\RDT_vector\\\\Data\\\\Noise_floor.bin\"\n",
    "    ]\n",
    "    \n",
    "    noise_floor_file = nothing\n",
    "    \n",
    "    for path in paths\n",
    "        if isfile(path)\n",
    "            noise_floor_file = path\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if isnothing(noise_floor_file)\n",
    "        println(\"File not found in any of the provided paths. Exiting program.\")\n",
    "        flush(stdout)\n",
    "        exit(1)\n",
    "    end\n",
    "    \n",
    "    println(\"Reading Noise Floor data from \", noise_floor_file)\n",
    "    \n",
    "    # Deserialize the DataFrame from the file\n",
    "    noise_floors_df = read_noise_floor_file(noise_floor_file)\n",
    "    \n",
    "    # Extract all spectral arrays from the DataFrame\n",
    "    spectral_values = noise_floors_df.Pden2\n",
    "    \n",
    "    # Convert the list of arrays into a matrix where each row is a spectrum\n",
    "    spectral_matrix = hcat(spectral_values...)'\n",
    "    \n",
    "    # Calculate the median spectra (median of each column)\n",
    "    median_spectra = median(spectral_matrix, dims=1)\n",
    "    \n",
    "    # Convert the results to vectors\n",
    "    median_spectra_vector = vec(median_spectra)\n",
    "\n",
    "    return(median_spectra_vector)\n",
    "\n",
    "end    # get_noise_floor() \n",
    "\n",
    "\n",
    "function handle_selection(infil_ref)\n",
    "####################################\n",
    "    \n",
    "    file_choice = get_value(lb)\n",
    "    infil_ref[] = hxv_directory * \"\\\\\" * file_choice[1]\n",
    "    println(\"Selected \", infil_ref[])\n",
    "    flush(stdout)\n",
    "\n",
    "    infil = infil_ref[]\n",
    "\n",
    "    # read HXV file to df\n",
    "    df = DataFrame(CSV.File(infil,header=0, delim=\",\", types=String))\n",
    "\n",
    "    # extract the datetime from the file name\n",
    "    date_str = split(infil,\".\")[1]\n",
    "    ll = length(date_str)\n",
    "    start_date = DateTime.(date_str[ll-16:ll-1], \"yyyy-mm-ddTHHhMMZ\")\n",
    "\n",
    "    # Create df of dates and NaN's\n",
    "    global wse_df = DataFrame(\n",
    "        Date = unix2datetime.(datetime2unix(start_date) .+ (0:sample_rate:sample_length - sample_rate)), \n",
    "        Heave = Vector{Float64}(undef, rec_len) .+= NaN,\n",
    "        North = Vector{Float64}(undef, rec_len) .+= NaN,\n",
    "        West = Vector{Float64}(undef, rec_len) .+= NaN,\n",
    "        GPS_flag = zeros(Int, rec_len)\n",
    "    )\n",
    "    \n",
    "    # Remove rows where any string length in the row is not equal to 4\n",
    "    filter!(row -> all(length.(collect(row)) .== 4), df)\n",
    "    println(nrow(df), \" rows available for processing\")\n",
    "    \n",
    "    # Initialize GPS flag\n",
    "    is_gps = false\n",
    "    \n",
    "    # Find locations of sync word \"7FFF\" in Column2\n",
    "    sync_word_location = findall(==(\"7FFF\"), df.Column2)\n",
    "\n",
    "    if !isempty(sync_word_location)\n",
    "        # Remove invalid locations (last row cannot be sync location)\n",
    "        if last(sync_word_location) == nrow(df)\n",
    "            pop!(sync_word_location)\n",
    "        end\n",
    "    \n",
    "        # Get next row data after valid sync_word_location\n",
    "        next_row_data = df.Column2[sync_word_location .+ 1]\n",
    "    \n",
    "        # Parse the first character and remaining characters as hex numbers\n",
    "        word_numbers = parse.(Int, SubString.(next_row_data, 1, 1), base=16)\n",
    "        words = parse.(Int, SubString.(next_row_data, 2, 4), base=16)\n",
    "    \n",
    "        # Check for GPS flag (7FFF followed by \"7000\")\n",
    "        is_gps = any((word_numbers .== 7) .& (words .== 0))\n",
    "    end\n",
    "\n",
    "    is_gps ? println(\"GPS buoy\") : println(\"MkIII buoy\")\n",
    "    \n",
    "    # Extract hexadecimal substring from Column1, parse it as integers\n",
    "    hex_arr = SubString.(df.Column1, 3, 4)\n",
    "    parsed_values = parse.(Int, hex_arr, base=16)\n",
    "    \n",
    "    # Calculate differences and handle wrap-around (for 8-bit overflow)\n",
    "    diffs = diff(parsed_values)\n",
    "    diffs[diffs .< 0] .+= 256\n",
    "    \n",
    "    # Compute cumulative sum and find valid indices\n",
    "    cumulative_values = cumsum([1; diffs])\n",
    "    valid_indices = findall(<=(rec_len), cumulative_values)\n",
    "    \n",
    "    # Select rows corresponding to valid cumulative values\n",
    "    valid_rows = cumulative_values[valid_indices]\n",
    "    \n",
    "    # Truncate the DataFrame to `rec_len` rows if needed\n",
    "    df = df[1:min(nrow(df), rec_len), :]\n",
    "\n",
    "    # Calculate heave, north, and west WSEs\n",
    "    wse_df[valid_rows, 2] .= get_displacements(SubString.(df.Column3, 1, 3))\n",
    "    north_hex = SubString.(df.Column3, 4, ) .* SubString.(df.Column4, 1, 2)\n",
    "    wse_df[valid_rows, 3] .= get_displacements(north_hex)\n",
    "    wse_df[valid_rows, 4] .= get_displacements(SubString.(df.Column4, 3, 4) .* SubString.(df.Column5, 1, 1))\n",
    "    \n",
    "    # need to replace any NaN's with 0's in order to calculate spectra\n",
    "    replace_nan(v) = map(x -> isnan(x) ? zero(x) : x, v)\n",
    "    wse_df.Heave = map(replace_nan, wse_df.Heave)\n",
    "    global heave = map(replace_nan, wse_df.Heave)\n",
    "    \n",
    "    # Detect all runs of errors\n",
    "    min_run_length = 7\n",
    "    runs_of_errors = find_runs_of_errors(wse_df.Heave, min_run_length)\n",
    "    \n",
    "    # Get the centers of the detected runs\n",
    "    global error_centers = get_run_centers(runs_of_errors)\n",
    "    \n",
    "##    heave = deepcopy(wse_df.Heave)  # Work on a copy of the data\n",
    "    \n",
    "    for error_center in error_centers\n",
    " ###################################################################################\n",
    "#==        \n",
    "        lower_offset = upper_offset = 50  # You can adapt this dynamically\n",
    "    \n",
    "        # Adjust offsets if near the edges\n",
    "        if error_center <= lower_offset\n",
    "            lower_offset = error_center - 1\n",
    "        end\n",
    "    \n",
    "        if error_center + upper_offset > length(heave)\n",
    "            upper_offset = length(heave) - error_center\n",
    "        end\n",
    "    \n",
    "        # Define left and right side points\n",
    "        left_side_points = error_center - lower_offset:error_center\n",
    "        right_side_points = error_center:error_center + upper_offset\n",
    "    \n",
    "        # Ensure both sides have sufficient points for fitting\n",
    "        if length(left_side_points) >= 4 && length(right_side_points) >= 4\n",
    "            fit_and_correct_heave!(heave, left_side_points, right_side_points)\n",
    "        else\n",
    "            # Fall back to handling small runs if fewer than 4 points are available\n",
    "            correct_small_runs!(heave, left_side_points, right_side_points)\n",
    "        end\n",
    "    \n",
    "        # Set the center heave value to 0\n",
    "        heave[error_center] = 0.0\n",
    "==#\n",
    "offset = 75        \n",
    "heave = deepcopy(wse_df.Heave)\n",
    "\n",
    "gps_errors = error_centers\n",
    "heave_length = length(heave)\n",
    "\n",
    "if !isempty(gps_errors)\n",
    "    \n",
    "#    println(length(gps_errors), \" GPS errors at \", Dates.format.(wse_df.Date, \"yyyy-mm-dd HH:MM\"))\n",
    "#    flush(stdout)\n",
    "\n",
    "    # Combine adjacent GPS errors into groups\n",
    "    grouped_errors = [gps_errors[1]]\n",
    "    for i in 2:length(gps_errors)\n",
    "        if gps_errors[i] - gps_errors[i-1] > 10  # e.g., if errors are > 10 points apart, treat them separately\n",
    "            push!(grouped_errors, gps_errors[i])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for ii in reverse(grouped_errors)\n",
    "        error_center = ii\n",
    "\n",
    "        if error_center <= 3\n",
    "            error_center = 3\n",
    "        end\n",
    "\n",
    "        if error_center >= heave_length - 3\n",
    "            error_center = heave_length - 3\n",
    "        end\n",
    "\n",
    "        # Dynamic window size based on wave variability (variance)\n",
    "        # You can adjust this logic based on how variable the data is\n",
    "        region_variability = var(heave[max(1, error_center-offset):min(heave_length, error_center+offset)])\n",
    "        lower_offset = upper_offset = min(max(round(Int, offset / sqrt(region_variability + 1)), 30), offset)\n",
    "\n",
    "        if error_center <= lower_offset\n",
    "            lower_offset = error_center - 1\n",
    "        end\n",
    "\n",
    "        if error_center + upper_offset > heave_length\n",
    "            upper_offset = heave_length - error_center\n",
    "        end\n",
    "\n",
    "        lower_offset = max(lower_offset, 2)\n",
    "        upper_offset = max(upper_offset, 2)\n",
    "\n",
    "        left_side_points = max(1, error_center - lower_offset):error_center\n",
    "        right_side_points = error_center:min(heave_length, error_center + upper_offset)\n",
    "\n",
    "        # Adaptive polynomial fitting by testing different degrees\n",
    "        best_fit_degree = 2\n",
    "        min_rss = Inf\n",
    "        \n",
    "        for degree in 1:4  # Test degrees from 1 to 4\n",
    "            fit1 = curve_fit(Polynomial, left_side_points, heave[left_side_points], degree)\n",
    "            fit2 = curve_fit(Polynomial, right_side_points, heave[right_side_points], degree)\n",
    "            rss = sum((heave[left_side_points] - fit1.(left_side_points)).^2) +\n",
    "                  sum((heave[right_side_points] - fit2.(right_side_points)).^2)\n",
    "            if rss < min_rss\n",
    "                min_rss = rss\n",
    "                best_fit_degree = degree\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Perform the best fit using the selected degree\n",
    "        fit1 = curve_fit(Polynomial, left_side_points, heave[left_side_points], best_fit_degree)\n",
    "        yfit1 = fit1.(left_side_points)\n",
    "        yfit1[end] = 0.0  # Set the last point of the left fit to 0\n",
    "\n",
    "        fit2 = curve_fit(Polynomial, right_side_points, heave[right_side_points], best_fit_degree)\n",
    "        yfit2 = fit2.(right_side_points)\n",
    "        yfit2[1] = 0.0  # Set the first point of the right fit to 0\n",
    "\n",
    "        # Apply polynomial results to WSEs on both sides of GPS error\n",
    "        heave[left_side_points] .= heave[left_side_points] - yfit1\n",
    "        heave[right_side_points] .= heave[right_side_points] - yfit2\n",
    "        heave[ii] = 0.0  # Set WSE at GPS error location to 0\n",
    "    end\n",
    "    \n",
    "end\n",
    "        #==#\n",
    "###################################################################################                \n",
    "    end\n",
    "    \n",
    "    tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "    suspects_count = length(error_centers)\n",
    "\n",
    "    if suspects_count == 0\n",
    "        error_string = \"No suspects flagged\"\n",
    "    elseif suspects_count == 1\n",
    "        error_string = \"1 suspect flagged\"\n",
    "    else\n",
    "        error_string = string(suspects_count,\" suspects flagged\")\n",
    "    end\n",
    "    \n",
    "    # calculare spectra\n",
    "    f2, Pden2 = calc_spectra(wse_df.Heave, sample_frequency)\n",
    "    f2_fixed, Pden2_fixed = calc_spectra(heave, sample_frequency)\n",
    "    \n",
    "    # convert frequency (Hz) to period (s)\n",
    "    periods_sec = convert_frequency_to_period(f2)\n",
    "    \n",
    "    # Plot for GPS buoy (all lines)\n",
    "    p1 = plot(wse_df.Date, wse_df.Heave, lc=:yellow, lw=:0.5, ylabel=\"WSE (m)\", label=\"Original data\", \n",
    "        legend=:topright, fg_legend=:transparent, bg_legend=:transparent, \n",
    "        xlims=(wse_df.Date[1] + Minute(0),wse_df.Date[1] + Minute(30)), ylims=(minimum(wse_df.Heave), maximum(wse_df.Heave)), xticks=(tm_tick,ticks))\n",
    "    \n",
    "    # Show GPS errors identified by Datawell\n",
    "    for jj in error_centers\n",
    "        p1 = vline!([wse_df.Date[jj]], lw=1, c=:red, label=\"\")\n",
    "    end\n",
    "\n",
    "    p1 = plot!(wse_df.Date, heave, color=:blue, lw=:1, alpha=:0.75, label=\"Corrected data\")\n",
    "    p1 = annotate!(wse_df.Date[50], maximum(heave)*0.9, text(error_string, :left, 12))\n",
    "    \n",
    "    # p2 and p3 plots\n",
    "    p2 = plot(f2, Pden2, lc=:yellow, lw=:2, alpha=:1, xlim=(0,0.64), ylim=(0,Inf), label=\"\", xlabel=\"Frequency (Hz)\", \n",
    "        ylabel=\"S(f) (m²/Hz)\", fg_legend=:transparent, bg_legend=:transparent)\n",
    "    p2 = plot!(f2_fixed, Pden2_fixed, lc=:blue, lw=:0.75, alpha=0.75, fillrange=0, fillcolor=:blue, fillalpha=0.1, label=\"\")\n",
    "    p2 = plot!(f2, median_spectra_vector, lw=:2, lc=:red, fillrange=0, fillalpha=0.075, fillcolor=:red, label=\"Median Noise Floor\")\n",
    "    \n",
    "    p3 = plot(plot(periods_sec, Pden2, lc=:yellow, lw=:2, label=\"\", yaxis=:log, yminorticks=10, minorgrid=:true, xlabel= \"Wave Period (s)\", \n",
    "        ylabel=\"S(f) (m²/Hz)\"), xlims=(0,200), legend=:bottomright, fg_legend=:transparent, bg_legend=:transparent)\n",
    "    p3 = plot!(periods_sec, Pden2_fixed, lc=:blue, lw=:0.75, label=\"\")\n",
    "    p3 = plot!(periods_sec, median_spectra_vector, lw=:2, lc=:red, fillrange=0.00001, fillalpha=0.075, fillcolor=:red, label=\"Median Noise Floor\")\n",
    "    \n",
    "    # Combine the three plots as before\n",
    "    l = @layout [a{0.5h}; b{0.5w} c{0.5w} [Plots.grid(1,1)]]\n",
    "    title = Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "    \n",
    "    p1_p2__p3_plot = plot(p1, p2, p3, framestyle = :box, leftmargin = 10Plots.mm, layout=l, suptitle=title, size=(1200, 800))\n",
    "    \n",
    "    #==\n",
    "    # Extract the date part from infil\n",
    "    filename = split(infil, \"\\\\\") |> last\n",
    "    date_part = split(filename, \"T\")[1]\n",
    "    \n",
    "    # Extract the time part from title and format it\n",
    "    time_part = replace(split(title, \" \")[2], \":\" => \"\")\n",
    "    \n",
    "    # Combine the parts into the desired format\n",
    "    output_filename = \".\\\\Plots\\\\\" * date_part * \"_\" * time_part * \".png\"\n",
    "    \n",
    "    ##    savefig(output_filename) \n",
    "    ==#\n",
    "    display(p1_p2__p3_plot)\n",
    "\n",
    "end    # handle_selection()\n",
    "\n",
    "\n",
    "function exit_callback()\n",
    "########################\n",
    "    \n",
    "    destroy(w)  # Close the window when Exit button is pressed\n",
    "\n",
    "end    # exit_callback()\n",
    "\n",
    "\n",
    "function create_button(parent, text, command)\n",
    "#############################################\n",
    "    \n",
    "    b = Button(parent, text=text)\n",
    "    bind(b, \"command\") do path\n",
    "        command()\n",
    "    end\n",
    "    pack(b)\n",
    "    \n",
    "    return(b)\n",
    "\n",
    "end    # create_button()\n",
    "\n",
    "\n",
    "function create_treeview(parent, files)\n",
    "#######################################\n",
    "    \n",
    "    lb = Treeview(parent, files)\n",
    "    scrollbars_add(parent, lb)\n",
    "    pack(lb, expand=true, fill=\"both\")\n",
    "\n",
    "    return(lb)\n",
    "    \n",
    "end    # create_treeview()\n",
    "\n",
    "\n",
    "################################################\n",
    "################################################\n",
    "##           START OF MAIN PROGRAM\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))\n",
    "\n",
    "const rec_len = 2304\n",
    "const sample_frequency = 1.28 # sample frequency in Hertz\n",
    "const sample_length = 1800 # record length in seconds\n",
    "const sample_rate = Float64(1/sample_frequency) # sample spacing in seconds\n",
    "\n",
    "#using Logging: NullLogger, with_logger\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "median_spectra_vector = get_noise_floor()\n",
    "\n",
    "hxv_directory = pick_folder()\n",
    "\n",
    "# Build list of all hxv files in the selected directory (case-insensitive)\n",
    "hxv_files = filter(x -> endswith(lowercase(x), \".hxv\"), readdir(hxv_directory))\n",
    "\n",
    "w = Toplevel(\"Select Date\", 235, 800)\n",
    "tcl(\"pack\", \"propagate\", w, false)\n",
    "\n",
    "# Create frames\n",
    "f = Frame(w)\n",
    "pack(f, expand=true, fill=\"both\")\n",
    "\n",
    "f1 = Frame(f)\n",
    "pack(f1, expand=true, fill=\"both\")\n",
    "\n",
    "# Create Treeview\n",
    "lb = create_treeview(f1, hxv_files)\n",
    "\n",
    "# Button with exit functionality\n",
    "create_button(f, \"Exit\", exit_callback)\n",
    "\n",
    "# Set button style\n",
    "tcl(\"ttk::style\", \"configure\", \"TButton\", foreground=\"blue\", font=\"arial 16 bold\")\n",
    "\n",
    "infil_ref = Ref(\"\")\n",
    "\n",
    "# Bind double-click event to Treeview\n",
    "bind(lb, \"<Double-1>\") do event\n",
    "    handle_selection(infil_ref)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Dates: Millisecond\n",
    "using Plots: scatter!\n",
    "\n",
    "\n",
    "function find_exact_zero_up_crossings_with_threshold(times, elevations, threshold_mm)\n",
    "#####################################################################################\n",
    "    \n",
    "    threshold = threshold_mm / 1000.0  # Convert mm to meters\n",
    "    crossings = []\n",
    "    \n",
    "    for i in 2:length(elevations)\n",
    "        if elevations[i-1] < -threshold && elevations[i] > threshold\n",
    "            # Linear interpolation to find the exact zero-crossing\n",
    "            t1, t2 = times[i-1], times[i]\n",
    "            e1, e2 = elevations[i-1], elevations[i]\n",
    "            fraction = (threshold - e1) / (e2 - e1)\n",
    "            zero_crossing_time = t1 + Millisecond(round(Int, fraction * (t2 - t1).value))\n",
    "            push!(crossings, zero_crossing_time)\n",
    "\n",
    "            # Check for zero crossing with linear interpolation between negative and positive elevations\n",
    "        elseif elevations[i-1] < -threshold && elevations[i] == 0 # threshold\n",
    "            t1, t2 = times[i-1], times[i]\n",
    "            e1, e2 = elevations[i-1], elevations[i]\n",
    "            fraction = (threshold - e1) / (e2 - e1)\n",
    "            zero_crossing_time = t1 + Millisecond(round(Int, fraction * (t2 - t1).value))\n",
    "            push!(crossings, zero_crossing_time)\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return(crossings)\n",
    "    \n",
    "end    # find_exact_zero_up_crossings_with_threshold()\n",
    "    \n",
    "\n",
    "function calculate_valid_wave_heights_periods(elevations, zero_crossings, times, threshold_mm)\n",
    "##############################################################################################\n",
    "    \n",
    "    threshold = threshold_mm / 1000.0  # Convert mm to meters\n",
    "    wave_heights = []\n",
    "    wave_periods = []\n",
    "    \n",
    "    for i in 1:(length(zero_crossings)-1)\n",
    "        start_time = zero_crossings[i]\n",
    "        end_time = zero_crossings[i+1]\n",
    "        start_idx = findfirst(t -> t >= start_time, times)\n",
    "        end_idx = findfirst(t -> t >= end_time, times)\n",
    "        wave_segment = elevations[start_idx:end_idx]\n",
    "        peak = maximum(wave_segment)\n",
    "        trough = minimum(wave_segment)\n",
    "        if peak > threshold && trough < -threshold\n",
    "            wave_height = peak - trough\n",
    "            wave_period = end_time - start_time\n",
    "            push!(wave_heights, wave_height)\n",
    "            push!(wave_periods, wave_period)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return(wave_heights, wave_periods)\n",
    "    \n",
    "end    # calculate_valid_wave_heights_periods()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "times = wse_df.Date\n",
    "elevations = wse_df.Heave\n",
    "\n",
    "threshold = 0\n",
    "\n",
    "exact_zero_up_crossings = find_exact_zero_up_crossings_with_threshold(times, elevations, threshold)\n",
    "wave_heights, wave_periods = calculate_valid_wave_heights_periods(elevations, exact_zero_up_crossings, times, threshold)\n",
    "\n",
    "tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(1))\n",
    "ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "start_minute = 10\n",
    "end_minute = 15\n",
    "\n",
    "title = \"Zero up-crossing points \" * Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "\n",
    "plot(title=title, size=(1200,600),\n",
    "    xlims=(wse_df.Date[1] + Minute(start_minute),wse_df.Date[1] + Minute(end_minute)), xlabel=\"Time (minutes)\", xticks=(tm_tick,ticks),\n",
    "    ylims=(minimum(wse_df.Heave), maximum(wse_df.Heave)), ylabel=\"WSEs (Metres)\",   \n",
    "    legend=:topright, leftmargin = 10Plots.mm, bottommargin = 10Plots.mm, fg_legend=:transparent, bg_legend=:transparent, framestyle=:box)\n",
    "\n",
    "plot!(times, elevations, marker=:circle, ms=:1, malpha=:0.25, label=\"Water Surface Elevation\\n\", lw=:0.5) \n",
    "for jj in outlier_indices\n",
    "    vline!([exact_zero_up_crossings[jj]], lw=:1, lc=:red, ls=:dash, label=\"\")\n",
    "end\n",
    "scatter!(exact_zero_up_crossings, zeros(length(exact_zero_up_crossings)), marker=:x, ms=:3, mc=:green, label=\"Exact Zero Up-Crossings\", color=:red)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Statistics, Plots\n",
    "\n",
    "function calc_confidence_limits(wave_periods_s, confidence_interval)\n",
    "####################################################################    \n",
    "\n",
    "    mean_wave_period = mean(wave_periods_s)\n",
    "    std_wave_period = std(wave_periods_s)\n",
    "    z_scores = [(x - mean_wave_period) / std_wave_period for x in wave_periods_s]\n",
    "    \n",
    "    upper_limit = mean_wave_period + confidence_interval * std_wave_period\n",
    "    lower_limit = mean_wave_period - confidence_interval * std_wave_period\n",
    "\n",
    "    return(upper_limit, lower_limit)\n",
    "\n",
    "end    # calc_confidence_limits()\n",
    "\n",
    "\n",
    "# Function to calculate IQR bounds and identify outliers\n",
    "function find_outliers_iqr(wave_periods_seconds_float)\n",
    "######################################################\n",
    "    \n",
    "    # Calculate the first and third quartiles\n",
    "    Q1 = quantile(wave_periods_seconds_float, 0.25)\n",
    "    Q3 = quantile(wave_periods_seconds_float, 0.75)\n",
    "    \n",
    "    # Calculate the IQR\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calculate the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify the indices of outliers\n",
    "    outlier_indices = findall(x -> x < lower_bound || x > upper_bound, wave_periods_seconds_float)\n",
    "\n",
    "    return(outlier_indices, lower_bound, upper_bound)\n",
    "    \n",
    "end    # find_outliers_iqr()\n",
    "\n",
    "\n",
    "wave_periods_s = [ii.value / 1000 for ii in wave_periods]\n",
    "confidence_interval_95 = 1.96  # For 95% confidence\n",
    "confidence_interval_99 = 2.576  # For 99% confidence\n",
    "\n",
    "limits_95 = calc_confidence_limits(wave_periods_s, confidence_interval_95)\n",
    "limits_99 = calc_confidence_limits(wave_periods_s, confidence_interval_99)\n",
    "\n",
    "# Calculate the time differences between zero-up crossings\n",
    "wave_periods_ms = diff(exact_zero_up_crossings)\n",
    "\n",
    "# Convert from Milliseconds to seconds by dividing each element by Millisecond(1000)\n",
    "wave_periods_seconds = wave_periods_ms / Millisecond(1000)\n",
    "\n",
    "# Optionally convert to a Float64 array if you want the values as numbers\n",
    "wave_periods_seconds_float = Float64.(wave_periods_seconds)\n",
    "\n",
    "# Apply the IQR method to wave_periods_seconds_float\n",
    "outlier_indices, lower_bound, upper_bound = find_outliers_iqr(wave_periods_seconds_float)\n",
    "\n",
    "# Initialize the plot\n",
    "tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(1))\n",
    "ticks = Dates.format.(tm_tick,\"MM\")\n",
    "\n",
    "start_minute = 0\n",
    "end_minute = 30\n",
    "\n",
    "title = \"Wave periods and confidence limits \" * Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "p1 = plot(\n",
    "    xlabel=\"Exact Zero Up Crossings (DateTime)\", ylabel=\"Wave Period (s)\", \n",
    "    title=title, \n",
    "    xlims=(wse_df.Date[1] + Minute(start_minute),wse_df.Date[1] + Minute(end_minute)),\n",
    "    xticks=(tm_tick, ticks), framestyle=:box, size=(1200, 600),\n",
    "    fg_legend=:transparent, bg_legend=:transparent,\n",
    "    legend=:topright, leftmargin = 10Plots.mm, bottommargin = 10Plots.mm, \n",
    ")\n",
    "\n",
    "# Count the number of wave periods exceeding the 99% confidence limit\n",
    "num_outliers_99 = count(x -> x > limits_99[2], wave_periods_seconds_float)\n",
    "suspect_string = string(num_outliers_99,\" wave periods exceeding 99% confidence limit!\")\n",
    "\n",
    "# Loop through each wave period to plot with varying bar widths and color based on outliers\n",
    "for i in 1:length(wave_periods_seconds_float)  # Adjust loop to match the shorter array\n",
    "\n",
    "    # Set color: red if wave period exceeds the 99% confidence limit, otherwise blue\n",
    "    bar_color = wave_periods_seconds_float[i] > limits_99[1] ? :red : :blue\n",
    "    bar_width = wave_periods_seconds_float[i] > limits_99[1] ? :3 : :1\n",
    "    \n",
    "    # Add a single bar for each wave period with custom width and color\n",
    "    bar!(p1, [exact_zero_up_crossings[i]], [wave_periods_seconds_float[i]], \n",
    "        lc=bar_color, lw=bar_width, fillcolor=bar_color, label=false)\n",
    "\n",
    "end\n",
    "\n",
    "# Add horizontal lines for 95% and 99% confidence limits\n",
    "hline!(p1, [limits_99[2]], color=:red, lw=:1, linestyle=:dash, label=\"99% Confidence Upper Limit\\n\")\n",
    "hline!(p1, [limits_95[2]], color=:green, lw=:2, linestyle=:dot, label=\"95% Confidence Upper Limit\")\n",
    "\n",
    "annotate!(wse_df.Date[1] + Minute(start_minute), maximum(wave_periods_seconds)*0.9, text(suspect_string, :left, 10))\n",
    "\n",
    "# Display the plot\n",
    "display(p1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate confidence limits for outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics, Plots, Dates\n",
    "\n",
    "# Function to calculate confidence limits\n",
    "function calc_confidence_limits(data, confidence_interval)\n",
    "####################################################################################\n",
    "    \n",
    "    mean_val = mean(data)\n",
    "    std_dev = std(data)\n",
    "    upper_limit = mean_val + confidence_interval * std_dev\n",
    "    lower_limit = mean_val - confidence_interval * std_dev\n",
    "\n",
    "    return(lower_limit, upper_limit)\n",
    "    \n",
    "end    # calc_confidence_limits()\n",
    "\n",
    "\n",
    "# Function to calculate outliers based on IQR\n",
    "function find_outliers_iqr(data)\n",
    "#################################################\n",
    "    \n",
    "    # Calculate the first and third quartiles\n",
    "    Q1 = quantile(data, 0.25)\n",
    "    Q3 = quantile(data, 0.75)\n",
    "\n",
    "    # Calculate the IQR\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calculate the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify the indices of outliers\n",
    "    outlier_indices = findall(x -> x < lower_bound || x > upper_bound, data)\n",
    "\n",
    "    return(outlier_indices, lower_bound, upper_bound)\n",
    "    \n",
    "end    # find_outliers_iqr()\n",
    "\n",
    "\n",
    "# Function to plot data with outliers and confidence intervals\n",
    "function plot_outliers(data, times, limits_95, limits_99, title_str::String, ylabel_str::String)\n",
    "################################################################################################\n",
    "    \n",
    "    # Apply the IQR method to the data\n",
    "    outlier_indices, lower_bound, upper_bound = find_outliers_iqr(data)\n",
    "\n",
    "    # Initialize the plot\n",
    "    tm_tick = range(times[1], times[end], step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick,\"MM\")\n",
    "    start_minute = 0\n",
    "    end_minute = 30\n",
    "\n",
    "    p1 = plot(\n",
    "        xlabel=\"Record Time (Minutes)\", xlabelfontsize=10, ylabel=ylabel_str, ylabelfontsize=10,\n",
    "        title=title_str, titlefontsize=:10,\n",
    "        xlims=(times[1] + Minute(start_minute), times[1] + Minute(end_minute)),\n",
    "        xticks=(tm_tick, ticks), framestyle=:box, size=(1200, 400),\n",
    "        fg_legend=:transparent, bg_legend=:transparent,\n",
    "        legend=:topright, leftmargin = 10Plots.mm, bottommargin = 10Plots.mm\n",
    "    )\n",
    "\n",
    "    # Count the number of values exceeding the 99% confidence limit\n",
    "    num_outliers_99 = count(x -> x > limits_99[1], data)\n",
    "    suspect_string = string(\"  \",num_outliers_99, \" values exceeding 99% confidence limit!\")\n",
    "\n",
    "    # Loop through each data point to plot with varying bar widths and color based on outliers\n",
    "    for i in 1:length(data)\n",
    "        # Set color: red if value exceeds the 99% confidence limit, otherwise blue\n",
    "        bar_color = data[i] > limits_99[1] ? :red : :blue\n",
    "        bar_width = data[i] > limits_99[1] ? :3 : :1\n",
    "        bar_alpha = data[i] > limits_99[1] ? :1 : :0.5\n",
    "        \n",
    "\n",
    "        # Add a single bar for each data point with custom width and color\n",
    "        bar!(p1, [times[i]], [data[i]], lc=bar_color, lw=bar_width, fillcolor=bar_color, alpha=bar_alpha, label=false)\n",
    "    end\n",
    "\n",
    "    # Add horizontal lines for 95% and 99% confidence limits\n",
    "    hline!(p1, [limits_99[1]], color=:red, lw=:2, linestyle=:dash, label=\"99% Confidence Upper Limit\\n\")\n",
    "    hline!(p1, [limits_95[1]], color=:green, lw=:2, linestyle=:dot, label=\"95% Confidence Upper Limit\")\n",
    "\n",
    "    # Annotate plot with the number of outliers\n",
    "    annotate!(times[1] + Minute(start_minute), maximum(data) * 0.9, text(suspect_string, :left, 10))\n",
    "\n",
    "    # Display the plot\n",
    "    display(p1)\n",
    "\n",
    "    return(outlier_indices)\n",
    "    \n",
    "end    # plot_outliers()\n",
    "\n",
    "\n",
    "function find_exact_zero_up_crossings_with_threshold(times, elevations, threshold_mm)\n",
    "#####################################################################################\n",
    "    \n",
    "    threshold = threshold_mm / 1000.0  # Convert mm to meters\n",
    "    crossings = []\n",
    "    \n",
    "    for i in 2:length(elevations)\n",
    "        if elevations[i-1] < -threshold && elevations[i] > threshold\n",
    "            # Linear interpolation to find the exact zero-crossing\n",
    "            t1, t2 = times[i-1], times[i]\n",
    "            e1, e2 = elevations[i-1], elevations[i]\n",
    "            fraction = (threshold - e1) / (e2 - e1)\n",
    "            zero_crossing_time = t1 + Millisecond(round(Int, fraction * (t2 - t1).value))\n",
    "            push!(crossings, zero_crossing_time)\n",
    "\n",
    "            # Check for zero crossing with linear interpolation between negative and positive elevations\n",
    "        elseif elevations[i-1] < -threshold && elevations[i] == 0 # threshold\n",
    "            t1, t2 = times[i-1], times[i]\n",
    "            e1, e2 = elevations[i-1], elevations[i]\n",
    "            fraction = (threshold - e1) / (e2 - e1)\n",
    "            zero_crossing_time = t1 + Millisecond(round(Int, fraction * (t2 - t1).value))\n",
    "            push!(crossings, zero_crossing_time)\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return(crossings)\n",
    "    \n",
    "end    # find_exact_zero_up_crossings_with_threshold()\n",
    "    \n",
    "\n",
    "function calculate_valid_wave_heights_periods(elevations, zero_crossings, times, threshold_mm)\n",
    "##############################################################################################\n",
    "    \n",
    "    threshold = threshold_mm / 1000.0  # Convert mm to meters\n",
    "    wave_heights = []\n",
    "    wave_periods = []\n",
    "    \n",
    "    for i in 1:(length(zero_crossings)-1)\n",
    "        start_time = zero_crossings[i]\n",
    "        end_time = zero_crossings[i+1]\n",
    "        start_idx = findfirst(t -> t >= start_time, times)\n",
    "        end_idx = findfirst(t -> t >= end_time, times)\n",
    "        wave_segment = elevations[start_idx:end_idx]\n",
    "        peak = maximum(wave_segment)\n",
    "        trough = minimum(wave_segment)\n",
    "        if peak > threshold && trough < -threshold\n",
    "            wave_height = peak - trough\n",
    "            wave_period = end_time - start_time\n",
    "            push!(wave_heights, wave_height)\n",
    "            push!(wave_periods, wave_period)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return(wave_heights, wave_periods)\n",
    "    \n",
    "end    # calculate_valid_wave_heights_periods()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "times = wse_df.Date\n",
    "elevations = wse_df.Heave\n",
    "\n",
    "threshold = 0\n",
    "\n",
    "exact_zero_up_crossings = find_exact_zero_up_crossings_with_threshold(times, elevations, threshold)\n",
    "wave_heights, wave_periods = calculate_valid_wave_heights_periods(elevations, exact_zero_up_crossings, times, threshold)\n",
    "\n",
    "confidence_interval_95 = 1.96  # For 95% confidence\n",
    "confidence_interval_99 = 2.576  # For 99% confidence\n",
    "\n",
    "# Usage for wave heights\n",
    "########################\n",
    "wave_heights_s = wave_heights\n",
    "limits_95_heights = calc_confidence_limits(wave_heights_s, confidence_interval_95)\n",
    "limits_99_heights = calc_confidence_limits(wave_heights_s, confidence_interval_99)\n",
    "\n",
    "# Title and ylabel for wave heights\n",
    "title_wave_heights = \"Wave Heights and Confidence Limits - \" * Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "ylabel_wave_heights = \"Wave Height (m)\"\n",
    "\n",
    "# Plot wave heights\n",
    "outlier_indices_heights = plot_outliers(wave_heights_s, exact_zero_up_crossings, limits_95_heights[2], limits_99_heights[2], title_wave_heights, ylabel_wave_heights)\n",
    "\n",
    "# Usage for wave periods\n",
    "########################\n",
    "wave_periods_s = [ii.value / 1000 for ii in wave_periods]\n",
    "limits_95_periods = calc_confidence_limits(wave_periods_s, confidence_interval_95)\n",
    "limits_99_periods = calc_confidence_limits(wave_periods_s, confidence_interval_99)\n",
    "\n",
    "# Title and ylabel for wave periods\n",
    "title_wave_periods = \"Wave Periods and Confidence Limits - \" * Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "ylabel_wave_periods = \"Wave Period (s)\"\n",
    "\n",
    "# Plot wave periods\n",
    "outlier_indices_periods = plot_outliers(wave_periods_s, exact_zero_up_crossings, limits_95_periods[2], limits_99_periods[2], title_wave_periods, ylabel_wave_periods)\n",
    "\n",
    "tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(1))\n",
    "ticks = Dates.format.(tm_tick,\"MM\")\n",
    "\n",
    "start_minute = 0\n",
    "end_minute = 30\n",
    "\n",
    "title = \"Zero up-crossing points - \" * Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "\n",
    "p1 = plot(title=title, titlefontsize=:10, size=(1200,400),\n",
    "    xlims=(wse_df.Date[1] + Minute(start_minute),wse_df.Date[1] + Minute(end_minute)), xlabel=\"Record Time (Minutes)\", xlabelfontsize=:10, xticks=(tm_tick,ticks),\n",
    "    ylims=(minimum(wse_df.Heave), maximum(wse_df.Heave)), ylabel=\"WSEs (m)\", ylabelfontsize=:10,  \n",
    "    legend=:topright, leftmargin = 10Plots.mm, bottommargin = 10Plots.mm, fg_legend=:transparent, bg_legend=:transparent, framestyle=:box)\n",
    "\n",
    "p1 = plot!(times, elevations, marker=:circle, ms=:1, malpha=:0.25, alpha=:0.5, label=\"Water Surface Elevation\\n\", lw=:0.5) \n",
    "for jj in outlier_indices_periods\n",
    "    p1 = vline!([exact_zero_up_crossings[jj]], lw=:2, lc=:red, ls=:dash, label=\"\")\n",
    "end\n",
    "p1 = scatter!(exact_zero_up_crossings, zeros(length(exact_zero_up_crossings)), marker=:x, ms=:3, mc=:green, label=\"Exact Zero Up-Crossings\", color=:red)\n",
    "\n",
    "display(p1)\n",
    "\n",
    "gr()\n",
    "using DSP\n",
    "\n",
    "function plot_spectrogram(wse_df, condition)\n",
    "################################################\n",
    "\n",
    "    nw=128;\n",
    "    spec = DSP.Periodograms.spectrogram(wse_df.Heave, nw, round(Int, nw*0.9); fs=1.28,window=hanning);\n",
    "    power_spec = DSP.Periodograms.power(spec)\n",
    "    max_spec = maximum(power_spec)\n",
    "    \n",
    "    start_date = first(wse_df.Date)\n",
    "    last_date = last(wse_df.Date)\n",
    "    \n",
    "    # display plots to screen\n",
    "    tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick,\"MM\")\n",
    "    \n",
    "    #p1 = heatmap(first(wse_df.Date) + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, power_spec, lw=0.25, c=cgrad(:Spectral, rev=true), clims=(0.0,max_spec), levels=10, fill=true)\n",
    "    p1 = contourf(first(wse_df.Date) + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, power_spec, lw=0.25, \n",
    "        c=cgrad(:Spectral, rev=true), clims=(0,max_spec), levels=10, fill=true)\n",
    "    \n",
    "    # draw grid lines on plot\n",
    "    frequency_grid_lines = 0:0.1:0.6\n",
    "    time_grid_lines = collect(start_date:Minute(1):last_date)\n",
    "    hline!(p1, frequency_grid_lines, lw=0.5, c=:white, label=\"\")   \n",
    "    vline!(p1, time_grid_lines, lw=0.5, c=:white, label=\"\")\n",
    "       \n",
    "    title = Dates.format.(wse_df.Date[1],\"yyyy-mm-dd HH:MM\") * condition * \" Spectrogram\"\n",
    "    plot_file = replace(\".\\\\Plots\\\\\" * replace(title, \" \" => \"_\") * \".png\", \":\" => \"\")\n",
    "    \n",
    "    plot_p1 = plot(p1, xlabel=\"Record Time (Minutes)\", xlabelfontsize=:10, xlim=(start_date,last_date), xticks=(tm_tick,ticks), xtickfontsize=7,\n",
    "            ylabel=\"Frequency (Hz)\", ylim=(0,0.64), ylabelfontsize=:10, ytickfontsize=8, \n",
    "            title=title, titlefontsize=:10, framestyle = :box,\n",
    "            leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1200,400), gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, colorbar=:false)\n",
    "#==    \n",
    "    try\n",
    "        # Output plot file name\n",
    "        savefig(plot_file)\n",
    "        println(\"\\nPlot file saved as \",plot_file)\n",
    "    catch\n",
    "        \"Alert: Plot not saved!\"\n",
    "    end\n",
    "==#    \n",
    "    display(plot_p1)\n",
    "\n",
    "    return()\n",
    "\n",
    "end\n",
    "\n",
    "gps_errors_count = 0\n",
    "if gps_errors_count == 0\n",
    "\n",
    "    plot_spectrogram(wse_df, \"\")\n",
    "\n",
    "else\n",
    "\n",
    "    plot_spectrogram(wse_df, \" Original\")\n",
    "    \n",
    "    fixed_df = deepcopy(wse_df)\n",
    "    fixed_df.Heave = fixed_heave\n",
    "    \n",
    "    plot_spectrogram(fixed_df, \" Fixed\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, Statistics\n",
    "\n",
    "# Calculate IQR bounds\n",
    "function find_iqr_bounds(heave_data)\n",
    "####################################\n",
    "    \n",
    "    Q1 = quantile(heave_data, 0.25)\n",
    "    Q3 = quantile(heave_data, 0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define lower and upper bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    return(lower_bound, upper_bound)\n",
    "    \n",
    "end    # find_iqr_bounds()\n",
    "\n",
    "\n",
    "# Function to find runs of outliers\n",
    "function find_outlier_runs(wave_periods_seconds_float) #df::DataFrame)\n",
    "#########################################\n",
    "\n",
    "    # Extract heave data\n",
    "    heave_data = wave_periods_seconds_float #df.Heave\n",
    "    \n",
    "    # Calculate IQR bounds\n",
    "    lower_bound, upper_bound = find_iqr_bounds(heave_data)\n",
    "    println(upper_bound)\n",
    "\n",
    "    # Identify outliers (boolean array)\n",
    "    is_outlier = (heave_data .< lower_bound) .| (heave_data .> upper_bound)\n",
    "\n",
    "    # Initialize variables to track runs\n",
    "    runs_of_outliers = []  # Array to store index ranges for outlier runs\n",
    "    current_run = Int[]  # Temporary array to track a single run\n",
    "\n",
    "    # Loop over the outlier flags to identify contiguous runs\n",
    "    for (i, outlier_flag) in enumerate(is_outlier)\n",
    "        if outlier_flag\n",
    "            push!(current_run, i)  # Add index to current run if it's an outlier\n",
    "        else\n",
    "            if !isempty(current_run)\n",
    "                push!(runs_of_outliers, current_run)  # Save the completed run\n",
    "                current_run = Int[]  # Reset for the next run\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Handle any run that ends at the last index\n",
    "    if !isempty(current_run)\n",
    "        push!(runs_of_outliers, current_run)\n",
    "    end\n",
    "\n",
    "    return(runs_of_outliers)\n",
    "\n",
    "end    # find_outlier_runs()\n",
    "\n",
    "# Function to display runs of outliers with timestamps\n",
    "function display_outlier_runs(df::DataFrame, outlier_runs)\n",
    "##########################################################\n",
    "    \n",
    "    println(\"Found $(length(outlier_runs)) runs of outliers:\")\n",
    "    for run in outlier_runs\n",
    "        global start_time = df.Date[run[1]]\n",
    "        global end_time = df.Date[run[end]]\n",
    "        run_length = length(run)\n",
    "        println(\"Outlier run from $start_time to $end_time with $run_length points\")\n",
    "    end\n",
    "    \n",
    "end    # display_outlier_runs()\n",
    "\n",
    "# Apply the functions\n",
    "outlier_runs = find_outlier_runs(wave_periods_seconds_float)\n",
    "display_outlier_runs(wse_df, outlier_runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using StatsBase\n",
    "\n",
    "function calc_confidence_limits_mad(wave_periods_s, confidence_interval)\n",
    "####################################################################    \n",
    "\n",
    "    median_wave_period = median(wave_periods_s)\n",
    "    mad_wave_period = mad(wave_periods_s, center=median(wave_periods_s), normalize=true)  # Scale factor for consistency with standard deviation\n",
    "    z_scores_mad = [(x - median_wave_period) / mad_wave_period for x in wave_periods_s]\n",
    "    \n",
    "    upper_limit_mad = median_wave_period + confidence_interval * mad_wave_period\n",
    "    lower_limit_mad = median_wave_period - confidence_interval * mad_wave_period\n",
    "\n",
    "    return(upper_limit_mad, lower_limit_mad)\n",
    "\n",
    "end    # calc_confidence_limits_mad()\n",
    "\n",
    "\n",
    "wave_periods_s = [ii.value / 1000 for ii in wave_periods]\n",
    "confidence_interval_95 = 1.96  # For 95% confidence\n",
    "confidence_interval_99 = 2.576  # For 99% confidence\n",
    "\n",
    "limits_95_mad = calc_confidence_limits_mad(wave_periods_s, confidence_interval_95)\n",
    "limits_99_mad = calc_confidence_limits_mad(wave_periods_s, confidence_interval_99)\n",
    "\n",
    "plot(seriestype=:line, label=\"Wave Periods (s)\", leftmargin = 10Plots.mm, bottommargin = 10Plots.mm, \n",
    "    xlabel = \"Wave Index\", ylabel = \"Wave Period (s)\", title = \"Wave Periods with 95% and 99% Confidence Interval (MAD)\",\n",
    "    framestyle = :box, fg_legend=:transparent, bg_legend=:transparent, legend=:topright, size=(1200,600))\n",
    "\n",
    "plot!(wave_periods_s, label=\"\")\n",
    "\n",
    "hline!([limits_99_mad[1]], color=:red, linestyle=:dash, label=\"99% Confidence Upper Limit\")\n",
    "hline!([limits_95_mad[1]], color=:red, linestyle=:dot, label=\"95% Confidence Upper Limit\")\n",
    "hline!([limits_95_mad[2]], color=:green, linestyle=:dot, label=\"95% Confidence Lower Limit\")\n",
    "hline!([limits_99_mad[2]], color=:green, linestyle=:dash, label=\"99% Confidence Lower Limit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Plots, Printf\n",
    "using Statistics\n",
    "\n",
    "function show_zero_x(heave, start_date)\n",
    "################################################\n",
    "\n",
    "    zero_up = []; valid_zero_up = []\n",
    "\n",
    "    for i in 2:length(heave)-1\n",
    "        if (heave[i]*heave[i+1] < 0 && heave[i+1] > 0) || (heave[i] == 0 && heave[i-1] < 0 && heave[i+1] > 0)\n",
    "            push!(zero_up,i)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # heave Threshold set at 10mm. Refer to Section 9 Wave statistics pp. 9-10 in Datawell Library Manual\n",
    "    threshold = 0.05\n",
    "\n",
    "##    valid_zero_up = []; \n",
    "    crest_points = []; trough_points = []\n",
    "    i = 1; j = 2\n",
    "\n",
    "    while j < length(zero_up)-1\n",
    "\n",
    "        crest = maximum(heave[zero_up[i]:zero_up[j]])\n",
    "        crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "        trough = minimum(heave[crest_point:zero_up[j]])\n",
    "\n",
    "        # Check that crest higher than threshold AND trough less than threshold - Possible Valid Wave!!\n",
    "        if (crest > threshold) & (trough < -threshold)\n",
    "            crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "            trough_point = crest_point + argmin(heave[crest_point:zero_up[j]]) - 1\n",
    "\n",
    "            push!(crest_points,crest_point)\n",
    "            push!(trough_points,trough_point)\n",
    "\n",
    "            next_crest = maximum(heave[zero_up[j]:zero_up[j+1]])\n",
    "\n",
    "            # Check that NEXT crest also exceeds threshold (if so then Valid Wave)\n",
    "            if (next_crest > threshold)\n",
    "    ##            println(\"Crest found at \",crest_point,\" Trough at \",trough_point)\n",
    "                push!(valid_zero_up,(zero_up[i],zero_up[j]));\n",
    "                i = j\n",
    "            end\n",
    "\n",
    "        end\n",
    "\n",
    "        j = j+1\n",
    "\n",
    "    end\n",
    "\n",
    "    # Process last recorded wave\n",
    "    #i = j\n",
    "    #j = j+1\n",
    "\n",
    "    crest = maximum(heave[zero_up[i]:zero_up[j]])\n",
    "    trough = minimum(heave[zero_up[i]:zero_up[j]])\n",
    "\n",
    "    if (crest > threshold) & (trough < -threshold)\n",
    "\n",
    "        crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "        trough_point = crest_point + argmin(heave[crest_point:zero_up[j]]) - 1\n",
    "        push!(valid_zero_up,(zero_up[i],zero_up[j]));\n",
    "\n",
    "    end\n",
    "\n",
    "    heights = []\n",
    "\n",
    "    for i in 1:length(valid_zero_up)\n",
    "\n",
    "        crest = maximum(heave[valid_zero_up[i][1]:valid_zero_up[i][2]]);\n",
    "        trough = minimum(heave[valid_zero_up[i][1]:valid_zero_up[i][2]]);\n",
    "        push!(heights,crest - trough)\n",
    "    ##    @printf(\"Wave %d = %2.3f\\n\",i,crest - trough)\n",
    "\n",
    "    end \n",
    "\n",
    "    # Get time-domain height parameters\n",
    "    sorted_heights = sort(heights, rev=true) # sort heights in reverse order heighestwave to loheave wave\n",
    "    hmax = maximum(sorted_heights)\n",
    "    hs = mean(sorted_heights[1:Int(ceil(length(sorted_heights)/3))])\n",
    "    h10 = mean(sorted_heights[1:Int(ceil(length(sorted_heights) / 10))])\n",
    "    hmean = mean(sorted_heights)\n",
    "\n",
    "    @printf(\"%s; Waves = %3d; Hmean = %4.2fm; Hs = %4.2fm; H10 = %4.2fm; Hmax = %4.2fm\\n\",Dates.format(start_date, \"yyyy-mm-dd HH:MM\"),length(heights), hmean, hs, h10, hmax)\n",
    "\n",
    "    ## Locate the zero-crossing points\n",
    "    global x_point = []\n",
    "    for i in 1:length(valid_zero_up)\n",
    "        push!(x_point,valid_zero_up[i][1] + abs(heave[valid_zero_up[i][1]]) / (heave[valid_zero_up[i][1]+1] - heave[valid_zero_up[i][1]]))\n",
    "    end\n",
    "\n",
    "    # Process final zero-crossing point\n",
    "    i = length(valid_zero_up)\n",
    "    push!(x_point,valid_zero_up[i][2] + abs(heave[valid_zero_up[i][2]]) / (heave[valid_zero_up[i][2]+1] - heave[valid_zero_up[i][2]]))\n",
    "\n",
    "    # Do plots\n",
    "    wse_point = collect(1:1:length(heave))\n",
    "    wse_1 = plot(wse_point, heave[wse_point], c=:blue, alpha=0.5, label = \"WSE's\", fillrange = 0, fillalpha = 0.175, fillcolor = :blue)\n",
    "    wse_1 = scatter!(wse_point, heave[wse_point], c=:white, ms=3, \n",
    "        markerstrokecolor=:blue, alpha=0.5, markerstrokewidth=0.5,label=\"WSE points\")\n",
    "    wse_1 = scatter!(zero_up, heave[zero_up], ms=3, c=:lightgreen, \n",
    "        markerstrokecolor=:lightgreen, series_annotations = text.(zero_up, :bottom, :red, :size, 10), \n",
    "        annotationhalign = :hcenter, label=\"Zero up-cross points\\n\")\n",
    "    wse_1 = scatter!(x_point, zeros(length(x_point)), c=:yellow, ms=5, \n",
    "        markerstrokecolor=:yellow, markershape=:diamond, label=\"Zero-crossing points\")\n",
    "    # heave Threshold set at 10mm. Refer to Section 9 Wave statistics pp. 9-10 in Datawell Library Manual\n",
    "    threshold = 0.05 \n",
    "\n",
    "    wse_1 = hline!([threshold; threshold], lw=0.2, ls =:dot, c=:red, label=\"Threshold\\n\")\n",
    "    wse_1 = hline!([-threshold; -threshold], lw=0.2, ls =:dot, c=:red, label=\"\")\n",
    "\n",
    "    wse_plot = plot(wse_1, size = (1500, 500), xlim=(1,50), ylim=(-1.5,1.5), framestyle = :box, \n",
    "        fg_legend=:transparent, bg_legend=:transparent, legend=:topright,\n",
    "        margin = 1Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, show=true)\n",
    "\n",
    "    display(wse_plot)\n",
    "    \n",
    "    return()\n",
    "\n",
    "end    # show_zero_x()\n",
    "\n",
    "show_zero_x(wse_df.Heave, first(wse_df.Date))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_up = []; valid_zero_up = []\n",
    "\n",
    "    for i in 2:length(heave)-1\n",
    "        if (heave[i]*heave[i+1] < 0 && heave[i+1] > 0) || (heave[i] == 0 && heave[i-1] < 0 && heave[i+1] > 0)\n",
    "            push!(zero_up,i)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # heave Threshold set at 10mm. Refer to Section 9 Wave statistics pp. 9-10 in Datawell Library Manual\n",
    "    threshold = 0.05\n",
    "\n",
    "##    valid_zero_up = []; \n",
    "    crest_points = []; trough_points = []\n",
    "    i = 1; j = 2\n",
    "\n",
    "    while j < length(zero_up)-1\n",
    "\n",
    "        crest = maximum(heave[zero_up[i]:zero_up[j]])\n",
    "        crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "        trough = minimum(heave[crest_point:zero_up[j]])\n",
    "\n",
    "        # Check that crest higher than threshold AND trough less than threshold - Possible Valid Wave!!\n",
    "        if (crest > threshold) & (trough < -threshold)\n",
    "            crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "            trough_point = crest_point + argmin(heave[crest_point:zero_up[j]]) - 1\n",
    "\n",
    "            push!(crest_points,crest_point)\n",
    "            push!(trough_points,trough_point)\n",
    "\n",
    "            next_crest = maximum(heave[zero_up[j]:zero_up[j+1]])\n",
    "\n",
    "            # Check that NEXT crest also exceeds threshold (if so then Valid Wave)\n",
    "            if (next_crest > threshold)\n",
    "    ##            println(\"Crest found at \",crest_point,\" Trough at \",trough_point)\n",
    "                push!(valid_zero_up,(zero_up[i],zero_up[j]));\n",
    "                i = j\n",
    "            end\n",
    "\n",
    "        end\n",
    "\n",
    "        j = j+1\n",
    "\n",
    "    end\n",
    "\n",
    "    # Process last recorded wave\n",
    "    #i = j\n",
    "    #j = j+1\n",
    "\n",
    "    crest = maximum(heave[zero_up[i]:zero_up[j]])\n",
    "    trough = minimum(heave[zero_up[i]:zero_up[j]])\n",
    "\n",
    "    if (crest > threshold) & (trough < -threshold)\n",
    "\n",
    "        crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "        trough_point = crest_point + argmin(heave[crest_point:zero_up[j]]) - 1\n",
    "        push!(valid_zero_up,(zero_up[i],zero_up[j]));\n",
    "\n",
    "    end\n",
    "\n",
    "    heights = []\n",
    "\n",
    "    for i in 1:length(valid_zero_up)\n",
    "\n",
    "        crest = maximum(heave[valid_zero_up[i][1]:valid_zero_up[i][2]]);\n",
    "        trough = minimum(heave[valid_zero_up[i][1]:valid_zero_up[i][2]]);\n",
    "        push!(heights,crest - trough)\n",
    "    ##    @printf(\"Wave %d = %2.3f\\n\",i,crest - trough)\n",
    "\n",
    "    end \n",
    "\n",
    "    # Get time-domain height parameters\n",
    "    sorted_heights = sort(heights, rev=true) # sort heights in reverse order heighestwave to loheave wave\n",
    "    hmax = maximum(sorted_heights)\n",
    "    hs = mean(sorted_heights[1:Int(ceil(length(sorted_heights)/3))])\n",
    "    h10 = mean(sorted_heights[1:Int(ceil(length(sorted_heights) / 10))])\n",
    "    hmean = mean(sorted_heights)\n",
    "\n",
    "    start_date = wse_df.Date[1]\n",
    "\n",
    "    @printf(\"%s; Waves = %3d; Hmean = %4.2fm; Hs = %4.2fm; H10 = %4.2fm; Hmax = %4.2fm\\n\",Dates.format(start_date, \"yyyy-mm-dd HH:MM\"),length(heights), hmean, hs, h10, hmax)\n",
    "\n",
    "    ## Locate the zero-crossing points\n",
    "    global x_point = []\n",
    "    for i in 1:length(valid_zero_up)\n",
    "        push!(x_point,valid_zero_up[i][1] + abs(heave[valid_zero_up[i][1]]) / (heave[valid_zero_up[i][1]+1] - heave[valid_zero_up[i][1]]))\n",
    "    end\n",
    "\n",
    "    # Process final zero-crossing point\n",
    "    i = length(valid_zero_up)\n",
    "    push!(x_point,valid_zero_up[i][2] + abs(heave[valid_zero_up[i][2]]) / (heave[valid_zero_up[i][2]+1] - heave[valid_zero_up[i][2]]))\n",
    "\n",
    "    # Do plots\n",
    "    wse_point = collect(1:1:length(heave))\n",
    "    wse_1 = plot(wse_point, heave[wse_point], c=:blue, alpha=0.5, label = \"WSE's\", fillrange = 0, fillalpha = 0.175, fillcolor = :blue)\n",
    "    wse_1 = scatter!(wse_point, heave[wse_point], c=:white, ms=3, \n",
    "        markerstrokecolor=:blue, alpha=0.5, markerstrokewidth=0.5,label=\"WSE points\")\n",
    "    wse_1 = scatter!(zero_up, heave[zero_up], ms=3, c=:lightgreen, \n",
    "        markerstrokecolor=:lightgreen, series_annotations = text.(zero_up, :bottom, :red, :size, 10), \n",
    "        annotationhalign = :hcenter, label=\"Zero up-cross points\\n\")\n",
    "    wse_1 = scatter!(x_point, zeros(length(x_point)), c=:yellow, ms=5, \n",
    "        markerstrokecolor=:yellow, markershape=:diamond, label=\"Zero-crossing points\")\n",
    "    # heave Threshold set at 10mm. Refer to Section 9 Wave statistics pp. 9-10 in Datawell Library Manual\n",
    "    threshold = 0.05 \n",
    "\n",
    "    wse_1 = hline!([threshold; threshold], lw=0.2, ls =:dot, c=:red, label=\"Threshold\\n\")\n",
    "    wse_1 = hline!([-threshold; -threshold], lw=0.2, ls =:dot, c=:red, label=\"\")\n",
    "\n",
    "    wse_plot = plot(wse_1, size = (1500, 500), xlim=(1,100), ylim=(-1.5,1.5), framestyle = :box, \n",
    "        fg_legend=:transparent, bg_legend=:transparent, legend=:topright,\n",
    "        margin = 1Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, show=true)\n",
    "\n",
    "    display(wse_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "heave = wse_df.Heave\n",
    "start_date = wse_df.Date[1]\n",
    "\n",
    "# Initialize arrays\n",
    "zero_up = Int[]\n",
    "valid_zero_up = Vector{Tuple{Int, Int}}()\n",
    "crest_points = Int[]\n",
    "trough_points = Int[]\n",
    "\n",
    "# Find zero crossings\n",
    "for i in 2:length(heave)-1\n",
    "    if (heave[i] * heave[i+1] < 0 && heave[i+1] > 0) || (heave[i] == 0 && heave[i-1] < 0 && heave[i+1] > 0)\n",
    "        push!(zero_up, i)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Set threshold\n",
    "threshold = 0.05\n",
    "\n",
    "# Process zero crossings\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "while j < length(zero_up)\n",
    "    crest_range = zero_up[i]:zero_up[j]\n",
    "    crest = maximum(heave[crest_range])\n",
    "    crest_point = zero_up[i] + argmax(heave[crest_range]) - 1\n",
    "    trough_range = crest_point:zero_up[j]\n",
    "    trough = minimum(heave[trough_range])\n",
    "\n",
    "    # Check for valid wave\n",
    "    if crest > threshold && trough < -threshold\n",
    "        crest_point = zero_up[i] + argmax(heave[crest_range]) - 1\n",
    "        trough_point = crest_point + argmin(heave[trough_range]) - 1\n",
    "\n",
    "        push!(crest_points, crest_point)\n",
    "        push!(trough_points, trough_point)\n",
    "\n",
    "        next_crest_range = zero_up[j]:zero_up[j+1]\n",
    "        next_crest = maximum(heave[next_crest_range])\n",
    "\n",
    "        # Check next crest\n",
    "        if next_crest > threshold\n",
    "            push!(valid_zero_up, (zero_up[i], zero_up[j]))\n",
    "            i = j\n",
    "        end\n",
    "    end\n",
    "\n",
    "    j += 1\n",
    "end\n",
    "\n",
    "heights = Float64[]\n",
    "\n",
    "# Process zero crossings\n",
    "for i in 1:length(zero_up)-1\n",
    "    crest_range = zero_up[i]:zero_up[i+1]\n",
    "    crest = maximum(heave[crest_range])\n",
    "    trough = minimum(heave[crest_range])\n",
    "\n",
    "    if crest > threshold && trough < -threshold\n",
    "        crest_point = zero_up[i] + argmax(heave[crest_range]) - 1\n",
    "        trough_point = crest_point + argmin(heave[crest_point:zero_up[i+1]]) - 1\n",
    "        push!(valid_zero_up, (zero_up[i], zero_up[i+1]))\n",
    "    end\n",
    "end\n",
    "\n",
    "# Calculate wave heights\n",
    "for (start, stop) in valid_zero_up\n",
    "    crest = maximum(heave[start:stop])\n",
    "    trough = minimum(heave[start:stop])\n",
    "    push!(heights, crest - trough)\n",
    "end\n",
    "\n",
    "# Get time-domain height parameters\n",
    "sorted_heights = sort(heights, rev=true)  # Sort heights in descending order\n",
    "hmax = sorted_heights[1]\n",
    "hs = mean(sorted_heights[1:ceil(Int, length(sorted_heights) / 3)])\n",
    "h10 = mean(sorted_heights[1:ceil(Int, length(sorted_heights) / 10)])\n",
    "hmean = mean(sorted_heights)\n",
    "\n",
    "@printf(\"%s; Waves = %3d; Hmean = %4.2fm; Hs = %4.2fm; H10 = %4.2fm; Hmax = %4.2fm\\n\",\n",
    "    Dates.format(start_date, \"yyyy-mm-dd HH:MM\"), length(heights), hmean, hs, h10, hmax)\n",
    "\n",
    "# Initialize array for zero-crossing points\n",
    "x_point = Float64[]\n",
    "\n",
    "# Function to calculate zero-crossing point\n",
    "function zero_crossing_point(start_idx, heave)\n",
    "    return start_idx + abs(heave[start_idx]) / (heave[start_idx + 1] - heave[start_idx])\n",
    "end\n",
    "\n",
    "# Locate zero-crossing points\n",
    "for (start, stop) in valid_zero_up\n",
    "    push!(x_point, zero_crossing_point(start, heave))\n",
    "end\n",
    "\n",
    "# Process final zero-crossing point\n",
    "final_idx = valid_zero_up[end][2]\n",
    "push!(x_point, zero_crossing_point(final_idx, heave))\n",
    "\n",
    "zero_crossing_times = DateTime[]\n",
    "\n",
    "# Initialize arrays to store integer and fractional parts\n",
    "integer_parts = Int[]\n",
    "fractional_parts = Float64[]\n",
    "\n",
    "# Separate each number into integer and fractional parts\n",
    "for x in x_point\n",
    "    int_part = floor(Int, x)\n",
    "    frac_part = x - int_part\n",
    "    push!(zero_crossing_times, wse_df.Date[int_part] + Millisecond(round(Int, frac_part / 1.28 * 1000)))\n",
    "\n",
    "end\n",
    "\n",
    "plot(size=(1200,600), xlims=(wse_df.Date[1] + Minute(10), wse_df.Date[1] + Minute(15)), framestyle = :box, \n",
    "    fg_legend=:transparent, bg_legend=:transparent, legend=:topright,\n",
    "    margin = 1Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dash, gridalpha=:0.5, show=true)\n",
    "\n",
    "plot!(wse_df.Date, wse_df.Heave, lw=:3, lc=:lightgrey, alpha=:1, label=\"\")\n",
    "plot!(wse_df.Date, wse_df.Heave, lw=:1, lc=:yellow, alpha=:1, label=\"WSEs\")\n",
    "\n",
    "scatter!(zero_crossing_times, zeros(length(x_point)), marker=:diamond, ms=:3, mc=:yellow, alpha=:0.25, label=\"Zero-crossing points\")\n",
    "\n",
    "threshold = 0.05\n",
    "hline!([threshold], lw=1, ls =:dot, c=:red, alpha=:0.75, label=\"Threshold\\n\")\n",
    "hline!([-threshold], lw=1, ls =:dot, c=:red, alpha=:0.75, label=\"\")\n",
    "\n",
    "vline!([wse_df.Date[trunc(Int,x_point[argmax(diff(x_point))])]], lw=:1, lc=:red, ls=:dash, label=\"Suspect\")\n",
    "\n",
    "#plot!(wse_df.Date, heave_corrected, label=\"Corrected heave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_point[argmax(diff(x_point))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "unique(x_point[findall(>(12),diff(x_point))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_point[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wse_df.Date[x_point[findall(>(20),diff(x_point))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacement curve-fitting code to be inserted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wse_df.Date[trunc(Int,x_point[argmax(diff(x_point))])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_point[argmax(diff(x_point))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "findall(>(20),diff(x_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "error_centers = [360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "argmax(diff(x_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to group adjacent GPS errors\n",
    "function group_errors(gps_errors, threshold=10)\n",
    "###############################################\n",
    "    \n",
    "    grouped = [gps_errors[1]]\n",
    "    \n",
    "    for i in 2:length(gps_errors)\n",
    "        if gps_errors[i] - gps_errors[i-1] > threshold\n",
    "            push!(grouped, gps_errors[i])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return(grouped)\n",
    "    \n",
    "end    # group_errors()\n",
    "\n",
    "\n",
    "# Function to dynamically calculate offsets based on region variability\n",
    "function calculate_offsets(error_center, heave, offset, heave_length)\n",
    "#####################################################################\n",
    "    \n",
    "    region_variability = var(heave[max(1, error_center - offset):min(heave_length, error_center + offset)])\n",
    "    dynamic_offset = min(max(round(Int, offset / sqrt(region_variability + 1)), 30), offset)\n",
    "    \n",
    "    lower_offset = min(dynamic_offset, error_center - 1)\n",
    "    upper_offset = min(dynamic_offset, heave_length - error_center)\n",
    "    \n",
    "    return(lower_offset, upper_offset)\n",
    "    \n",
    "end    # calculate_offsets()\n",
    "\n",
    "\n",
    "# Function to perform polynomial fitting\n",
    "function perform_best_fit(heave, left_side_points, right_side_points, degrees=1:4)\n",
    "##################################################################################\n",
    "    \n",
    "    min_rss = Inf\n",
    "    best_degree = 2  # Default degree\n",
    "\n",
    "    for degree in degrees\n",
    "        fit1 = curve_fit(Polynomial, left_side_points, heave[left_side_points], degree)\n",
    "        fit2 = curve_fit(Polynomial, right_side_points, heave[right_side_points], degree)\n",
    "        \n",
    "        rss = sum((heave[left_side_points] - fit1.(left_side_points)).^2) +\n",
    "              sum((heave[right_side_points] - fit2.(right_side_points)).^2)\n",
    "        \n",
    "        if rss < min_rss\n",
    "            min_rss = rss\n",
    "            best_degree = degree\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Perform fitting with the best degree\n",
    "    fit1 = curve_fit(Polynomial, left_side_points, heave[left_side_points], best_degree)\n",
    "    fit2 = curve_fit(Polynomial, right_side_points, heave[right_side_points], best_degree)\n",
    "\n",
    "    yfit1 = fit1.(left_side_points)\n",
    "    yfit2 = fit2.(right_side_points)\n",
    "    \n",
    "    # Set boundary conditions\n",
    "    yfit1[end] = 0.0\n",
    "    yfit2[1] = 0.0\n",
    "\n",
    "    return(yfit1, yfit2)\n",
    "    \n",
    "end    # perform_best_fit()\n",
    "\n",
    "\n",
    "# Main function for correcting heave data around GPS or suspect errors\n",
    "function correct_gps_errors(heave, gps_errors, offset)\n",
    "######################################################\n",
    "    \n",
    "    heave_length = length(heave)\n",
    "    grouped_errors = group_errors(gps_errors)\n",
    "\n",
    "    for error_center in reverse(grouped_errors)\n",
    "        # Boundary adjustments for edge cases\n",
    "        error_center = clamp(error_center, 3, heave_length - 3)\n",
    "\n",
    "        # Calculate dynamic offsets\n",
    "        lower_offset, upper_offset = calculate_offsets(error_center, heave, offset, heave_length)\n",
    "\n",
    "        left_side_points = max(1, error_center - lower_offset):error_center\n",
    "        right_side_points = error_center:min(heave_length, error_center + upper_offset)\n",
    "\n",
    "        # Perform the best polynomial fit\n",
    "        yfit1, yfit2 = perform_best_fit(heave, left_side_points, right_side_points)\n",
    "\n",
    "        # Apply polynomial results to WSEs on both sides of GPS error\n",
    "        heave[left_side_points] .= heave[left_side_points] - yfit1\n",
    "        heave[right_side_points] .= heave[right_side_points] - yfit2\n",
    "\n",
    "        # Set the center WSE at the GPS error location to 0\n",
    "        heave[error_center] = 0.0\n",
    "    end\n",
    "\n",
    "    return(heave)\n",
    "    \n",
    "end    # correct_gps_errors()\n",
    "\n",
    "# Example of running the main function\n",
    "heave = deepcopy(wse_df.Heave)\n",
    "offset = 50\n",
    "gps_errors = error_centers\n",
    "heave_corrected = correct_gps_errors(heave, gps_errors, offset);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot(size=(1200,600), xlims=(wse_df.Date[1] + Minute(10),wse_df.Date[1] + Minute(15)))\n",
    "#plot!(wse_df.Date, wse_df.Heave)\n",
    "plot!(wse_df.Date, heave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot(heave,size=(1200,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = dates_ordinal; y = heave\n",
    "initial_params = curve_fit(Polynomial, x, y, degree).coeffs\n",
    "\n",
    "function poly_residuals(params, x, y)\n",
    "    return y .- (params .+ params .* x .+ params .* x.^2 .+ params .* x.^3)\n",
    "end\n",
    "\n",
    "bisquare_weights(poly_residuals(params, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using CurveFit\n",
    "using DataFrames\n",
    "using Dates\n",
    "using Plots\n",
    "using Optim\n",
    "\n",
    "# Define the bisquare weight function\n",
    "function bisquare_weights(residuals, c=4.685)\n",
    "    mad = median(abs.(residuals .- median(residuals)))\n",
    "    u = residuals / (c * mad)\n",
    "    weights = (abs.(u) .< 1) .* (1 .- u.^2).^2\n",
    "    return weights\n",
    "end\n",
    "\n",
    "# Define the residual function for least squares fitting\n",
    "function poly_residuals(params, x, y)\n",
    "    degree = length(params) - 1\n",
    "    y_fit = sum(params[i+1] .* x.^i for i in 0:degree)\n",
    "    return y .- y_fit\n",
    "end\n",
    "\n",
    "# Perform robust fitting using bisquare weights\n",
    "function bisquare_fit(x, y, degree)\n",
    "    initial_params = curve_fit(Polynomial, x, y, degree).coeffs\n",
    "    result = optimize(params -> sum(bisquare_weights(poly_residuals(params, x, y)) .* poly_residuals(params, x, y).^2),\n",
    "                      initial_params, BFGS())\n",
    "    return result.minimizer\n",
    "end\n",
    "\n",
    "# Example usage with wse_df.Date and wse_df.Heave\n",
    "function main()\n",
    "    # Sample data for demonstration\n",
    "    dates = DateTime(2023, 1, 1):Day(1):DateTime(2023, 4, 10)\n",
    "    heave = sin.(LinRange(0, 10, length(dates))) .+ randn(length(dates)) * 0.1\n",
    "    heave[1:10:end] .+= 5  # Adding outliers\n",
    "\n",
    "    # Convert dates to ordinal for fitting\n",
    "    dates_ordinal = Dates.value.(dates)\n",
    "\n",
    "    # Perform bisquare fit\n",
    "    degree = 3\n",
    "    fitted_params = bisquare_fit(dates_ordinal, heave, degree)\n",
    "\n",
    "    # Calculate fitted values\n",
    "    fitted_heave = sum(fitted_params[i+1] .* dates_ordinal.^i for i in 0:degree)\n",
    "\n",
    "    # Plot the data and the fit\n",
    "    plot(dates, heave, seriestype=:scatter, label=\"Data\")\n",
    "    plot!(dates, fitted_heave, label=\"Bisquare Fit\", lw=2)\n",
    "    xlabel!(\"Date\")\n",
    "    ylabel!(\"Heave\")\n",
    "    title!(\"Bisquare Fit of Heave Data\")\n",
    "end\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using CurveFit\n",
    "using DataFrames\n",
    "using Dates\n",
    "using Plots\n",
    "using Optim\n",
    "\n",
    "# Define the bisquare weight function\n",
    "function bisquare_weights(residuals, c=4.685)\n",
    "    mad = median(abs.(residuals .- median(residuals)))\n",
    "    u = residuals / (c * mad)\n",
    "    weights = (abs.(u) .< 1) .* (1 .- u.^2).^2\n",
    "    return weights\n",
    "end\n",
    "\n",
    "# Define the residual function for least squares fitting\n",
    "function poly_residuals(params, x, y)\n",
    "    degree = length(params) - 1\n",
    "    y_fit = sum(params[i+1] .* x.^i for i in 0:degree)\n",
    "    return y .- y_fit\n",
    "end\n",
    "\n",
    "# Perform robust fitting using bisquare weights\n",
    "function bisquare_fit(x, y, degree)\n",
    "    initial_params = curve_fit(Polynomial, x, y, degree).coeffs\n",
    "    result = optimize(params -> sum(bisquare_weights(poly_residuals(params, x, y)) .* poly_residuals(params, x, y).^2),\n",
    "                      initial_params, BFGS())\n",
    "    return result.minimizer\n",
    "end\n",
    "\n",
    "function main()\n",
    "    # Sample data for demonstration\n",
    "    infil = \"E://hayp//2009-05-10//hayp_2009-05-10T00h00K.hxv\"\n",
    "    date_str = split(infil,\".\")[1]\n",
    "    ll = length(date_str)\n",
    "    start_date = DateTime.(date_str[ll-16:ll-1], \"yyyy-mm-ddTHHhMMZ\")\n",
    "    # Sample data for demonstration\n",
    "    dates = unix2datetime.(datetime2unix.(start_date) .+ (0:sample_rate:sample_length - sample_rate))\n",
    "    heave = wse_df.Heave\n",
    "    \n",
    "    # Convert dates to ordinal for fitting\n",
    "    dates_ordinal = Dates.value.(dates)\n",
    "\n",
    "    # Perform bisquare fit\n",
    "    degree = 3\n",
    "    fitted_params = bisquare_fit(dates_ordinal, heave, degree)\n",
    "\n",
    "    # Calculate fitted values\n",
    "    fitted_heave = sum(fitted_params[i+1] .* dates_ordinal.^i for i in 0:degree)\n",
    "\n",
    "    # Plot the data and the fit\n",
    "    plot(dates, heave, seriestype=:scatter, label=\"Data\")\n",
    "    plot!(dates, fitted_heave, label=\"Bisquare Fit\", lw=2)\n",
    "    xlabel!(\"Date\")\n",
    "    ylabel!(\"Heave\")\n",
    "    title!(\"Bisquare Fit of Heave Data\")\n",
    "end\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wse_df = deepcopy(original_df)\n",
    "\n",
    "using DataFrames, DSP\n",
    "using SavitzkyGolay: savitzky_golay\n",
    "\n",
    "function correct_gps_errors_with_savgol_reverse!(df::DataFrame)\n",
    "###############################################################\n",
    "    \n",
    "    # Define the window size and polynomial order for the Savitzky-Golay filter\n",
    "    default_window_size = 23\n",
    "    polynomial_order = 3\n",
    "\n",
    "    # Find GPS error indices\n",
    "    gps_error_idxs = findall(df.GPS_flag .== 1)\n",
    "\n",
    "    # Process GPS errors in reverse order\n",
    "    for idx in reverse(gps_error_idxs)\n",
    "        # Define the range to apply the filter (120 points before and after the GPS error)\n",
    "        start_idx = max(1, idx - 150)\n",
    "        end_idx = min(size(df, 1), idx + 150)\n",
    "#==\n",
    "        # Get the range of Heave values for smoothing\n",
    "        heave_range = df.Heave[start_idx:end_idx]\n",
    "\n",
    "        # Apply the Savitzky-Golay filter\n",
    "        smoothed_heave = savitzky_golay(heave_range, window_size, polynomial_order).y\n",
    "\n",
    "        # Vectorized operation: Update the Heave values, leaving GPS error point unchanged\n",
    "        valid_idxs = setdiff(start_idx:end_idx, idx)  # Indices excluding the GPS error\n",
    "        df.Heave[valid_idxs] .-= smoothed_heave[valid_idxs .- start_idx .+ 1]  # Apply corrections\n",
    "    end\n",
    "==#\n",
    "\n",
    "        # Adjust window size dynamically if we are near the start or end\n",
    "        actual_window_size = min(default_window_size, end_idx - start_idx + 1)\n",
    "\n",
    "        # Handle cases near the start (insufficient data before the point)\n",
    "        if idx <= actual_window_size ÷ 2\n",
    "            # Use linear interpolation or just fill with a constant for the early points\n",
    "            heave_range = df.Heave[start_idx:end_idx]\n",
    "            smoothed_heave = fill(mean(heave_range), length(heave_range))  # Example: replace with mean\n",
    "        else\n",
    "            # Get the range of Heave values for smoothing\n",
    "            heave_range = df.Heave[start_idx:end_idx]\n",
    "            \n",
    "            # Apply the Savitzky-Golay filter with the adjusted window size\n",
    "            smoothed_heave = savitzky_golay(heave_range, actual_window_size, polynomial_order).y\n",
    "        end\n",
    "\n",
    "        # Vectorized operation: Update the Heave values, leaving GPS error point unchanged\n",
    "        valid_idxs = setdiff(start_idx:end_idx, idx)  # Indices excluding the GPS error\n",
    "        df.Heave[valid_idxs] .-= smoothed_heave[valid_idxs .- start_idx .+ 1]  # Apply corrections\n",
    "\n",
    "    end\n",
    "            \n",
    "end    # correct_gps_errors_with_savgol_reverse!()\n",
    "\n",
    "\n",
    "tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(5))\n",
    "ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "p1 = plot(size=(1200, 600), lw=:1, xlims=(wse_df.Date[1] + Minute(0),wse_df.Date[1] + Minute(30)), ylabel=\"WSE (m)\", #ylims=(minimum(heave), maximum(heave)), \n",
    "          xticks=(tm_tick,ticks), leftmargin = 10Plots.mm, framestyle = :box)\n",
    "\n",
    "# Plot for GPS buoy (all lines)\n",
    "#p1 = plot!(wse_df.Date, wse_df.Heave, lc=:yellow, lw=:1, label=\"Original\")\n",
    "\n",
    "##==\n",
    "# Show GPS errors identified by Datawell\n",
    "gps_flag = findall(==(1), wse_df.GPS_flag)\n",
    "for jj in gps_flag\n",
    "    p1 = vline!([wse_df.Date[jj]], lw=1, c=:red, label=\"\")\n",
    "end\n",
    "#==#\n",
    "\n",
    "correct_gps_errors_with_savgol_reverse!(wse_df)\n",
    "\n",
    "\n",
    "p1 = plot!(wse_df.Date, wse_df.Heave, lc=:blue, lw=:0.5, alpha=:0.5, label=\"Corrected\")\n",
    "\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, DSP\n",
    "\n",
    "function correct_gps_errors_with_savgol_reverse!(df::DataFrame, window_size::Int, polynomial_order::Int)\n",
    "    # Create an array to store the fitted values (smoothed curve)\n",
    "    fit_vals = similar(df.Heave, Float64)\n",
    "    fill!(fit_vals, NaN)  # Initialize with NaN values\n",
    "\n",
    "    gps_error_idxs = findall(df.GPS_flag .== 1)\n",
    "\n",
    "    # Process from the end of the array to the start\n",
    "    for idx in reverse(gps_error_idxs)\n",
    "        # Define the range to fit the curve (120 points before and after)\n",
    "        start_idx = max(1, idx - 120)\n",
    "        end_idx = min(size(df, 1), idx + 120)\n",
    "\n",
    "        # Get the data to fit the curve (this will include the GPS error itself)\n",
    "        range_df = df[start_idx:end_idx, :]\n",
    "\n",
    "        # Apply the Savitzky-Golay filter to the Heave values in this range\n",
    "        smoothed_heave = savitzky_golay(range_df.Heave, window_size, polynomial_order).y  # Correct usage\n",
    "\n",
    "        # Store the smoothed values in fit_vals\n",
    "        for i in start_idx:end_idx\n",
    "            fit_vals[i] = smoothed_heave[i - start_idx + 1]  # Store smoothed value\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Adjust Heave values by subtracting the fitted values (smoothed)\n",
    "    for idx in reverse(gps_error_idxs)\n",
    "        start_idx = max(1, idx - 120)\n",
    "        end_idx = min(size(df, 1), idx + 120)\n",
    "        \n",
    "        for i in start_idx:end_idx\n",
    "            if i != idx  # Do not adjust the NaN point (the GPS error itself)\n",
    "                df.Heave[i] -= fit_vals[i]  # Subtract the fitted value\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return fit_vals  # Return the fitted values (smoothed curve)\n",
    "end\n",
    "\n",
    "# Usage:\n",
    "# wse_df is the DataFrame containing the :Date, :Heave, :GPS_flag columns.\n",
    "window_size = 23  # Adjust as necessary\n",
    "polynomial_order = 3  # Adjust as necessary\n",
    "\n",
    "adjusted_fit = correct_gps_errors_with_savgol_reverse!(wse_df, window_size, polynomial_order)\n",
    "\n",
    "# Now `adjusted_fit` should contain the Savitzky-Golay smoothed values for the affected regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using CSV, DataFrames\n",
    "\n",
    "# Assuming wse_df is your DataFrame\n",
    "CSV.write(\".\\\\wse.csv\", wse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Interpolations\n",
    "using SavitzkyGolay: savitzky_golay\n",
    "using Statistics: mean\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "Date = wse_df.Date\n",
    "Heave = wse_df.Heave\n",
    "GPS_flag = wse_df.GPS_flag\n",
    "\n",
    "\n",
    "function correct_gps_errors!(df::DataFrame)\n",
    "    for row in eachrow(df)\n",
    "        if row.GPS_flag == 1\n",
    "            row.Heave = NaN  # Mark GPS errors with NaN\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function fit_curve_with_savgol(df::DataFrame, window_size::Int, poly_order::Int)\n",
    "    # Correct GPS errors\n",
    "    correct_gps_errors!(df)\n",
    "    \n",
    "    # Remove NaN values (if any)\n",
    "    df = dropmissing(df, :Heave)\n",
    "\n",
    "    # Savitzky-Golay smoothing\n",
    "    smoothed_heave = savitzky_golay(df.Heave, window_size, poly_order)\n",
    "    \n",
    "    return smoothed_heave\n",
    "end\n",
    "\n",
    "# Applying the Savitzky-Golay filter\n",
    "window_size = 31  # You can adjust this\n",
    "poly_order = 3  # Typically 2 or 3\n",
    "fitted_heave = fit_curve_with_savgol(wse_df, window_size, poly_order)\n",
    "\n",
    "tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(5))\n",
    "ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "mean_val = mean(filter(!isnan, wse_df.Heave))\n",
    "\n",
    "# Plot for GPS buoy (all lines)\n",
    "p1 = plot(wse_df.Date, wse_df.Heave, lc=:yellow, lw=:3, ylabel=\"WSE (m)\", label=\"\", \n",
    "          xlims=(wse_df.Date[1] + Minute(0),wse_df.Date[1] + Minute(30)), ylims=(minimum(wse_df.Heave), maximum(wse_df.Heave)), \n",
    "          xticks=(tm_tick,ticks), size=(1200, 600), framestyle=:box)\n",
    "\n",
    "p1 = plot!(wse_df.Date, wse_df.Heave-fitted_heave.y .+ mean_val, lc=:blue, label=\"\")\n",
    "\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save noise floor to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "paths = [\n",
    "        \"C:\\\\Users\\\\Jim\\\\Julia_programs\\\\Datawell\\\\RDT_vector\\\\Data\\\\Noise_floor.bin\",\n",
    "        \"C:\\\\Users\\\\PC1\\\\Julia_programs\\\\Datawell\\\\RDT_vector\\\\Data\\\\Noise_floor.bin\"\n",
    "    ]\n",
    "\n",
    "noise_floor_file = nothing\n",
    "\n",
    "for path in paths\n",
    "    if isfile(path)\n",
    "        noise_floor_file = path\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "if isnothing(noise_floor_file)\n",
    "    println(\"File not found in any of the provided paths. Exiting program.\")\n",
    "    flush(stdout)\n",
    "    exit(1)\n",
    "end\n",
    "\n",
    "println(\"Reading Noise Floor data from \", noise_floor_file)\n",
    "\n",
    "# Deserialize the DataFrame from the file\n",
    "noise_floors_df = read_noise_floor_file(noise_floor_file)\n",
    "\n",
    "# Extract all spectral arrays from the DataFrame\n",
    "spectral_values = noise_floors_df.Pden2\n",
    "\n",
    "# Convert the list of arrays into a matrix where each row is a spectrum\n",
    "spectral_matrix = hcat(spectral_values...)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "noise_floors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "p1 = plot(size=(1000,1000), xaxis=:log, yaxis=:log, xlims=(1e-3,1e0), ylims=(1e-6,1e0),\n",
    "    fg_legend=:false, bg_legend=:transparent, framestyle=:box)\n",
    "\n",
    "f2 = noise_floors_df.f2[1]\n",
    "f2[1] = 0.001\n",
    "\n",
    "for i in 1:size(spectral_matrix, 1)\n",
    "\n",
    "    p1 = plot!(f2, spectral_matrix[i,:], label=\"\")\n",
    "\n",
    "end\n",
    "\n",
    "p1 = plot!(f2, mean(spectral_matrix, dims=1)[1,:], lw=:2, lc=:blue, label=\"Mean\")\n",
    "p1 = plot!(f2, median(spectral_matrix, dims=1)[1,:], lw=:2, lc=:red, label=\"Median\")\n",
    "\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "\n",
    "# Function to identify runs of all-positive or all-negative Heave values\n",
    "function detect_runs_of_errors(df::DataFrame; run_threshold)\n",
    "    # Create a new column for marking detected error runs\n",
    "    df.Error_Run = falses(size(df, 1))\n",
    "    \n",
    "    # Track the start and length of runs of all-positive or all-negative values\n",
    "    run_start = nothing\n",
    "    run_sign = 0\n",
    "    run_length = 0\n",
    "    \n",
    "    for i in 1:size(df, 1)\n",
    "        current_heave = df.Heave[i]\n",
    "        \n",
    "        # Determine the sign of the current Heave value\n",
    "        current_sign = sign(current_heave)\n",
    "        \n",
    "        if current_sign != 0 && (run_sign == 0 || current_sign == run_sign)\n",
    "            # Continue the run\n",
    "            run_length += 1\n",
    "            run_sign = current_sign\n",
    "            if isnothing(run_start)\n",
    "                run_start = i\n",
    "            end\n",
    "        else\n",
    "            # Check if the run exceeds the threshold, if so mark as error\n",
    "            if run_length >= run_threshold && !isnothing(run_start)\n",
    "                df.Error_Run[run_start:i-1] .= true\n",
    "            end\n",
    "            # Reset for a new run\n",
    "            run_start = nothing\n",
    "            run_length = 0\n",
    "            run_sign = current_sign != 0 ? current_sign : 0\n",
    "            run_start = current_sign != 0 ? i : nothing\n",
    "            run_length = current_sign != 0 ? 1 : 0\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Final check at the end of the loop\n",
    "    if run_length >= run_threshold && !isnothing(run_start)\n",
    "        df.Error_Run[run_start:end] .= true\n",
    "    end\n",
    "\n",
    "    return df\n",
    "end\n",
    "\n",
    "# Example usage (adjust `run_threshold` to change the minimum length of a run):\n",
    "wse_df = detect_runs_of_errors(wse_df, run_threshold=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "\n",
    "# Function to detect runs of all-positive or all-negative Heave values with tolerance for sign changes\n",
    "function detect_runs_with_tolerance(df::DataFrame; run_threshold::Int=3, tolerance::Int=1)\n",
    "    # Create a new column to mark detected error runs\n",
    "    df.Error_Run = falses(size(df, 1))\n",
    "    \n",
    "    # Variables to track the start, length, and sign changes in a run\n",
    "    run_start = nothing\n",
    "    run_sign = 0\n",
    "    run_length = 0\n",
    "    sign_changes = 0\n",
    "    \n",
    "    for i in 1:size(df, 1)\n",
    "        current_heave = df.Heave[i]\n",
    "        current_sign = sign(current_heave)\n",
    "        \n",
    "        if current_sign != 0\n",
    "            if run_sign == 0 || current_sign == run_sign\n",
    "                # Continue the run\n",
    "                run_length += 1\n",
    "                if isnothing(run_start)\n",
    "                    run_start = i\n",
    "                end\n",
    "            else\n",
    "                # Allow for tolerated sign changes\n",
    "                if sign_changes < tolerance\n",
    "                    sign_changes += 1\n",
    "                    run_length += 1\n",
    "                else\n",
    "                    # If tolerance is exceeded, finalize the run and reset\n",
    "                    if run_length >= run_threshold && !isnothing(run_start)\n",
    "                        df.Error_Run[run_start:i-1] .= true\n",
    "                    end\n",
    "                    # Reset for a new run\n",
    "                    run_start = i\n",
    "                    run_sign = current_sign\n",
    "                    run_length = 1\n",
    "                    sign_changes = 0\n",
    "                end\n",
    "            end\n",
    "            run_sign = current_sign\n",
    "        else\n",
    "            # Treat zero values as valid points (continue run without changing sign)\n",
    "            if run_sign != 0\n",
    "                run_length += 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Final check at the end of the loop\n",
    "    if run_length >= run_threshold && !isnothing(run_start)\n",
    "        df.Error_Run[run_start:end] .= true\n",
    "    end\n",
    "\n",
    "    return df\n",
    "end\n",
    "\n",
    "# Example usage (adjust `run_threshold` for minimum run length, and `tolerance` for allowed sign changes):\n",
    "wse_df = detect_runs_with_tolerance(wse_df, run_threshold=7, tolerance=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set start time for plots that follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "first_time = wse_df.Date[1] #DateTime(2009,05,25,0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Plots: scatter!\n",
    "using Statistics: mean, cumsum\n",
    "\n",
    "function moving_average(data, window_size)\n",
    "    return [mean(data[max(1, i-window_size):min(end, i+window_size)]) for i in 1:length(data)]\n",
    "end\n",
    "\n",
    "first_time = first_time #DateTime(2009,05,25,0,10)\n",
    "last_time = first_time + Minute(10)\n",
    "suspect = findall(==(true), wse_df.Error_Run)\n",
    "plot(wse_df.Date, wse_df.Heave, lc=:yellow, lw=:2, size=(1200,600), xlims=(first_time,last_time), title=\"\", fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "#scatter!(wse_df.Date[suspect], wse_df.Heave[suspect])\n",
    "\n",
    "using SavitzkyGolay: savitzky_golay\n",
    "\n",
    "window_size = 23\n",
    "polynomial_order = 3\n",
    "\n",
    "# Apply Savitzky-Golay filter\n",
    "sg = savitzky_golay(wse_df.Heave, window_size, polynomial_order)\n",
    "\n",
    "sg_mean = mean(wse_df.Heave .- sg.y)\n",
    "\n",
    "sg_heave = wse_df.Heave .- sg.y\n",
    "#plot!(wse_df.Date,  sg_heave, lw=:1, lc=:blue, label=\"SG Filtered\")\n",
    "\n",
    "width = 10\n",
    "\n",
    "ma_filtered = moving_average(wse_df.Heave, width)\n",
    "plot!(wse_df.Date,  ma_filtered, lw=:2, lc=:red, ls=:dot, alpha=:0.5, label=\"\\nSG Filter\")\n",
    "#plot!(wse_df.Date[suspect],ma_filtered[suspect])\n",
    "plot!(wse_df.Date,  wse_df.Heave .- ma_filtered, lw=:1, lc=:blue, label=\"\\nSG Filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate deviation of running mean from zero line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, Statistics\n",
    "\n",
    "# Function to compute a running mean\n",
    "function running_mean(data::Vector{T}, window_size::Int) where T\n",
    "################################################################\n",
    "    \n",
    "    cumulative_sum = [0; cumsum(data)]  # Compute cumulative sum\n",
    "    mean_vals = (cumulative_sum[window_size+1:end] .- cumulative_sum[1:end-window_size]) ./ window_size\n",
    "    \n",
    "    return vcat(fill(mean(data[1:window_size]), window_size-1), mean_vals)  # Prepend means for start points\n",
    "\n",
    "end    # running_mean()\n",
    "\n",
    "\n",
    "# Function to detect runs of all-positive or all-negative values using the running mean\n",
    "function detect_runs_with_running_mean(df::DataFrame; run_threshold::Int=3, window_size::Int=5, tolerance::Int=1)\n",
    "    #############################################################################################################\n",
    "    \n",
    "    # Compute the running mean for the Heave column\n",
    "    running_mean_heave = running_mean(df.Heave, window_size)\n",
    "    \n",
    "    # Create a new column to mark detected error runs\n",
    "    df.Error_Run = falses(size(df, 1))\n",
    "    \n",
    "    # Variables to track the start, length, and sign changes in a run\n",
    "    run_start = nothing\n",
    "    run_sign = 0\n",
    "    run_length = 0\n",
    "    sign_changes = 0\n",
    "    \n",
    "    for i in 1:size(df, 1)\n",
    "        current_heave = running_mean_heave[i]\n",
    "        current_sign = sign(current_heave)\n",
    "        \n",
    "        if current_sign != 0\n",
    "            if run_sign == 0 || current_sign == run_sign\n",
    "                # Continue the run\n",
    "                run_length += 1\n",
    "                if isnothing(run_start)\n",
    "                    run_start = i\n",
    "                end\n",
    "            else\n",
    "                # Allow for tolerated sign changes\n",
    "                if sign_changes < tolerance\n",
    "                    sign_changes += 1\n",
    "                    run_length += 1\n",
    "                else\n",
    "                    # If tolerance is exceeded, finalize the run and reset\n",
    "                    if run_length >= run_threshold && !isnothing(run_start)\n",
    "                        df.Error_Run[run_start:i-1] .= true\n",
    "                    end\n",
    "                    # Reset for a new run\n",
    "                    run_start = i\n",
    "                    run_sign = current_sign\n",
    "                    run_length = 1\n",
    "                    sign_changes = 0\n",
    "                end\n",
    "            end\n",
    "            run_sign = current_sign\n",
    "        else\n",
    "            # Treat zero values as valid points (continue run without changing sign)\n",
    "            if run_sign != 0\n",
    "                run_length += 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Final check at the end of the loop\n",
    "    if run_length >= run_threshold && !isnothing(run_start)\n",
    "        df.Error_Run[run_start:end] .= true\n",
    "    end\n",
    "\n",
    "    return(df)\n",
    "    \n",
    "end    # detect_runs_with_running_mean()\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "\n",
    "first_time = first_time #DateTime(2009,05,10,0,00)\n",
    "last_time = first_time + Minute(10)\n",
    "\n",
    "#==\n",
    "Given data sampling rate of 1.28 Hz and the GPS error span of ~ 3 minutes:\n",
    "\n",
    "1. Window Size for Running Mean (window_size): 1.28 Hz means there are 1.28 data points per second.\n",
    "    In 3 minutes (180 seconds), you will have approximately: 1.28 × 180 ≈ 230 data points\n",
    "    Set the window_size to cover a smaller period, such as 10–20 seconds, to smooth the data effectively. \n",
    "    \n",
    "    This would correspond to: Window Size≈1.28×10=12.8to1.28×20=25.6\n",
    "    So, a reasonable starting point for the running mean window size might be 15 to 25 data points.\n",
    "\n",
    "2. Run Threshold (run_threshold):\n",
    "    Since a GPS error can span up to 3 minutes, this corresponds to about 230 data points.\n",
    "    The run_threshold should capture a significant part of this error. \n",
    "    Might not want to set it to the full 230 points but a smaller fraction, such as 20% of the full span:\n",
    "    Run Threshold ≈ 0.2 × 230 = 46\n",
    "    Start by setting the run_threshold to 40–50 data points.\n",
    "\n",
    "3. Tolerance (tolerance):\n",
    "    Tolerance allows for minor sign changes within a run. Since GPS errors can span 3 minutes (about 230 points), allow a few sign changes, but not too many.\n",
    "    Setting a tolerance of 1 to 2 sign changes should be reasonable, as this allows for very minor fluctuations while keeping the overall run detection robust.\n",
    "==#\n",
    "\n",
    "window_size = 25  # Adjust window size for the running mean\n",
    "run_threshold = 45  # Minimum run length\n",
    "tolerance = 2  # Allowed number of sign changes\n",
    "\n",
    "wse_df = detect_runs_with_running_mean(wse_df, run_threshold=run_threshold, window_size=window_size, tolerance=tolerance)\n",
    "\n",
    "suspect = findall(==(true), wse_df.Error_Run)\n",
    "\n",
    "title=\"Locate deviation of running mean from zero\"\n",
    "\n",
    "plot(wse_df.Date, wse_df.Heave, lc=:yellow, lw=:2, label=\"Original Heave\", size=(1200,600), \n",
    "    xlims=(first_time,last_time), title=title, fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "scatter!(wse_df.Date[suspect], wse_df.Heave[suspect], marker=:x, ms=:2, label=\"Suspects\")\n",
    "\n",
    "function moving_average(data, window_size)\n",
    "    return [mean(data[max(1, i-window_size):min(end, i+window_size)]) for i in 1:length(data)]\n",
    "end\n",
    "width = 25\n",
    "ma_filtered = moving_average(wse_df.Heave[suspect], width)\n",
    "plot!(wse_df.Date[suspect],  ma_filtered, lw=:2, lc=:red, ls=:dot, alpha=:0.5, label=\"Moving average filter\")\n",
    "plot!(wse_df.Date[suspect],  wse_df.Heave[suspect] .- ma_filtered, lw=:1, lc=:blue, label=\"Filtered\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit SavitzkyGolay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set initial time for closer inspection\n",
    "first_time = first_time #DateTime(2009,05,10,0,00)\n",
    "last_time = first_time + Minute(10)\n",
    "\n",
    "subset = findall(first_time.<= wse_df.Date .< last_time);\n",
    "\n",
    "using SavitzkyGolay: savitzky_golay\n",
    "using Plots: plot\n",
    "\n",
    "window_size = 23\n",
    "polynomial_order = 3\n",
    "\n",
    "# Apply Savitzky-Golay filter\n",
    "sg = savitzky_golay(wse_df.Heave, window_size, polynomial_order)\n",
    "\n",
    "title=\"SavitzkyGolay \"*string(window_size)\n",
    "\n",
    "plot(wse_df.Date, wse_df.Heave, label=\"\", lw=:1, lc=:yellow, size=(1200,600), xlims=(first_time, last_time), \n",
    "    title=title, fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "\n",
    "if is_gps   \n",
    "    # Find indices of all values equal to 1 (represents Datawell GPS flag)\n",
    "    gps_flag = findall(==(1), wse_df.GPS_flag)\n",
    "    # Show GPS errors identified by Datawell\n",
    "    for jj in gps_flag\n",
    "        vline!([wse_df.Date[jj]], lw=1, c=:red, label=\"\")\n",
    "    end\n",
    "    annotate!(first_time, maximum(wse_df.Heave[subset])*0.9, text(string(length(gps_flag),\" GPS errors flagged\"), :left, 12))\n",
    "end\n",
    "\n",
    "sg_heave = wse_df.Heave .- sg.y\n",
    "plot!(wse_df.Date,  sg_heave, lw=:1, lc=:blue, label=\"SG Filtered\")\n",
    "plot!(wse_df.Date,  sg.y, label=\"Filter\", lw=:2, lc=:red, ls=:dot, alpha=:0.5)\n",
    "using Plots: hline!\n",
    "\n",
    "mean_line = mean(wse_df.Heave .- sg.y)\n",
    "hline!([mean_line],lc=:green, label=\"M.W.L.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SavitzkyGolay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using SavitzkyGolay: savitzky_golay\n",
    "using Plots: plot, hline!\n",
    "\n",
    "window_size = 23\n",
    "polynomial_order = 3\n",
    "\n",
    "# Apply Savitzky-Golay filter\n",
    "sg = savitzky_golay(wse_df.Heave, window_size, polynomial_order)\n",
    "title = Dates.format(first_time, \"yyyy-mm-dd HH:MM\") * \" SavitzkyGolay \"*string(window_size)\n",
    "\n",
    "p1 = plot(wse_df.Date, wse_df.Heave, lc=:yellow, lw=:2, label=\"\", size=(1200,600), xlims=(first_time, last_time), \n",
    "    title=title, fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "\n",
    "if is_gps   \n",
    "    # Find indices of all values equal to 1 (represents Datawell GPS flag)\n",
    "    gps_flag = findall(==(1), wse_df.GPS_flag)\n",
    "    # Show GPS errors identified by Datawell\n",
    "    for jj in gps_flag\n",
    "        vline!([wse_df.Date[jj]], lw=1, c=:red, label=\"\")\n",
    "    end\n",
    "    annotate!(first_time, maximum(wse_df.Heave[subset])*0.9, text(\"  \"*string(length(gps_flag),\" GPS errors flagged\"), :left, 12))\n",
    "end\n",
    "\n",
    "p1 = plot!(wse_df.Date,  sg.y, label=\"\\nFilter\", lw=:2, lc=:red, ls=:dot, alpha=:0.5)\n",
    "p1 = plot!(wse_df.Date, sg_heave, lc=:blue, lw=:1, label=\"\\nMa Filtered\")\n",
    "\n",
    "mean_line = mean(fixed_heave)\n",
    "p1 = hline!([mean_line],lc=:green, label=\"M.W.L.\")\n",
    "\n",
    "display(p1)\n",
    "\n",
    "ps_w = welch_pgram(wse_df.Heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "f2 = freq(ps_w)\n",
    "Pden2 = power(ps_w)\n",
    "\n",
    "ps_w = welch_pgram(fixed_heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "fixed_f2 = freq(ps_w)\n",
    "fixed_Pden2 = power(ps_w)\n",
    "\n",
    "ps_w = welch_pgram(sg_heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "sg_f2 = freq(ps_w)\n",
    "sg_Pden2 = power(ps_w)\n",
    "\n",
    "p2 = plot(f2, Pden2, lc=:grey, lw=:0.5, alpha=1, label=\"Orig.\", fillrange=0, fillcolor=:yellow, fillalpha=0.1, xlim=(0,0.64), title=title,\n",
    "    ylim=(0,Inf), xlabel=\"Frequency (Hz)\", ylabel=\"S(f) (m²/Hz)\", fg_legend=:transparent, bg_legend=:transparent, size=(800,800), framestyle = :box)\n",
    "\n",
    "#p2 = plot!(fixed_f2, fixed_Pden2, lc=:blue, lw=:2, alpha=1, label=\"JW Filter\")\n",
    "\n",
    "p2 = plot!(sg_f2, sg_Pden2, lc=:red, lw=:1, alpha=1, label=\"\\nSg Filter\\n\")\n",
    "\n",
    "p2 = plot!(f2, median_spectra_vector, lw=:2, lc=:red, fillrange=0, fillalpha=0.075, fillcolor=:red, label=\"\\nMedian Noise Floor\")\n",
    "\n",
    "display(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Statistics: median, mean\n",
    "\n",
    "function moving_average(data, window_size)\n",
    "    return [mean(data[max(1, i-window_size):min(end, i+window_size)]) for i in 1:length(data)]\n",
    "end\n",
    "\n",
    "width = 11\n",
    "\n",
    "ma_filtered = moving_average(wse_df.Heave, width)\n",
    "ma_heave = wse_df.Heave .- ma_filtered\n",
    "\n",
    "title = Dates.format(first_time, \"yyyy-mm-dd HH:MM\") * \" Moving average \"*string(width)\n",
    "\n",
    "p1 = plot(wse_df.Date, wse_df.Heave, label=\"\", lw=:2, lc=:yellow, size=(1200,600), xlims=(first_time, last_time), \n",
    "    title=title, fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "p1 = plot!(wse_df.Date,  ma_filtered, label=\"\\nMa Filter\", lw=:2, lc=:red, ls=:dot, alpha=:0.5)\n",
    "p1 = plot!(wse_df.Date,  ma_heave, lw=:1, lc=:blue, label=\"\\nMa Filtered\")\n",
    "\n",
    "mean_line = median(ma_heave)\n",
    "p1 = hline!([mean_line],lc=:green, label=\"M.W.L.\")\n",
    "\n",
    "display(p1)\n",
    "\n",
    "ps_w = welch_pgram(wse_df.Heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "f2 = freq(ps_w)\n",
    "Pden2 = power(ps_w)\n",
    "\n",
    "ps_w = welch_pgram(ma_heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "ma_f2 = freq(ps_w)\n",
    "ma_Pden2 = power(ps_w)\n",
    "\n",
    "p2 = plot(f2, Pden2, lc=:grey, lw=:0.5, alpha=1, label=\"Orig.\", fillrange=0, fillcolor=:yellow, fillalpha=0.1, xlim=(0,0.64), title=title,\n",
    "    ylim=(0,Inf), xlabel=\"Frequency (Hz)\", ylabel=\"S(f) (m²/Hz)\", fg_legend=:transparent, bg_legend=:transparent, size=(800,800), framestyle = :box)\n",
    "\n",
    "#p2 = plot!(fixed_f2, fixed_Pden2, lc=:blue, lw=:2, alpha=1, label=\"JW Filter\")\n",
    "\n",
    "p2 = plot!(ma_f2, ma_Pden2, lc=:red, lw=:1, alpha=1, label=\"\\nMa Filter\\n\")\n",
    "p2 = plot!(f2, median_spectra_vector, lw=:2, lc=:red, fillrange=0, fillalpha=0.075, fillcolor=:red, label=\"\\nMedian Noise Floor\")\n",
    "\n",
    "display(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FastRunningMedian() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using FastRunningMedian: running_median\n",
    "\n",
    "width = 11\n",
    "medi_filter = running_median(wse_df.Heave, width)\n",
    "medi_heave = wse_df.Heave-medi_filter\n",
    "\n",
    "title = Dates.format(first_time, \"yyyy-mm-dd HH:MM\") * \" Fast Running Median \"*string(width)\n",
    "\n",
    "p1 = plot(wse_df.Date, wse_df.Heave, lc=:yellow, lw=:2, label=\"\", size=(1200,600), xlims=(first_time, last_time), \n",
    "    title=title, fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "p1 = plot!(wse_df.Date, medi_heave, lc=:blue, lw=:1, label=\"\\nMedi Filtered\")\n",
    "p1 = plot!(wse_df.Date,  ma_filtered, label=\"\\nMa Filter\", lw=:2, lc=:red, ls=:dot, alpha=:0.5)\n",
    "\n",
    "mean_line = median(medi_heave)\n",
    "p1 = hline!([mean_line],lc=:green, label=\"M.W.L.\")\n",
    "\n",
    "display(p1)\n",
    "\n",
    "ps_w = welch_pgram(wse_df.Heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "f2 = freq(ps_w)\n",
    "Pden2 = power(ps_w)\n",
    "\n",
    "ps_w = welch_pgram(medi_heave, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "medi_f2 = freq(ps_w)\n",
    "medi_Pden2 = power(ps_w)\n",
    "\n",
    "p2 = plot(f2, Pden2, lc=:grey, lw=:0.5, alpha=1, label=\"Orig.\", fillrange=0, fillcolor=:yellow, fillalpha=0.1, xlim=(0,0.64), title=title,\n",
    "    ylim=(0,Inf), xlabel=\"Frequency (Hz)\", ylabel=\"S(f) (m²/Hz)\", fg_legend=:transparent, bg_legend=:transparent, size=(800,800), framestyle = :box)\n",
    "\n",
    "#p2 = plot!(fixed_f2, fixed_Pden2, lc=:blue, lw=:2, alpha=1, label=\"JW Filter\")\n",
    "\n",
    "p2 = plot!(medi_f2, medi_Pden2, lc=:red, lw=:1, alpha=1, label=\"\\nMedi Filter\\n\")\n",
    "p2 = plot!(f2, median_spectra_vector, lw=:2, lc=:red, fillrange=0, fillalpha=0.075, fillcolor=:red, label=\"\\nMedian Noise Floor\")\n",
    "\n",
    "display(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Daubechies Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Wavelets\n",
    "\n",
    "wavelet_type = WT.db4\n",
    "level = 4\n",
    "\n",
    "# Perform wavelet denoising\n",
    "wavelet_filtered = Wavelets.denoise(wse_df.Heave, wavelet(wavelet_type), L=level)\n",
    "\n",
    "plot(wse_df.Date, wse_df.Heave, label=\"\", lw=:1, lc=:yellow, size=(1200,600), xlims=(first_time, last_time),\n",
    "    title = Dates.format(first_time, \"yyyy-mm-dd HH:MM\") * \" Daubechies Wavelet - Level \"*string(level), fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "plot!(wse_df.Date,  wse_df.Heave .- wavelet_filtered, lc=:blue, label=\"Filtered\")\n",
    "plot!(wse_df.Date,  wavelet_filtered, label=\"\\nFilter\", lw=:2, lc=:red, ls=:dot, alpha=:0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Symlet Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Wavelets\n",
    "\n",
    "wavelet_type = WT.sym4\n",
    "level = 4\n",
    "\n",
    "# Perform wavelet denoising\n",
    "wavelet_filtered = Wavelets.denoise(wse_df.Heave, wavelet(wavelet_type), L=level)\n",
    "\n",
    "plot(wse_df.Date, wse_df.Heave, label=\"\", lw=:1, lc=:yellow, size=(1200,600), xlims=(first_time, last_time),\n",
    "    title = Dates.format(first_time, \"yyyy-mm-dd HH:MM\") * \" Symlet Wavelet - Level \"*string(level), fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "plot!(wse_df.Date,  wse_df.Heave .- wavelet_filtered, lc=:blue, label=\"Filtered\")\n",
    "plot!(wse_df.Date,  wavelet_filtered, label=\"\\nFilter\", lw=:2, lc=:red, ls=:dot, alpha=:0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Coiflet Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Wavelets\n",
    "\n",
    "wavelet_type = WT.coif4\n",
    "level = 4\n",
    "\n",
    "# Perform wavelet denoising\n",
    "wavelet_filtered = Wavelets.denoise(wse_df.Heave, wavelet(wavelet_type), L=level)\n",
    "\n",
    "plot(wse_df.Date, wse_df.Heave, label=\"\", lw=:1, lc=:yellow, size=(1200,600), xlims=(first_time, last_time),\n",
    "    title = Dates.format(first_time, \"yyyy-mm-dd HH:MM\") * \" Coiflet Wavelet - Level \"*string(level), fg_legend=:transparent, bg_legend=:transparent, framestyle = :box)\n",
    "plot!(wse_df.Date,  wse_df.Heave .- wavelet_filtered, lc=:blue, label=\"Filtered\")\n",
    "plot!(wse_df.Date,  wavelet_filtered, label=\"\\nFilter\", lw=:2, lc=:red, ls=:dot, alpha=:0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DSP, ContinuousWavelets, Plots, Wavelets, FFTW\n",
    "using Dates: Microsecond\n",
    "\n",
    "function plot_scaleogram(wse_df, condition)\n",
    "################################################\n",
    "    heave = wse_df.Heave\n",
    "    n=length(wse_df.Heave);\n",
    "    t = range(1,n/1.28,length=n);\n",
    "\n",
    "    c = wavelet(Morlet(8), β=0.75);\n",
    "    res = ContinuousWavelets.cwt(heave, c)\n",
    "\n",
    "    freqs = getMeanFreq(ContinuousWavelets.computeWavelets(n, c)[1])\n",
    "#    freqs[1] = 0\n",
    "\n",
    "    start_date = first(wse_df.Date)\n",
    "    last_date = last(wse_df.Date)\n",
    "\n",
    "    # display plots to screen\n",
    "    tm_tick = range(start_date,last_date,step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "    p1 = heatmap(wse_df.Date, ((freqs.-minimum(freqs))./maximum(freqs)).*0.64, abs.(res)', c=cgrad(:Spectral, rev=true), xlims=(start_date,last_date))                \n",
    "\n",
    "    for i in 0:0.1:0.6\n",
    "        hline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "    end\n",
    "\n",
    "    for i in start_date:Minute(5):last_date\n",
    "        vline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "    end\n",
    "\n",
    "    # Plot spectrogram over scalogram\n",
    "    nw=128;\n",
    "    spec = DSP.Periodograms.spectrogram(heave, nw, 120; fs=1.28,window=hanning);\n",
    "\n",
    "    # display plots to screen\n",
    "    tm_tick = range(start_date,last_date,step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "\n",
    "    p1 = plot!(first(wse_df.Date) + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, DSP.Periodograms.power(spec), lw=1, c=cgrad(:Spectral, rev=true), colorbar=false) \n",
    "    title = Dates.format.(wse_df.Date[1],\"yyyy-mm-dd HH:MM\") * condition * \" Scaleogram\"\n",
    "    plot_wavelet = plot(p1, \n",
    "        xlabel=\"Time\", xlims=(start_date,last_date), xticks=(tm_tick,ticks), xtickfontsize=7,\n",
    "        ylabel=\"Frequency (Hz)\", ylim=(0,0.64), ytickfontsize=8, \n",
    "        title=title, framestyle = :box,\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1400, 500), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "    \n",
    "    # display plots to screen\n",
    "    display(plot_wavelet)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # plot_scaleogram()\n",
    "\n",
    "using DSP\n",
    "\n",
    "if gps_errors_count == 0\n",
    "\n",
    "    plot_scaleogram(wse_df, \"\")\n",
    "\n",
    "else\n",
    "\n",
    "    plot_scaleogram(wse_df, \" Original\")\n",
    "    \n",
    "    fixed_df = deepcopy(wse_df)\n",
    "    fixed_df.Heave = fixed_heave\n",
    "    \n",
    "    plot_scaleogram(fixed_df, \" Fixed\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Available memory in Gb\n",
    "Sys.free_memory() / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr()\n",
    "using DSP\n",
    "\n",
    "function plot_spectrogram(wse_df, condition)\n",
    "################################################\n",
    "\n",
    "    nw=128;\n",
    "    spec = DSP.Periodograms.spectrogram(wse_df.Heave, nw, round(Int, nw*0.9); fs=1.28,window=hanning);\n",
    "    power_spec = DSP.Periodograms.power(spec)\n",
    "    max_spec = maximum(power_spec)\n",
    "    \n",
    "    start_date = first(wse_df.Date)\n",
    "    last_date = last(wse_df.Date)\n",
    "    \n",
    "    # display plots to screen\n",
    "    tm_tick = range(first(wse_df.Date),last(wse_df.Date),step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "    \n",
    "    #p1 = heatmap(first(wse_df.Date) + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, power_spec, lw=0.25, c=cgrad(:Spectral, rev=true), clims=(0.0,max_spec), levels=10, fill=true)\n",
    "    p1 = contourf(first(wse_df.Date) + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, power_spec, lw=0.25, c=cgrad(:Spectral, rev=true), clims=(0,max_spec), levels=10, fill=true)\n",
    "    \n",
    "    # draw grid lines on plot\n",
    "    for i in 0:0.1:0.6\n",
    "        hline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "    end\n",
    "    \n",
    "    for i in start_date:Minute(5):last_date\n",
    "        vline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "    end\n",
    "    \n",
    "    title = Dates.format.(wse_df.Date[1],\"yyyy-mm-dd HH:MM\") * condition * \" Spectrogram\"\n",
    "    plot_file = replace(\".\\\\Plots\\\\\" * replace(title, \" \" => \"_\") * \".png\", \":\" => \"\")\n",
    "    \n",
    "    plot_p1 = plot(p1, xlabel=\"Time\", xlim=(start_date,last_date), xticks=(tm_tick,ticks), xtickfontsize=7,\n",
    "            ylabel=\"Frequency (Hz)\", ylim=(0,0.64), ytickfontsize=8, \n",
    "            title=title, framestyle = :box,\n",
    "            leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1600,500), gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, colorbar=true)\n",
    "#==    \n",
    "    try\n",
    "        # Output plot file name\n",
    "        savefig(plot_file)\n",
    "        println(\"\\nPlot file saved as \",plot_file)\n",
    "    catch\n",
    "        \"Alert: Plot not saved!\"\n",
    "    end\n",
    "==#    \n",
    "    display(plot_p1)\n",
    "\n",
    "    return()\n",
    "\n",
    "end\n",
    "\n",
    "gps_errors_count = 0\n",
    "if gps_errors_count == 0\n",
    "\n",
    "    plot_spectrogram(wse_df, \"\")\n",
    "\n",
    "else\n",
    "\n",
    "    plot_spectrogram(wse_df, \" Original\")\n",
    "    \n",
    "    fixed_df = deepcopy(wse_df)\n",
    "    fixed_df.Heave = fixed_heave\n",
    "    \n",
    "    plot_spectrogram(fixed_df, \" Fixed\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore different spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using FFTW\n",
    "\n",
    "function calc_power_spectrum(signal)\n",
    "#####################################\n",
    "    \n",
    "    N = length(signal)\n",
    "    dt = 1/sample_frequency\n",
    "    \n",
    "    f = fft(signal) |> fftshift\n",
    "    freqs = fftfreq(N, 1/dt) |> fftshift\n",
    "\n",
    "    Sxx = 2 * dt ^ 2 / N * (f .* conj(f))\n",
    "\n",
    "    pos_vals = findall( x -> x >= 0, freqs)\n",
    "    \n",
    "    return(freqs[pos_vals],Sxx[pos_vals])\n",
    "    \n",
    "end    # calc_power_spectrum()\n",
    "\n",
    "\n",
    "sample_frequency = 1.28\n",
    "\n",
    "# signal \n",
    "heave = wse_df.Heave\n",
    "t = 1:length(heave)\n",
    "\n",
    "freqs,Pden1 = calc_power_spectrum(wse_df.Heave)\n",
    "ps_w = welch_pgram(heave, 256, 0; onesided=true, nfft=256, fs=sample_frequency, window=hanning);\n",
    "Pden2 = power(ps_w)\n",
    "f2 = freq(ps_w)\n",
    "\n",
    "time_domain = plot(t, heave, label=\"\", title = \"Heave\", xlim=(0, 1800))\n",
    "freq_domain = plot(freqs,real(Pden1), title = \"Spectrum\", xlim=(0, 0.64), ylim=(0,maximum(real(Pden1)*1.05))) \n",
    "freq_domain = plot!(f2,Pden2,lw=2,c=:red) \n",
    "\n",
    "plot(time_domain, freq_domain, layout = (2,1), label=\"\", framestyle = :box,\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1400, 800), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using FFTW, Statistics, Plots\n",
    "\n",
    "# Assuming surface_elevations is your 30 minutes of water surface elevations \n",
    "surface_elevations = heave  # replace with your actual data\n",
    "\n",
    "# Apply FFT to compute the spectrum\n",
    "n = length(surface_elevations)\n",
    "frequencies = fftfreq(n, 1/1.28)\n",
    "Pxx = abs.(fft(surface_elevations).^2 / n)\n",
    "\n",
    "# We just want the positive frequencies from the FFT result\n",
    "mask = frequencies .>= 0\n",
    "frequencies = frequencies[mask]\n",
    "Pxx = Pxx[mask]\n",
    "\n",
    "# Plot the Power Spectrum\n",
    "plot(frequencies, Pxx)\n",
    "plot!(f2,Pden2,lw=2,c=:red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fftfreq(2048, 1/1.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "freqs,Pden1 = calc_power_spectrum(wse_df.Heave[2048])\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "function calc_csd(X,Y)\n",
    "#####################################\n",
    "\n",
    "    N = length(wse_df.Heave)\n",
    "    dt = 1/sample_frequency\n",
    "\n",
    "    x = fft(X) |> fftshift\n",
    "    y = fft(Y) |> fftshift\n",
    "\n",
    "    return(2 * dt ^ 2 / N * (x .* conj(y)))\n",
    "    \n",
    "end    # calc_csd()\n",
    "\n",
    "\n",
    "Pden1 = calc_csd(wse_df.Heave,wse_df.Heave)\n",
    "f1 = fftfreq(N, 1/dt) |> fftshift\n",
    "\n",
    "Pden2 = power(ps_w)\n",
    "f2 = freq(ps_w)\n",
    "\n",
    "freq_domain = plot(f1,real(Pden1), xlim=(0, 0.64), ylim=(0,maximum(real(Pden1)*1.05)), label = \"CSD\")\n",
    "freq_domain = plot!(f2,Pden2,lw=2,c=:blue, label=\"Welch\")\n",
    "\n",
    "plot(freq_domain, layout = (1,1), label=\"\", framestyle = :box,\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1400, 600), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate calculating cross-spectra (in order to calculate Fourier coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using FFTW\n",
    "using DSP\n",
    "\n",
    "\n",
    "function calc_csd(X,Y,sample_frequency)\n",
    "#####################################\n",
    "\n",
    "    N = length(X)\n",
    "    dt = 1/sample_frequency\n",
    "\n",
    "    x = fftshift(fft(X))\n",
    "    y = fftshift(fft(Y))\n",
    "\n",
    "    return((2 * dt^2 / N) .* (x .* conj(y)) .* sample_frequency)\n",
    "    \n",
    "end    # calc_csd()\n",
    "\n",
    "\n",
    "function calc_csd_welch(X,Y,sample_frequency,len,olap)\n",
    "#####################################\n",
    "\n",
    "    segmentsX = arraysplit(X,len,olap)\n",
    "    segmentsY = arraysplit(Y,len,olap)\n",
    "\n",
    "#    N = length(X)\n",
    "    dt = 1/sample_frequency\n",
    "    \n",
    "    csd = []\n",
    "\n",
    "    for i in eachindex(segmentsX)\n",
    "        \n",
    "        x = fftshift(fft(segmentsX[i] )) \n",
    "        y = fftshift(fft(segmentsY[i] ))\n",
    "\n",
    "        push!(csd, (2 * dt^2 / len) .* (x .* conj(y)) .* sample_frequency .* tukey)\n",
    "\n",
    "    end\n",
    "        \n",
    "    return(mean(csd,dims=1)[1])\n",
    "    \n",
    "end    # calc_csd_welch()\n",
    "\n",
    "\n",
    "N = length(wse_df.Heave)\n",
    "sample_frequency = 1.28\n",
    "dt = 1/sample_frequency\n",
    "global tukey = DSP.Windows.tukey(256,66/256)\n",
    "\n",
    "Cvv = real(calc_csd(wse_df.Heave,wse_df.Heave,sample_frequency))\n",
    "Cnn = real(calc_csd(wse_df.North,wse_df.North,sample_frequency))\n",
    "Cww = real(calc_csd(wse_df.West,wse_df.West,sample_frequency))\n",
    "Cnw = real(calc_csd(wse_df.North,wse_df.West,sample_frequency))\n",
    "Qvn = imag(calc_csd(wse_df.Heave,wse_df.North,sample_frequency))\n",
    "Qvw = imag(calc_csd(wse_df.Heave,wse_df.West,sample_frequency));\n",
    "\n",
    "csd_vv = calc_csd_welch(wse_df.Heave,wse_df.Heave,sample_frequency,256,0)\n",
    "csd_nn = calc_csd_welch(wse_df.North,wse_df.North,sample_frequency,256,0)\n",
    "csd_ww = calc_csd_welch(wse_df.West,wse_df.West,sample_frequency,256,0)\n",
    "csd_nw = calc_csd_welch(wse_df.North,wse_df.West,sample_frequency,256,0)\n",
    "csd_vn = calc_csd_welch(wse_df.Heave,wse_df.North,sample_frequency,256,0)\n",
    "csd_vw = calc_csd_welch(wse_df.Heave,wse_df.West,sample_frequency,256,0)\n",
    "\n",
    "f1 = fftshift(fftfreq(N, sample_frequency))\n",
    "f1_pos_vals = findall( x -> x >= 0, f1 )\n",
    "\n",
    "f2 = fftshift(fftfreq(length(csd_vv), 1/dt))\n",
    "f2_pos_vals = findall( x -> x >= 0, f2 )\n",
    "\n",
    "ps_w = welch_pgram(wse_df.Heave, 256, 0; fs=sample_frequency, window=tukey)\n",
    "Pden3 = ps_w.power\n",
    "f3 = ps_w.freq\n",
    "\n",
    "freq_domain = plot(f1[f1_pos_vals], Cvv[f1_pos_vals], lw=1, c=:lightblue, label = \"CSD calc\")\n",
    "freq_domain = plot!(f2[f2_pos_vals], real(csd_vv[f2_pos_vals]), lw=4, c=:yellow, label=\"Welch calc.\")\n",
    "freq_domain = plot!(f3, Pden3, lw=2, c=:red, label=\"DSP Welch\")\n",
    "\n",
    "\n",
    "spectral_plots = plot(freq_domain, layout = (1,1), label=\"\", framestyle = :box, fg_legend = :false,\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1400, 600), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "\n",
    "display(spectral_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and plot Fourier coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the co- and quad- spectra\n",
    "cvv = real(csd_vv[f2_pos_vals])\n",
    "cnn = real(csd_nn[f2_pos_vals])\n",
    "cww = real(csd_ww[f2_pos_vals])\n",
    "cnw = real(csd_nw[f2_pos_vals])\n",
    "qvn = imag(csd_vn[f2_pos_vals])\n",
    "qvw = imag(csd_vw[f2_pos_vals])\n",
    "\n",
    "# Calculate the Fourier Coefficients\n",
    "a1 = qvn ./ (cvv .* (cnn .+ cww)).^0.5\n",
    "b1 = qvw ./ (cvv .* (cnn .+ cww)).^0.5\n",
    "\n",
    "a2 = (cnn .- cww) ./ (cnn .+ cww)\n",
    "b2 = (2 .* cnw) ./ (cnn .+ cww);\n",
    "\n",
    "# check for any Nans in the data\n",
    "replace!(a1, NaN=>0)\n",
    "replace!(b1, NaN=>0)\n",
    "replace!(a2, NaN=>0)\n",
    "replace!(b2, NaN=>0);\n",
    "\n",
    "# Calculate the Centred Fourier Coefficients\n",
    "θ₀ = atan.(b1,a1)\n",
    "m1 = (a1.^2 .+ b1.^2).^0.5\n",
    "m2 = a2.*cos.(2*θ₀) .+ b2.*sin.(2*θ₀)\n",
    "n2 = -a2.*sin.(2*θ₀) .+ b2.*cos.(2*θ₀)\n",
    "\n",
    "# Calculate the spread\n",
    "σc = (2 .* (1 .- m1)).^0.5;\n",
    "\n",
    "N = length(wse_df.Heave)\n",
    "f1 = 0.005:0.005:0.64\n",
    "\n",
    "p1 = plot(f1,a1, lw=2, c=:blue, label=\"a₁\", fg_legend = :false, title=(\"a₁ and b₁\"))\n",
    "p1 = plot!(f1,b1, lw=2, c=:red, label=\"b₁\")\n",
    "\n",
    "p2 = plot(f1,a2, lw=2, c=:blue, label=\"a₂\", fg_legend = :false, title=(\"a₂ and b₂\"))\n",
    "p2 = plot!(f1,b2, lw=2, c=:red, label=\"b₂\")\n",
    "\n",
    "p3 = plot(f1,m1, lw=2, c=:blue, label=\"m₁\", fg_legend = :false, title=(\"m₁ and m₂\"))\n",
    "p3 = plot!(f1,m2, lw=2, c=:red, label=\"m₂\")\n",
    "\n",
    "p4 = plot(f1,n2, lw=2, c=:blue, label=\"n₂\", fg_legend = :false, title=(\"n₂\"))\n",
    "\n",
    "coefficients_plot = plot(p1, p2, p3, p4, layout = (2,2), framestyle = :box, titlefontsize=10, xlabel=\"Frequency (Hz)\", xlabelfontsize=9,\n",
    "        leftmargin = 12Plots.mm, bottommargin = 12Plots.mm, grid=true, size=(1400, 800), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "\n",
    "display(coefficients_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DSP\n",
    "\n",
    "tukey = DSP.Windows.tukey(256,66/256)\n",
    "tukey1 = DSP.Windows.tukey(2304,594/2304)\n",
    "#p1 = plot(f1[f1_pos_vals], Cvv[f1_pos_vals], lw=4, c=:yellow, label = \"CSD calc\")\n",
    "#p1 = plot!(freq(periodogram(wse_df.Heave,onesided=true,nfft=2304,fs=sample_frequency, window=tukey1)),power(periodogram(wse_df.Heave,onesided=true,nfft=2304,fs=sample_frequency, window=tukey1)),lw=1,c=:pink)\n",
    "p1 = plot(freq(welch_pgram(wse_df.Heave, 256, 0; fs=sample_frequency, window=tukey)),power(welch_pgram(wse_df.Heave, 256, 0; fs=sample_frequency, window=tukey)),c=:red,lw=3, label=\"DSP Welch\")\n",
    "p1 = plot!(f2[f2_pos_vals], real(csd_vv[f2_pos_vals]), lw=2, c=:yellow, label=\"Welch calc. JW\")\n",
    "\n",
    "plot(p1, framestyle = :box,\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1400, 600), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = (rad2deg.(π/2 .- atan.(b1,a1)) .+ 450).%360\n",
    "\n",
    "if is_gps\n",
    "    dir_type = \" True\"\n",
    "else\n",
    "    dir_type = \" Magnetic\"\n",
    "end\n",
    "\n",
    "p1 = plot(f2[f2_pos_vals], direction, lw=3, c=:blue, yflip=true, ylim=[0,360], yticks = 0:30:360, grid=true, label=\"Direction\", ylabel=\"Direction (\"*L\"^o\"*dir_type*\")\", legend=:topright)\n",
    "p1 = plot!(f2[f2_pos_vals], direction .+ rad2deg.(σc), fillrange = direction .- rad2deg.(σc), fillalpha = 0.25, lw=0, c = 1, label = \"Spread\")\n",
    "\n",
    "max_spec = max(maximum(power(welch_pgram(wse_df.Heave, 256, 0; fs=sample_frequency, window=tukey))),maximum(real(csd_vv[f2_pos_vals]))) * 1.05\n",
    "\n",
    "p1 = plot!(twinx(),freq(welch_pgram(wse_df.Heave, 256, 0; fs=sample_frequency, window=tukey)),ylabel=\"Spectral Density (m\"*L\"^2\"*\"/Hz.)\",\n",
    "    power(welch_pgram(wse_df.Heave, 256, 0; fs=sample_frequency, window=tukey)),c=:red,lw=3, label=\"DSP Welch\\n\", ylim=[0,max_spec], legend=:bottomright)\n",
    "p1 = plot!(twinx(),f2[f2_pos_vals], real(csd_vv[f2_pos_vals]), lw=2, c=:yellow, label=\"Welch calc. JW\", ylim=[0,max_spec], legend=:bottomright)\n",
    "\n",
    "p_all = plot(p1, framestyle = :box, xlim=[0,0.64], title=\"Spectra and Direction\", xlabel=\"Frequency (Hz)\",\n",
    "        leftmargin = 15Plots.mm, rightmargin = 25Plots.mm, bottommargin = 15Plots.mm, size=(1400, 600), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, fg_legend = :false)\n",
    "\n",
    "display(p_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Spectrum file data to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "function process_vector_1(i)\n",
    "# Identify the system file word number and system file word\n",
    "    \n",
    "    word_number = parse(Int, SubString.(i, 1, 1), base=16)*16^0\n",
    "    word = parse(Int, SubString.(i, 2, 2), base=16)*16^2 + parse(Int, SubString.(i, 3, 3), base=16)*16^1 + parse(Int, SubString.(i, 4, 4), base=16)*16^0\n",
    "    println(word_number,' ',word)\n",
    "    \n",
    "    # Test whether buoy is MkIII or DWR-G - see p.51 Table 5.7.5a. Organization and significance of the system file data \n",
    "    # If DWR-G:\n",
    "    #     Av0 = 0; Ax0 = 0; Ay0 = 0; O = 0; and Inclination = 0\n",
    "    if (word_number == 7 && word == 0)\n",
    "        is_gps = true\n",
    "    end\n",
    "    \n",
    "    return()\n",
    "    \n",
    "end    # process_vector_1(i) \n",
    "\n",
    "    \n",
    "is_gps = false\n",
    "sync_word_location = findall(x -> x == \"7FFF\", df.Column2)\n",
    "\n",
    "for j in sync_word_location\n",
    "   \n",
    "#    process_vector_1(df.Column2[j+1])\n",
    "\n",
    "    i = df.Column2[j+2]\n",
    "    val = parse(Int, SubString.(i, 1, 1), base=16)*16^0\n",
    "##    println(df.Column2[j])  #bitstring(UInt16(df.Column2[j])))\n",
    "    \n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "function get_cyclic_word(word)\n",
    "##########################################    \n",
    "# get cyclic data block - see 5.7.1.2 Spectrum fill or full wave spectrum\n",
    "    \n",
    "    a = \"\"\n",
    "        for i in 1:4\n",
    "            a = a * bitstring(parse(Int8, SubString.(word, i, i), base=16))[5:8]\n",
    "        end\n",
    "    \n",
    "    return(a)\n",
    "    \n",
    "end    # get_cyclic_word()\n",
    "\n",
    "\n",
    "function calc_freq(freq_index)\n",
    "##########################################    \n",
    "    \n",
    "    if freq_index < 15\n",
    "        return(0.025 + freq_index*0.005)\n",
    "    elseif freq_index > 15\n",
    "        return(0.11+ (freq_index-16)*0.01)\n",
    "    else\n",
    "        return(0.10125)    # n = 15 is an intermediate point (see paragraph above table 5.7.3)\n",
    "    end\n",
    "    \n",
    "end    # calc_freq()   \n",
    "\n",
    "\n",
    "datawell_df = DataFrame([name => Float64[] for name in [\"Frequency\", \"RPSD\", \"Direction\", \"Spread\", \"m2\", \"n2\", \"k\"]])\n",
    "system_df = DataFrame([name => Any[] for name in [\"Word_No\", \"System_file_word\"]])\n",
    "\n",
    "is_gps = false\n",
    "sync_word_location = findall(x -> x == \"7FFF\", df.Column2)\n",
    "\n",
    "for j in sync_word_location\n",
    "    \n",
    "    cyclic_word_2 = get_cyclic_word(df.Column2[j+1])\n",
    "    Word_No = parse(Int, cyclic_word_2[1:4]; base=2)\n",
    "    System_file_word = cyclic_word_2[5:16]\n",
    "    push!(system_df,[Word_No,System_file_word])\n",
    "        \n",
    "    for k in 2:4:16\n",
    "        if j+k+2 < length(df.Column2)\n",
    "            \n",
    "            cyclic_word_3 = get_cyclic_word(df.Column2[j+k])\n",
    "            Slsb = parse(Int8, cyclic_word_3[1:2]; base=2)\n",
    "            f_index = parse(Int16, cyclic_word_3[3:8]; base=2)\n",
    "            direction = parse(Int16, cyclic_word_3[9:16]; base=2)\n",
    "            Direction = direction * 360 / 256\n",
    "            Frequency = calc_freq(f_index)\n",
    "            \n",
    "            cyclic_word_4 = get_cyclic_word(df.Column2[j+k+1])\n",
    "            M2lsb = parse(Int16, cyclic_word_4[1:2]; base=2)\n",
    "            N2lsb = parse(Int16, cyclic_word_4[1:2]; base=2)\n",
    "            rpsd = parse(Int16, cyclic_word_4[5:16]; base=2)\n",
    "            RPSD = exp(-rpsd/200)\n",
    "\n",
    "            cyclic_word_5 = get_cyclic_word(df.Column2[j+k+2])\n",
    "            spread = parse(Int16, cyclic_word_5[1:8]; base=2)\n",
    "            Spread = 0.4476 * ( spread + Slsb/4)\n",
    "            M2 = parse(Int16, cyclic_word_5[9:16]; base=2)\n",
    "            m2 = (M2 + M2lsb/4 -128) / 128\n",
    "            \n",
    "            cyclic_word_6 = get_cyclic_word(df.Column2[j+k+3])\n",
    "            N2 = parse(Int16, cyclic_word_6[1:8]; base=2)\n",
    "            K = parse(Int16, cyclic_word_6[9:16]; base=2)\n",
    "            n2 = (N2 + N2lsb/4 - 128)/128\n",
    "            k = K/100\n",
    "            \n",
    "##            println(cyclic_word_3,' ',f_index,' ',Frequency,' ',RPSD,' ',Direction,' ',Spread)\n",
    "            push!(datawell_df,[Frequency,RPSD,Direction,Spread,m2,n2,k])\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = findall(==(0.025), datawell_df.Frequency);\n",
    "\n",
    "p1 = plot(datawell_df.Frequency[records[1]:records[2]-1],datawell_df.Direction[records[1]:records[2]-1], lw=3, c=:blue, \n",
    "    yflip=true, ylim=[0,360], yticks = 0:30:360, grid=true, label=\"Direction\", ylabel=\"Direction (\"*L\"^o\"*dir_type*\")\", legend=:topright)\n",
    "p1 = plot!(datawell_df.Frequency[records[1]:records[2]-1], datawell_df.Direction[records[1]:records[2]-1] .+ datawell_df.Spread[records[1]:records[2]-1], \n",
    "    fillrange = datawell_df.Direction[records[1]:records[2]-1] .- datawell_df.Spread[records[1]:records[2]-1], fillalpha = 0.25, lw=0, c = 1, label = \"Spread\")\n",
    "\n",
    "p1 = plot!(twinx(),datawell_df.Frequency[records[1]:records[2]-1],datawell_df.RPSD[records[1]:records[2]-1],\n",
    "     c=:red,lw=3, label=\"Normalized DSP Datawell\\n\", ylabel=\"Spectral Density (m\"*L\"^2\"*\"/Hz.)\", legend=:bottomright)\n",
    "\n",
    "p_all = plot(p1, framestyle = :box, xlim=[0,0.64], title=\"DATAWELL Spectra and Direction\", xlabel=\"Frequency (Hz)\",\n",
    "        leftmargin = 15Plots.mm, rightmargin = 25Plots.mm, bottommargin = 15Plots.mm, size=(1400, 600), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, fg_legend = :false)\n",
    "\n",
    "display(p_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot(datawell_df.Frequency[records[1]:records[2]-1],datawell_df.m2[records[1]:records[2]-1],lw=2,c=:red,label=\"m2\",fg_legend = :false)\n",
    "plot!(datawell_df.Frequency[records[1]:records[2]-1],datawell_df.n2[records[1]:records[2]-1],lw=2,c=:green,label=\"n2\")\n",
    "plot!(datawell_df.Frequency[records[1]:records[2]-1],datawell_df.k[records[1]:records[2]-1],lw=2,c=:blue,label=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate System words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "function process_system_file_word_0(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    Tp = parse(Int16, system_file_word[1:4]; base=2)\n",
    "    M = parse(Int8, system_file_word[5]; base=2)\n",
    "    T = parse(Int8, system_file_word[6]; base=2)\n",
    "    F = parse(Int8, system_file_word[7]; base=2)\n",
    "    C = parse(Int8, system_file_word[8]; base=2)\n",
    "    Tn = parse(Int16, system_file_word[9:12]; base=2)\n",
    "    \n",
    "    println(\"Transmission No. \",Tn)\n",
    "    println(\"System file word 0: Tp = \",Tp,\"; M = \",M,\"; T = \",T,\"; F = \",F,\"; C = \",C,\"; Tn = \",Tn)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_0()\n",
    "\n",
    "\n",
    "function process_system_file_word_1(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    Hrms = parse(Int16, system_file_word[1:12]; base=2) / 400\n",
    "    m0 = Hrms ^ 2\n",
    "    Hm0 = 4 * sqrt(m0)\n",
    "    @printf(\"System file word 1: Hrms = %5.2fm; m0 = %5.5f; Hm0 = %5.2fm\\n\",Hrms,m0,Hm0)\n",
    "    push!(hm0,Hm0)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_1()\n",
    "\n",
    "\n",
    "function process_system_file_word_2(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    fz = parse(Int16, system_file_word[5:12]; base=2) / 400\n",
    "    Tz = 1/fz\n",
    "    @printf(\"System file word 2: fz = %5.4fHz; Tz = %5.2fs\\n\",fz,Tz)\n",
    "    push!(tz,Tz)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_2()\n",
    "\n",
    "\n",
    "function process_system_file_word_3(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    PSDmax = 5000 * exp(-parse(Int32, system_file_word[1:12]; base=2) / 200)\n",
    "    @printf(\"System file word 3: PSDmax = %5.4fm²/Hz\\n\",PSDmax)\n",
    "    push!(max_PSD,PSDmax)\n",
    "\n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_3()\n",
    "\n",
    "\n",
    "function process_system_file_word_4(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    Tr = parse(Int16, system_file_word[3:12]; base=2) / 20 - 5\n",
    "    println(\"System file word 4: Tr = \",Tr,\"ᵒC\")\n",
    "\n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_4()\n",
    "\n",
    "\n",
    "function process_system_file_word_5(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    Tw = parse(Int16, system_file_word[3:12]; base=2) / 20 - 5\n",
    "    println(\"System file word 5: Tw = \",Tw,\"ᵒC\")\n",
    "    push!(tw,Tw)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_5()\n",
    "\n",
    "\n",
    "function process_system_file_word_6(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    B = parse(Int16, system_file_word[10:12]; base=2)\n",
    "    tol = parse(Int16, system_file_word[1:8]; base=2)\n",
    "    println(\"System file word 6: B = \",B,\"; Tol = \",tol)\n",
    "\n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_6()\n",
    "\n",
    "\n",
    "function process_system_file_word_7(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    Av0 = parse(Int16, system_file_word[2:12]; base=2) / 800\n",
    "    sign = parse(Int16, system_file_word[1]; base=2)\n",
    "    \n",
    "    if (sign==0)   \n",
    "        println(\"System file word 7: Av0 = \",Av0,\"m/s²\")\n",
    "    elseif (sign==1)\n",
    "        println(\"System file word 7: Av0 = \",-Av0,\"m/s²\")\n",
    "    end\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_7()\n",
    "\n",
    "\n",
    "function process_system_file_word_8(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    Ax0 = parse(Int16, system_file_word[2:12]; base=2)\n",
    "    sign = parse(Int16, system_file_word[1]; base=2)\n",
    "    \n",
    "    if (sign==0)   \n",
    "        println(\"System file word 8: Ax0 = \",Ax0,\"m/s²\")\n",
    "    elseif (sign==1)\n",
    "        println(\"System file word 8: Ax0 = \",-Ax0,\"m/s²\")\n",
    "    end\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_8()\n",
    "\n",
    "\n",
    "function process_system_file_word_9(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    Ay0 = parse(Int16, system_file_word[2:12]; base=2)\n",
    "    sign = parse(Int16, system_file_word[1]; base=2)\n",
    "    \n",
    "    if (sign==0)   \n",
    "        println(\"System file word 9: Ay0 = \",Ay0,\"m/s²\")\n",
    "    elseif (sign==1)\n",
    "        println(\"System file word 9: Ay0 = \",-Ay0,\"m/s²\")\n",
    "    end\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_9()\n",
    "\n",
    "\n",
    "function process_system_file_word_10(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    LatMSB = system_file_word[2:12]\n",
    "    SignLat = system_file_word[1]\n",
    "    \n",
    "##    println(\"System file word 10 done\")\n",
    "    \n",
    "    return(LatMSB,SignLat)\n",
    "    \n",
    "    end    # process_system_file_word_10()\n",
    "\n",
    "\n",
    "function process_system_file_word_11(system_file_word,LatMSB,SignLat)\n",
    "##########################################    \n",
    "    \n",
    "    global LatLSB = system_file_word[1:12]\n",
    "    \n",
    "    lat = 90 * (parse(Int32, (LatMSB*LatLSB); base=2) / 2^23)\n",
    "    if (SignLat=='0')   \n",
    "        @printf(\"System file words 10 and 11: Latitude = %5.4fᵒN\\n\",lat)\n",
    "        push!(latitude,lat)\n",
    "    elseif (SignLat=='1')\n",
    "        @printf(\"System file words 10 and 11: Latitude = %5.4fᵒS\\n\",lat)\n",
    "        push!(latitude,-lat)        \n",
    "    end\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_11()\n",
    "\n",
    "\n",
    "function process_system_file_word_12(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    SignLon = system_file_word[1]\n",
    "    LonMSB = system_file_word[2:12]\n",
    "    \n",
    "##    println(\"System file word 12 done\")\n",
    "    \n",
    "    return(LonMSB,SignLon)\n",
    "    \n",
    "    end    # process_system_file_word_12()\n",
    "\n",
    "\n",
    "function process_system_file_word_13(system_file_word,LonMSB,Sign)\n",
    "##########################################    \n",
    "    \n",
    "    global LonLSB = system_file_word[1:12]\n",
    "    \n",
    "    lon = 180 * (parse(Int64, (LonMSB*LonLSB); base=2) / 2^23)\n",
    "   \n",
    "    if (Sign=='0')   \n",
    "        @printf(\"System file words 12 and 13: Longitude = %5.4fᵒE\\n\",lon)\n",
    "        push!(longitude,lon)\n",
    "        \n",
    "    elseif (Sign=='1')\n",
    "        @printf(\"System file words 12 and 13: Longitude = %5.4fᵒW\\n\",lon)\n",
    "        push!(longitude,-lon)\n",
    "    end\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_13()\n",
    "\n",
    "\n",
    "function process_system_file_word_14(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    O = 360 * (parse(Int16, system_file_word[5:12]; base=2) / 256)\n",
    "    \n",
    "    println(\"System file word 14: O = \",O,\"ᵒ\")\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_14()\n",
    "\n",
    "\n",
    "function process_system_file_word_15(system_file_word)\n",
    "##########################################    \n",
    "    \n",
    "    IncMSB = parse(Int16, system_file_word[5:12]; base=2)\n",
    "    IncLSB = parse(Int16, system_file_word[1:4]; base=2)\n",
    "    \n",
    "    I = (90/128) * (IncMSB - 128 + IncLSB/16)\n",
    "#    println(\"System file word 15: I = \",I,\"ᵒ\\n\")\n",
    "    @printf(\"System file word 15: I = %5.4fᵒ\\n\\n\",I)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # process_system_file_word_12()\n",
    "\n",
    "global hm0 = []\n",
    "global tz = []\n",
    "global max_PSD = []\n",
    "global tw = []\n",
    "global latitude = []\n",
    "global longitude = []\n",
    "\n",
    "for i in eachindex(system_df.Word_No)\n",
    "    if (system_df.Word_No[i]==0)\n",
    "        process_system_file_word_0(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==1)\n",
    "        process_system_file_word_1(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==2)\n",
    "        process_system_file_word_2(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==3)\n",
    "        process_system_file_word_3(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==4)\n",
    "        process_system_file_word_4(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==5)\n",
    "        process_system_file_word_5(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==6)\n",
    "        process_system_file_word_6(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==7)\n",
    "        process_system_file_word_7(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==8)\n",
    "        process_system_file_word_8(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==9)\n",
    "        process_system_file_word_9(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==10)\n",
    "        global LatMSB,SignLat = process_system_file_word_10(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==11)\n",
    "        try\n",
    "            process_system_file_word_11(system_df.System_file_word[i],LatMSB,SignLat)\n",
    "        catch\n",
    "            println(\"Attempted to process System file word 11 - but no word 10 available\")\n",
    "        end\n",
    "    elseif (system_df.Word_No[i]==12)\n",
    "        global LonMSB,SignLon = process_system_file_word_12(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==13)\n",
    "        try\n",
    "            process_system_file_word_13(system_df.System_file_word[i],LonMSB,SignLon)\n",
    "        catch\n",
    "            println(\"Attempted to process System file word 13 - but no word 12 available\")\n",
    "        end\n",
    "    elseif (system_df.Word_No[i]==14)\n",
    "        process_system_file_word_14(system_df.System_file_word[i])\n",
    "    elseif (system_df.Word_No[i]==15)\n",
    "        process_system_file_word_15(system_df.System_file_word[i])\n",
    "    else\n",
    "        println(\"Error - Invalid System file word\")       \n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "@printf(\"\\nHm0 = %5.2fm; Tz = %5.2fs; Tw = %5.2fᵒC; Pdenₘₐₓ = %5.4fm²/Hz; Latitude = %5.4fᵒ; Longitude = %5.4fᵒ\\n\",\n",
    "    mode(hm0),mode(tz),mode(tw),mode(max_PSD),mode(latitude),mode(longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate filtering GPS errors (simple case - fails if first gps point too close to start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "\n",
    "gps_errors = findall(isodd,parse.(Int,SubString.(string.(df.Column4), 2, 2), base = 16))\n",
    "\n",
    "GPS_file = \"C:\\\\Users\\\\Jim\\\\Python_programs\\\\Datawell\\\\Read_HXV\\\\Datawell_GPS_filter.csv\"\n",
    "aa = CSV.read(GPS_file, DataFrame, header=false)\n",
    "\n",
    "i=1\n",
    "center = gps_errors[i]\n",
    "\n",
    "gps_points = Int(trunc(nrow(aa)/2))\n",
    "vals = -gps_points:gps_points\n",
    "\n",
    "gps_dates = wse_df.Date[center] + Microsecond.(collect(-gps_points:gps_points) / 1.28 * 1000000)\n",
    "\n",
    "# locate the max and min GPS displacements\n",
    "max_val = maximum(wse_df.Heave[center.+vals])\n",
    "min_val = minimum(wse_df.Heave[center.+vals])\n",
    "multiplier = (max_val - min_val) * .5\n",
    "\n",
    "##println(max_val,' ',min_val,' ',multiplier)\n",
    "gps_filter = aa.Column1.*multiplier\n",
    "filtered_heave = wse_df.Heave[center.+vals] .- gps_filter\n",
    "\n",
    "# Set area between max and min GPS displacements to zero\n",
    "filtered_heave[argmin(wse_df.Heave[center.+vals]):argmax(wse_df.Heave[center.+vals])] .= 0.0\n",
    "\n",
    "# plot the Datawell GPS filter\n",
    "p1 = plot(gps_dates,gps_filter, c=:red, la=0.25, lw=3, label=\"Datawell GPS filter\\n\") \n",
    "\n",
    "# plot the original heave\n",
    "##p1 = plot!(gps_dates, wse_df.Heave[center.+vals], label=\"GPS error @ \"*string(wse_df.Date[center]), fillrange = 0, fillalpha = 0.05, fillcolor = :red)\n",
    "\n",
    "# plot the filtered heave\n",
    "p1 = plot!(gps_dates,filtered_heave,c=:green,label=\"Filtered heave\", fillrange = 0, fillalpha = 0.05, fillcolor = :green)\n",
    "    \n",
    "# plot all GPS errors in range\n",
    "#p1 = [vline!([wse_df.Date[ii]], c=:red, lw=1, ls=:dash,label=\"\") for ii in gps_errors]\n",
    "\n",
    "# plot GPS error of interest\n",
    "p1 = vline!([wse_df.Date[center]], c=:blue, lw=2, ls=:dash,label=\"\")\n",
    "\n",
    "# display area of uncertainity - all filtered WSEs in this area have been set to zero\n",
    "#p1 = vline!([gps_dates[argmin(wse_df.Heave[center.+vals])+1]], c=:green, lw=1, ls=:dash,label=\"\")\n",
    "#p1 = vline!([gps_dates[argmax(wse_df.Heave[center.+vals])-1]], c=:red, lw=1, ls=:dash,label=\"\")\n",
    "p1 = vspan!([gps_dates[argmin(wse_df.Heave[center.+vals])], gps_dates[argmax(wse_df.Heave[center.+vals])]],\n",
    "    lw=1, linecolor = :grey, fillalpha = 0.15, fillcolor = :grey,label=\"Area of uncertainity\")\n",
    "\n",
    "title_string = Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "\n",
    "plot_p1 = plot(p1, xlim=(first(gps_dates),last(gps_dates)),\n",
    "    size = (1400, 600), title=title_string, titlefontsize=10, framestyle = :box, fg_legend=:transparent, legend=:bottomleft,\n",
    "    leftmargin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "\n",
    "display(plot_p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate filtering GPS errors (conditional case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read Datawell filter data from .csv file to df\n",
    "GPS_file = \"C:\\\\Users\\\\Jim\\\\Python_programs\\\\Datawell\\\\Read_HXV\\\\Datawell_GPS_filter.csv\"\n",
    "datawell_filter_df = CSV.read(GPS_file, DataFrame, header=false)\n",
    "\n",
    "# add df column of filter point numbers (should be -378 to +378, centered on 0)\n",
    "filter_range = Int(trunc(nrow(datawell_filter_df)/2))\n",
    "filter_points = -filter_range:filter_range\n",
    "insertcols!(datawell_filter_df,1,:Points =>filter_points)\n",
    "\n",
    "# get a list of the gps errors in the selected record\n",
    "gps_errors = findall(isodd,parse.(Int,SubString.(string.(df.Column4), 2, 2), base = 16))\n",
    "\n",
    "# start at the first gps error\n",
    "i=1\n",
    "\n",
    "#Identify where GPS error is centered in wse\n",
    "center = gps_errors[i]\n",
    "\n",
    "# add df column of filter points from start of record (values less than zero will not be included in error correction process)\n",
    "datawell_filter_df.Location = datawell_filter_df.Points .+ center\n",
    "\n",
    "# locate filter points to be used\n",
    "filter_points1 = findall(x->x>0, datawell_filter_df.Location)\n",
    "\n",
    "# locate heave points to be used\n",
    "heave_points = findall(x->x>0, center.+filter_points1)\n",
    "\n",
    "plot(wse_df.Date[heave_points],wse_df.Heave[heave_points],\n",
    "    size = (1400, 600), title=title_string, titlefontsize=10, framestyle = :box, fg_legend=:transparent, legend=:bottomleft,\n",
    "    leftmargin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "plot!(wse_df.Date[heave_points],datawell_filter_df.Column1[filter_points1])\n",
    "vline!([wse_df.Date[center]], c=:blue, lw=2, ls=:dash,label=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Scatter Plot of WSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using GLMakie\n",
    "using CairoMakie\n",
    "GLMakie.activate!()\n",
    "\n",
    "heave = wse_df.Heave\n",
    "north = wse_df.North\n",
    "west = wse_df.West;\n",
    "\n",
    "date_string = Dates.format(first(wse_df.Date), \"dd/mm/yyyy HH:MM\")\n",
    "\n",
    "fig = Figure(size=(1200,1200))\n",
    "\n",
    "lscene = LScene(fig[1, 1], show_axis=true, width=1200, height=800)\n",
    "\n",
    "cmap =:redgreensplit\n",
    "\n",
    "meshscatter!(lscene, north, west, heave, markersize=0.0075, colormap=cmap, color=heave, alpha=0.5, fxaa=true, ssao=true)\n",
    "lines!(lscene, north, west, heave, markersize=0.01, linewidth=1, linestyle=:dot, colormap=cmap, color=heave)\n",
    "\n",
    "CairoMakie.Colorbar(fig[2, :], limits=(minimum(heave), maximum(heave)), label=\"WSE heights (m)\\n\"*date_string, labelsize=:20, \n",
    "        width=500, height=30, vertical=false, flipaxis=false, colormap=cmap)\n",
    "\n",
    "resize_to_layout!(fig)\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
